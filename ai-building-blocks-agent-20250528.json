[
  {
    "path": "README.md",
    "content": "# Cisco AI Building Blocks — Agent Service\n\n![Cisco AI Building Blocks](app/assets/ai_building_blocks.png)\n\n> **DISCLAIMER — USE AT YOUR OWN RISK**\n> This software is provided *“as is”*, without warranty of any kind. Cisco Systems, Inc. and contributors shall not be liable for any claim, damages, or other liability arising from its use. This project is intended **solely for demonstration and development**. By using the code you acknowledge that you have read, understood, and accepted these terms.\n\n---\n\n## What is the Agent Service?\n\nThe **Agent Service** is the runtime half of the *Cisco AI Building Blocks Suite*.\n\n* **Agent Service (this repo)** — FastAPI micro‑service that merges an LLM, retrieval‑augmented generation (RAG), and live Cisco platform APIs.\n* **Database Project** — companion pipeline that builds the vector indexes (Azure AI Search, Chroma, Elastic, …) consumed by the Agent.\n\nTogether they form a turnkey sandbox for exploring Gen‑AI‑powered automation on Cisco Meraki, Catalyst Center, Spaces, Webex, and more.\n\n---\n\n## 1 · Features\n\n| Capability                | Summary                                                                                                    |\n| ------------------------- | ---------------------------------------------------------------------------------------------------------- |\n| **Chat + RAG**            | Context‑aware chat backed by domain, API‑docs, and event indexes built by the Database Project.            |\n| **Function calling**      | The LLM can emit `function_call` JSON; a dispatcher triggers real Cisco REST APIs and returns live data.   |\n| **Unified Service layer** | One abstraction for Meraki, Catalyst, Spaces, Webex (extendable to Nexus, XDR, etc.).                      |\n| **Pluggable stack**       | Swap LLM (Azure OpenAI, Llama 3, local HF) and vector back‑end (Chroma, Azure Search, Elastic) via `.env`. |\n| **Static sample UI**      | A minimal HTML/JS front‑end in `static/` shows how to query the agent and render results.                  |\n\n---\n\n## 2 · High‑Level Architecture\n\n```mermaid\nflowchart TD\n    subgraph Client\n        FE[Browser / CLI / Bot]\n    end\n\n    subgraph FastAPI[Agent Service]\n        CHAT[chat_routes.py] -->|may call| RETRIEVE[Retrievers]\n        CHAT -->|LLM chat| LLM[LLM Wrapper]\n        LLM -->|function_call| DISPATCH[Function Dispatcher]\n        DISPATCH --> SERVICE[Unified Cisco Service]\n        SERVICE -->|REST| MERAKI[Meraki API]\n        SERVICE --> CATALYST[Catalyst Center API]\n        SERVICE --> SPACES[Spaces API]\n        SERVICE --> WEBEX[Webex API]\n        RETRIEVE -->|vector| VDB[(Vector DBs)]\n    end\n\n    FE --> CHAT\n```\n\n*Vector DBs* are produced by the **Cisco AI Building Blocks Database** project.\n\n---\n\n\n## 3 · Environment Blocks\n\nThe suite uses consistent env‑var prefixes:\n\n* **DOMAIN\\_**\\* — domain knowledge / summaries layer\n* **FASTAPI\\_**\\* — API‑docs layer\n* **EVENTS\\_**\\* — raw events layer\n* **AGENTIC\\_**\\* — (reserved) future agent‑of‑agents layer\n\nSee [`env‑guide.md`](example_environment_variables_guide.MD) for a full cheat‑sheet.\n\n---\n\n## 4 · Extending the Suite\n\n1. **Add a Cisco platform** → implement a `CiscoXClient` + register in `unified_service.py`.\n2. **New vector store** → write a retriever under `retrievers/` and set `<LAYER>_VECTOR_BACKEND`.\n3. **New LLM** → implement `BaseLLM` subclass and configure `<LAYER>_LLM_PROVIDER`.\n\nPRs welcome!\n\n---\n\n## 5 · License\n\nApache 2.0 • (c) 2025 Cisco Systems, Inc.\n\n---\n\n*Made with ❤️ by the Cisco AI Building Blocks team.*\n"
  },
  {
    "path": "app/__init__.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## ai-building-blocks-agent/app/__init__.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n"
  },
  {
    "path": "app/config.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n# ai-building-blocks-agent/app/config.py\n# Copyright (c) 2025 Jeff Teeter\n# Cisco Systems, Inc.\n# Licensed under the Apache License, Version 2.0 (see LICENSE)\n# Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n\"\"\"Centralised environment-variable resolver.\n\n*   Guarantees a **single source of truth** (no scattered ``os.getenv`` calls).\n*   Supports the hierarchical naming scheme::\n\n        <LAYER>_[<PROVIDER>_]KEY\n\n    Examples:\n        * ``FASTAPI_AZURE_LLM_MODEL=gpt-4o``\n        * ``DOMAIN_CHROMA_VECTOR_BACKEND=chroma``\n        * ``EVENTS_DEBUG_MODE=false``\n\nIf a requested var is missing, resolution falls back gracefully:\n\n1. ``<LAYER>_<PROVIDER>_<KEY>``\n2. ``<LAYER>_<KEY>``\n3. ``<KEY>``\n4. Provided ``default`` (if any)\n\nBoolean casting accepts common truthy strings: ``1, true, yes, on`` (case-insensitive).\n\"\"\"\n\n\n\nimport logging\nimport os\nfrom typing import Any, Callable, Optional, TypeVar\n\nfrom dotenv import load_dotenv\n\n# --------------------------------------------------------------------------- #\n# Initialisation                                                              #\n# --------------------------------------------------------------------------- #\n\nload_dotenv(override=False)  # never clobber system-exported envs\n_logger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\n\n# Default layer when the caller omits *layer* and ACTIVE_LAYER is unset\nDEFAULT_LAYER: str = os.getenv(\"ACTIVE_LAYER\", \"FASTAPI\").upper()\n\n# --------------------------------------------------------------------------- #\n# Helpers                                                                     #\n# --------------------------------------------------------------------------- #\n\n\ndef _to_bool(raw: str | bool | None) -> bool:\n    \"\"\"Cast an env string to bool with generous truthy values.\"\"\"\n    if isinstance(raw, bool):\n        return raw\n    if raw is None:\n        return False\n    return raw.strip().lower() in {\"1\", \"true\", \"t\", \"yes\", \"y\", \"on\"}\n\n\n_CASTERS: dict[type, Callable[[str], Any]] = {\n    bool: _to_bool,\n    int: int,\n    float: float,\n    str: str,\n}\n\n# --------------------------------------------------------------------------- #\n# Public API                                                                  #\n# --------------------------------------------------------------------------- #\n\n\nclass Config:\n    \"\"\"Callable env-lookup utility.\"\"\"\n\n    def __call__(\n        self,\n        key: str,\n        *,\n        layer: str | None = None,\n        provider: str | None = None,\n        default: Optional[T] = None,\n        cast: type[T] = str,  # type: ignore[assignment]\n    ) -> T | None:\n        \"\"\"Resolve and cast an environment variable.\n\n        Parameters\n        ----------\n        key\n            Core variable name (case-insensitive), e.g. ``\"LLM_MODEL\"``.\n        layer\n            Layer prefix (``FASTAPI``, ``DOMAIN``, …).\n            Defaults to ``DEFAULT_LAYER``.\n        provider\n            Optional provider infix (``AZURE``, ``CHROMA`` …).\n        default\n            Fallback value if the env var is absent *or* cannot be cast.\n        cast\n            Desired return type – one of ``str``, ``int``, ``float``, ``bool``.\n        \"\"\"\n        if cast not in _CASTERS:\n            raise TypeError(\n                f\"Unsupported cast type: {cast!r}. \"\n                f\"Choose from {', '.join(t.__name__ for t in _CASTERS)}.\"\n            )\n\n        parts: list[str] = []\n        if layer := (layer or DEFAULT_LAYER):\n            parts.append(layer.upper())\n        if provider:\n            parts.append(provider.upper())\n        parts.append(key.upper())\n        env_key = \"_\".join(parts)\n\n        val: str | None = os.getenv(env_key)\n\n        # 2️⃣ try without provider\n        if val is None and provider:\n            val = os.getenv(f\"{layer}_{key}\".upper())\n\n        # 3️⃣ fall back to plain key\n        if val is None:\n            val = os.getenv(key.upper())\n\n        if val is None:\n            return default  # type: ignore[return-value]\n\n        try:\n            return _CASTERS[cast](val)  # type: ignore[return-value]\n        except Exception as exc:  # noqa: BLE001\n            _logger.warning(\n                \"CONFIG: failed to cast %s=%r to %s – %s\",\n                env_key,\n                val,\n                cast.__name__,\n                exc,\n            )\n            return default  # type: ignore[return-value]\n\n\ncfg = Config()\n\n# Convenience wrappers – purely syntactic sugar\nget_bool = lambda k, **kw: cfg(k, cast=bool, **kw)  # noqa: E731\nget_int = lambda k, **kw: cfg(k, cast=int, **kw)  # noqa: E731\nget_float = lambda k, **kw: cfg(k, cast=float, **kw)  # noqa: E731\nget_str = lambda k, **kw: cfg(k, cast=str, **kw)  # noqa: E731\n\n# --------------------------------------------------------------------------- #\n# Quick self-test                                                             #\n# --------------------------------------------------------------------------- #\n\nif __name__ == \"__main__\":  # pragma: no cover\n    logging.basicConfig(level=\"INFO\", format=\"%(levelname)s: %(message)s\")\n\n    print(\"ACTIVE_LAYER:\", DEFAULT_LAYER)\n    print(\"FASTAPI -> DEBUG_MODE (bool):\",\n          cfg(\"DEBUG_MODE\", layer=\"FASTAPI\", cast=bool))\n    print(\"FASTAPI+AZURE -> LLM_MODEL (str):\",\n          cfg(\"LLM_MODEL\", layer=\"FASTAPI\", provider=\"AZURE\"))\n"
  },
  {
    "path": "app/main.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## ai-building-blocks-agent/app/main.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\nimport logging\nimport structlog\nimport os\nfrom pathlib import Path\nfrom typing import Annotated, Any\n\nfrom fastapi import Depends, FastAPI, HTTPException, Request, Response\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import HTMLResponse\nfrom fastapi.staticfiles import StaticFiles\n\nfrom prometheus_client import Histogram\n\nfrom app.config import cfg\nfrom app.telemetry import init_telemetry  # <— our helper\n\n# ── structured logging ─────────────────────────────────────────\nstructlog.configure(\n    wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),\n    processors=[\n        structlog.processors.TimeStamper(fmt=\"iso\"),\n        structlog.processors.JSONRenderer(),\n    ],\n)\nlog = structlog.get_logger(\"ai-agent\")\n\n# ── Optional retrievers import ─────────────────────────────────\ntry:\n    from retrievers.chroma_retriever import ChromaRetriever\nexcept ImportError as exc:\n    ChromaRetriever = None\n    logging.warning(\"Chroma retriever unavailable – %s\", exc)\n\ntry:\n    from retrievers.azure_search_retriever import AzureSearchRetriever\nexcept ImportError as exc:\n    AzureSearchRetriever = None\n    logging.warning(\"Azure Search retriever unavailable – %s\", exc)\n\n# ── Logging setup ───────────────────────────────────────────────\nlogger = logging.getLogger(\"ai_agent\")\nlogging.basicConfig(\n    level=os.getenv(\"LOG_LEVEL\", \"INFO\"),\n    format=\"%(levelname)s: %(message)s\",\n)\n\n# ── FastAPI app instantiation ──────────────────────────────────\napp = FastAPI(title=\"AI Building Blocks Agent\", version=\"0.3.0-dev\")\n\n# ── Initialize telemetry: tracing + metrics ───────────────────\ninit_telemetry(app)\n\n# ── Custom histogram: /chat latency ───────────────────────────\nrequest_latency = Histogram(\n    \"chat_latency_ms\",\n    \"End-to-end latency of /chat route in milliseconds\",\n)\n\n# ── Static & assets ────────────────────────────────────────────\nPROJECT_ROOT = Path(__file__).resolve().parent.parent\nSTATIC_DIR   = PROJECT_ROOT / \"static\"\nASSETS_DIR   = PROJECT_ROOT / \"app\" / \"assets\"\n\napp.mount(\"/static\", StaticFiles(directory=STATIC_DIR), name=\"static\")\napp.mount(\"/assets\", StaticFiles(directory=ASSETS_DIR), name=\"assets\")\n\n@app.get(\"/\", tags=[\"meta\"], response_class=HTMLResponse)\nasync def root() -> HTMLResponse:\n    \"\"\"\n    Single-page chat UI (opens *static/index.html*).\n    \"\"\"\n    index_path = STATIC_DIR / \"index.html\"\n    if not index_path.exists():\n        return HTMLResponse(\n            \"<h2>index.html not found — did you copy the static folder?</h2>\",\n            status_code=404,\n        )\n    return HTMLResponse(index_path.read_text(encoding=\"utf-8\"), status_code=200)\n\n# ── CORS ───────────────────────────────────────────────────────\norigins = cfg(\"CORS_ORIGINS\", cast=str, default=\"*\") or \"*\"\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[origins] if origins != \"*\" else [\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# ── Startup: choose vector retriever ───────────────────────────\n@app.on_event(\"startup\")\nasync def _startup() -> None:\n    layer    = cfg(\"ACTIVE_LAYER\", default=\"FASTAPI\").upper()\n    backend  = cfg(\"VECTOR_BACKEND\", layer=layer,\n                   default=os.getenv(\"FASTAPI_VECTOR_BACKEND\", \"chroma\")).lower()\n    provider = cfg(\"LLM_PROVIDER\", layer=layer,\n                   default=os.getenv(\"FASTAPI_LLM_PROVIDER\", \"azure\"))\n\n    logger.info(\"=== AI-Agent startup ===\")\n    logger.info(\"Layer=%s | VectorBackend=%s | LLM=%s\", layer, backend, provider)\n\n    if backend == \"chroma\":\n        if ChromaRetriever is None:\n            raise RuntimeError(\"chromadb not installed but VECTOR_BACKEND='chroma'.\")\n        app.state.retriever = ChromaRetriever(layer=layer)\n    elif backend in {\"azure\", \"azure_search\", \"azsearch\"}:\n        if AzureSearchRetriever is None:\n            raise RuntimeError(\"Azure Search retriever dependencies missing.\")\n        app.state.retriever = AzureSearchRetriever(layer=layer)\n    else:\n        logger.warning(\"Unknown VECTOR_BACKEND '%s' – retriever disabled.\", backend)\n        app.state.retriever = None\n\n    if not cfg(\"VECTOR_ENABLED\", layer=layer, cast=bool, default=True):\n        logger.warning(\"%s: VECTOR layer disabled – using NullRetriever.\", layer)\n        from retrievers.null_retriever import NullRetriever\n        app.state.retriever = NullRetriever()\n        return\n\n# ── Dependency injection helper ───────────────────────────────\ndef get_retriever(request: Request):\n    r = getattr(request.app.state, \"retriever\", None)\n    if r is None:\n        raise HTTPException(404, \"Vector retriever not configured.\")\n    return r\n\nRetrieverDep = Annotated[Any, Depends(get_retriever)]\n\n# ── Sample utility endpoints ─────────────────────────────────\n@app.get(\"/search\")\nasync def search_endpoint(q: str, retriever: RetrieverDep):\n    if hasattr(retriever, \"query\"):\n        return retriever.query(q, k=5)\n    return {\"error\": \"Retriever does not implement 'query'.\"}\n\n@app.get(\"/health\", tags=[\"meta\"])\nasync def health():\n    return {\"status\": \"ok\", \"layer\": cfg(\"ACTIVE_LAYER\", default=\"FASTAPI\")}\n\n# ── Routers ───────────────────────────────────────────────────\ntry:\n    from app.routers import chat_routes          # noqa: WPS433\n    app.include_router(chat_routes.router, prefix=\"/chat\")\nexcept ImportError as exc:\n    logger.warning(\"chat_routes not present – skipping (%s).\", exc)\n\ntry:\n    from app.routers.meraki_routes import router as meraki_router\n    app.include_router(meraki_router)  \n    logger.info(\"Mounted Meraki routes\")\nexcept ImportError as exc:\n    logger.warning(\"meraki_routes not present – skipping (%s)\", exc)\n\n"
  },
  {
    "path": "app/telemetry.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## AI-Building-Blocks-Agent/app/telemetry.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n\n\"\"\"\nConfigure OpenTelemetry tracing and Prometheus metrics for a FastAPI app.\nDisables outbound OTLP requests when OTEL_TRACES_EXPORTER is set to \"none\".\n\"\"\"\nimport os\n\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\nfrom opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom prometheus_fastapi_instrumentator import Instrumentator\n\n# Use the built-in ALWAYS_OFF sampler for disabling traces\nfrom opentelemetry.sdk.trace.sampling import ALWAYS_OFF\n\n\ndef init_telemetry(app):\n    \"\"\"\n    Initialize tracing and metrics. Honor OTEL_TRACES_EXPORTER=none to disable tracing exports.\n    \"\"\"\n    service_name = os.getenv(\"OTEL_SERVICE_NAME\", \"ai-building-blocks-agent\")\n    resource = Resource.create({\"service.name\": service_name})\n\n    # Determine if tracing exports should be disabled via environment\n    traces_exporter = os.getenv(\"OTEL_TRACES_EXPORTER\", \"otlp\").lower()\n    if traces_exporter == \"none\":\n        # Disable all tracing (no spans recorded or exported)\n        provider = TracerProvider(resource=resource, sampler=ALWAYS_OFF)\n        trace.set_tracer_provider(provider)\n    else:\n        # Configure OTLP exporter\n        otlp_endpoint = os.getenv(\"OTEL_EXPORTER_OTLP_ENDPOINT\", \"http://tempo:4318\")\n        provider = TracerProvider(resource=resource)\n        exporter = OTLPSpanExporter(endpoint=f\"{otlp_endpoint.rstrip('/')}/v1/traces\")\n        provider.add_span_processor(BatchSpanProcessor(exporter))\n        trace.set_tracer_provider(provider)\n\n        # Instrument FastAPI and outgoing HTTP calls\n        FastAPIInstrumentor.instrument_app(\n            app,\n            tracer_provider=provider,\n            server_request_hook=lambda span, scope: span.set_attribute(\"span.kind\", \"server\"),\n        )\n        RequestsInstrumentor().instrument()\n\n    # ── Prometheus metrics setup (always enabled) ───────────────────────────────\n    Instrumentator().instrument(app).expose(app, include_in_schema=False)\n"
  },
  {
    "path": "app/llm/azure_openai.py",
    "content": "# app/llm/azure_openai.py  (temporary shim)\nfrom app.llm.llm_factory import AzureOpenAIClient\n__all__ = [\"AzureOpenAIClient\"]\n"
  },
  {
    "path": "app/llm/base_llm.py",
    "content": "# app/llm/base_llm.py\nclass BaseLLM:                       # legacy name used by older modules\n    \"\"\"Removed during refactor – kept only for backward-compat.\"\"\"\n    async def chat(self, *a, **kw):   # never called – real work is in llm_factory\n        raise NotImplementedError\n"
  },
  {
    "path": "app/llm/dynamic.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## ai-building-blocks-agent/app/llm/dynamic.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n \n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n\n\"\"\"\nDynamic helper that chooses the correct vector-search retriever and\nbuilds the diet-function list the LLM receives.\n\n* Never raises AttributeError if a given retriever class name is missing.\n* Supports Chroma, Azure Cognitive Search and Elastic back-ends out-of-the-box.\n* Falls back to the first “SomethingRetriever” class it can find, so adding new\n  back-ends later is zero-touch.\n\nENV VARS\n--------\nFASTAPI_VECTOR_BACKEND          chroma | azure | elastic   (default: chroma)\nFASTAPI_CHROMA_COLLECTION_PLATFORM  override Chroma collection name\n\"\"\"\nimport importlib\nimport inspect\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Any, Dict, List\n\n# ─────────────────────────────────────────────────────────────────────────────\n# 1.  Pick retriever backend\n# ─────────────────────────────────────────────────────────────────────────────\nBACKEND = os.getenv(\"FASTAPI_VECTOR_BACKEND\", \"chroma\").lower()\n\nRETRIEVER_MODULES: dict[str, str] = {\n    \"chroma\":  \"retrievers.chroma_retriever\",\n    \"azure\":   \"retrievers.azure_search_retriever\",\n    \"elastic\": \"retrievers.elastic_retriever\",\n}\nif BACKEND not in RETRIEVER_MODULES:\n    raise ValueError(\n        f\"Unsupported FASTAPI_VECTOR_BACKEND={BACKEND!r}. \"\n        f\"Expected one of: {', '.join(RETRIEVER_MODULES)}\"\n    )\n\nmod = importlib.import_module(RETRIEVER_MODULES[BACKEND])\n\n# Try the common names first, then fall back to “any *Retriever class”\nRetriever = (\n    getattr(mod, \"FunctionRetriever\", None)\n    or getattr(mod, \"ChromaRetriever\", None)\n    or getattr(mod, \"AzureSearchRetriever\", None)\n    or getattr(mod, \"ElasticRetriever\", None)\n)\n\nif Retriever is None:\n    for attr in dir(mod):\n        obj = getattr(mod, attr)\n        if inspect.isclass(obj) and attr.endswith(\"Retriever\"):\n            Retriever = obj\n            break\n\nif Retriever is None:  # still nothing?  bail out clearly.\n    raise ImportError(\n        f\"No retriever class found in module {RETRIEVER_MODULES[BACKEND]}\"\n    )\n\n# ─────────────────────────────────────────────────────────────────────────────\n# 2.  Instantiate a singleton\n# ─────────────────────────────────────────────────────────────────────────────\nif BACKEND == \"chroma\":\n    retriever = Retriever(\n        collection_name=os.getenv(\n            \"FASTAPI_CHROMA_COLLECTION_PLATFORM\",\n            \"platform-summaries-index\",\n        )\n    )\n    # honour dynamic override if the Chroma client exposes helpers\n    custom_col = os.getenv(\n        \"FASTAPI_CHROMA_COLLECTION_PLATFORM\",\n        \"platform-summaries-index\",\n    )\n    if hasattr(retriever, \"set_collection\"):\n        retriever.set_collection(custom_col)\n    elif hasattr(retriever, \"collection_name\"):\n        setattr(retriever, \"collection_name\", custom_col)\nelse:  # Azure / Elastic (both accept layer kwarg)\n    retriever = Retriever(layer=\"FASTAPI\")\n\n# ─────────────────────────────────────────────────────────────────────────────\n# 3.  Full-schema KV (lazy build + cache)\n# ─────────────────────────────────────────────────────────────────────────────\n\nDYNAMIC_CACHE_ROOT = Path(\n    os.getenv(\n        \"PLATFORM_DYNAMIC_CACHE_PATH\",\n        # Default: <repo-root>/ai-building-blocks-agent/app/platform_dynamic_cache\n        (Path(__file__).resolve().parent.parent / \"platform_dynamic_cache\").as_posix(),\n    )\n).resolve()\nDYNAMIC_CACHE_ROOT.mkdir(parents=True, exist_ok=True)\n\nCACHE_FILE = DYNAMIC_CACHE_ROOT / \"full_schemas.json\"\nSPEC_DIR   = Path(__file__).resolve().parent / \"openapi_specs\"\n\n\ndef _build_full_kv() -> Dict[str, Any]:\n    \"\"\"Load `openapi_specs/full_*.json` files into a single lookup table.\"\"\"\n    kv: dict[str, Any] = {}\n    for fp in SPEC_DIR.glob(\"full_*.json\"):\n        platform = fp.stem.replace(\"full_\", \"\")\n        spec     = json.loads(fp.read_text(encoding=\"utf-8\"))\n        for path_item in spec.get(\"paths\", {}).values():\n            for op in path_item.values():\n                op_id = op.get(\"operationId\")\n                if op_id:\n                    kv[f\"{platform}:{op_id}\"] = op\n    CACHE_FILE.write_text(json.dumps(kv), encoding=\"utf-8\")\n    return kv\n\n\nFULL_KV: Dict[str, Any] = (\n    json.loads(CACHE_FILE.read_text(encoding=\"utf-8\"))\n    if CACHE_FILE.exists()\n    else _build_full_kv()\n)\n\n# ─────────────────────────────────────────────────────────────────────────────\n# 4.  Public helpers\n# ─────────────────────────────────────────────────────────────────────────────\ndef build_functions_for_llm(\n    query: str,\n    enabled: list[str],\n    *,\n    token_budget: int = int(os.getenv(\"FASTAPI_FUNCTION_TOKEN_BUDGET\", 16_384)),\n    k: int = int(os.getenv(\"FASTAPI_FUNCTION_TOP_K\", 50)),\n \n) -> List[dict]:\n    \"\"\"\n    Return a list of diet-function JSON objects that the LLM will receive.\n\n    1. Top-k vector search over enabled platforms.\n    2. Lexical fallback: include any function whose *name* contains a user token\n       (case-insensitive, plural⇢singular). Lexical hits are prepended so they\n       always survive trimming.\n    3. Trim JSON payload to fit within *token_budget*.\n    \"\"\"\n    # ── 1. vector search ────────────────────────────────────────────────\n    vec_hits: list[dict] = retriever.query(\n        query,\n        k=k,\n        filter={\"platform\": {\"$in\": enabled}},\n    )\n    have: set[str] = {d[\"name\"] for d in vec_hits}\n\n    # ── 2. lexical fallback  ────────────────────────────────────────────\n    raw_tokens = [t.lower().strip(\".,!?\") for t in query.split()]\n    tokens: set[str] = set()\n    for tok in raw_tokens:\n        tokens.add(tok)\n        if tok.endswith(\"s\") and len(tok) > 3:\n            tokens.add(tok[:-1])\n\n    lex_hits: list[dict] = []\n    for platform in enabled:\n        every = retriever.query(\n            \"\",                  # empty query returns everything\n            k=200,               # 🔥 lowered from 1000 → 200\n            filter={\"platform\": {\"$in\": [platform]}},\n        )\n        for d in every:\n            if d[\"name\"] in have:\n                continue\n            if any(tok in d[\"name\"].lower() for tok in tokens):\n                lex_hits.append(d)\n                have.add(d[\"name\"])\n\n    # lexical hits *first* → they survive trimming\n    docs = lex_hits + vec_hits\n\n    # ── 3. unwrap  + schema-sanity  + trimming ─────────────────────────\n    size = 0\n    out: list[dict] = []\n\n    for d in docs:\n        try:\n            fn_schema = json.loads(d[\"content\"])\n        except (KeyError, json.JSONDecodeError):\n            continue\n\n        # fix OpenAI array-schema quirk (missing \"items\")\n        for pschema in fn_schema.get(\"parameters\", {}).get(\"properties\", {}).values():\n            if pschema.get(\"type\") == \"array\" and \"items\" not in pschema:\n                pschema[\"items\"] = {\"type\": \"string\"}\n\n        payload = json.dumps(fn_schema, separators=(\",\", \":\"))\n        if size + len(payload) > token_budget:\n            break\n\n        size += len(payload)\n        out.append(fn_schema)\n\n    return out\n\n\ndef full_schema_lookup(platform: str, name: str) -> dict | None:\n    \"\"\"Return the full OpenAPI operation object (or `None` if not found).\"\"\"\n    return FULL_KV.get(f\"{platform}:{name}\")\n\n"
  },
  {
    "path": "app/llm/llm_factory.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## ai-building-blocks-agent/app/llm/llm_factory.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n \n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\nimport asyncio\nimport json\nimport logging\nimport os\nimport sys\nimport typing\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Literal, Optional, Union\nimport aiohttp\nimport openai\nfrom app.config import cfg\n\n\"\"\"LLM Factory – async wrapper over multiple back‑ends.\n\nSupported providers (pick via `<LAYER>_LLM_PROVIDER`):\n----------------------------------------------------\n* **azure**   – Azure OpenAI Chat Completions\n* **openai**  – Public OpenAI API\n* **llama3**  – Ollama‑style `/api/generate` (or any HTTP that follows that schema)\n* **hf_local** – On‑prem model via *transformers* **or** *llama‑cpp‑python* (CPU‑friendly)\n* **tgi** / **vllm** – Generic OpenAI‑compatible REST service (HuggingFace TGI 1.4+, vLLM ≥0.4)\n\nEach client exposes `await chat(messages, **kwargs) -> str` where\n`messages = [{\"role\": \"user\"|\"assistant\"|\"system\", \"content\": \"...\"}]`.\n\"\"\"\nlogger = logging.getLogger(__name__)\n\n# ---------------------------------------------------------------------------\n# Base interface\n# ---------------------------------------------------------------------------\nclass LLMClientBase:\n    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:  # noqa: D401\n        raise NotImplementedError\n\n\n# ---------------------------------------------------------------------------\n# Azure OpenAI\n# ---------------------------------------------------------------------------\nclass AzureOpenAIClient(LLMClientBase):\n    def __init__(self, *, layer: str):\n        self.deployment   = cfg(\"OPENAI_MODEL\",    layer=layer, provider=\"AZURE\")\n        self.endpoint     = cfg(\"OPENAI_ENDPOINT\", layer=layer, provider=\"AZURE\")\n        self.api_key      = cfg(\"OPENAI_KEY\",      layer=layer, provider=\"AZURE\")\n        self.api_version  = cfg(\"OPENAI_API_VERSION\",\n                                layer=layer, provider=\"AZURE\", default=\"2023-05-15\")\n\n        if not all((self.deployment, self.endpoint, self.api_key)):\n            raise ValueError(\"Azure OpenAI credentials incomplete.\")\n        openai.api_type    = \"azure\"\n        openai.api_base    = self.endpoint\n        openai.api_version = self.api_version\n        openai.api_key     = self.api_key\n\n    # NEW implementation ─ keeps the whole dict when a function call is returned\n    async def chat(\n        self,\n        messages: List[Dict[str, str]],\n        **kwargs,\n) -> Union[str, Dict[str, Any]]:       # <-- use Union from typing\n\n \n        \"\"\"\n        • If the model answers with plain text → return that string.  \n        • If the model decides to call a function → return the *entire*\n          message dict so `function_call` is preserved.\n        \"\"\"\n        resp = await openai.ChatCompletion.acreate(\n            deployment_id=self.deployment,\n            messages=messages,\n            **kwargs,\n        )\n        msg = resp.choices[0].message.to_dict()\n        return msg if msg.get(\"function_call\") else msg.get(\"content\", \"\")\n    \n# ---------------------------------------------------------------------------\n# Public OpenAI\n# ---------------------------------------------------------------------------\nclass OpenAIClient(LLMClientBase):\n    def __init__(self, *, layer: str):\n        self.api_key = cfg(\"OPENAI_API_KEY\", layer=layer)\n        self.model = cfg(\"OPENAI_MODEL\", layer=layer, default=\"gpt-4o-mini\")\n        if not self.api_key:\n            raise ValueError(\"OPENAI_API_KEY missing\")\n        openai.api_key = self.api_key\n\n    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:  # noqa: D401\n        resp = await openai.ChatCompletion.acreate(model=self.model, messages=messages, **kwargs)  # type: ignore[attr-defined]\n        return resp.choices[0].message.content\n\n\n# ---------------------------------------------------------------------------\n# Llama 3 via simple generate endpoint (e.g. Ollama)\n# ---------------------------------------------------------------------------\nclass Llama3Client(LLMClientBase):\n    def __init__(self, *, layer: str):\n        self.base_url = cfg(\"LLAMA3_BASE_URL\", layer=layer, default=\"http://localhost:11434\")\n        self.model = cfg(\"LLAMA3_MODEL_NAME\", layer=layer, default=\"llama3\")\n\n    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:  # noqa: D401\n        prompt = \"\\n\".join([m[\"content\"] for m in messages])\n        async with aiohttp.ClientSession() as sess:\n            async with sess.post(f\"{self.base_url}/api/generate\", json={\"model\": self.model, \"prompt\": prompt, **kwargs}, timeout=60) as resp:\n                resp.raise_for_status()\n                data = await resp.json()\n                return data[\"response\"]\n\n\n# ---------------------------------------------------------------------------\n# HF‑Local (transformers *or* llama‑cpp‑python)\n# ---------------------------------------------------------------------------\nclass HFLocalClient(LLMClientBase):\n    \"\"\"Runs a model in‑process – good for CPU‑only boxes.\"\"\"\n\n    def __init__(self, *, layer: str):\n        self.engine = cfg(\"HF_LOCAL_ENGINE\", layer=layer, default=\"transformers\").lower()\n        self.model_path = cfg(\"HF_LOCAL_MODEL_PATH\", layer=layer)  # local .gguf or HF model id\n        self.model_name = cfg(\"HF_LOCAL_MODEL_NAME\", layer=layer, default=\"mistralai/Mistral-7B-Instruct-v0.2\")\n        self.max_new = cfg(\"HF_LOCAL_MAX_TOKENS\", layer=layer, cast=int, default=256)\n\n        if self.engine == \"transformers\":\n            try:\n                from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # type: ignore\n            except ImportError:\n                raise ImportError(\"transformers not installed – pip install transformers sentencepiece\")\n            model_id = self.model_path or self.model_name\n            logger.info(\"Loading transformers model %s (this may take a while)…\", model_id)\n            tok = AutoTokenizer.from_pretrained(model_id)\n            mod = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True)\n            self.pipe = pipeline(\"text-generation\", model=mod, tokenizer=tok)\n        elif self.engine == \"llama_cpp\":\n            try:\n                from llama_cpp import Llama  # type: ignore\n            except ImportError:\n                raise ImportError(\"llama-cpp-python not installed – pip install llama-cpp-python\")\n            gguf_path = Path(self.model_path or \"./model.gguf\")\n            if not gguf_path.exists():\n                raise FileNotFoundError(f\"GGUF model not found at {gguf_path}\")\n            self.llm = Llama(model_path=str(gguf_path), n_ctx=4096)\n        else:\n            raise ValueError(f\"Unknown HF_LOCAL_ENGINE '{self.engine}' (transformers | llama_cpp)\")\n\n    async def _run_blocking(self, prompt: str) -> str:\n        if self.engine == \"transformers\":\n            res = self.pipe(prompt, max_new_tokens=self.max_new, do_sample=True)\n            return res[0][\"generated_text\"][len(prompt) :].strip()\n        else:  # llama_cpp\n            res = self.llm.create_chat_completion(messages=[{\"role\": \"user\", \"content\": prompt}], max_tokens=self.max_new)\n            return res[\"choices\"][0][\"message\"][\"content\"].strip()\n\n    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:  # noqa: D401\n        prompt = \"\\n\".join([m[\"content\"] for m in messages])\n        loop = asyncio.get_running_loop()\n        return await loop.run_in_executor(None, partial(self._run_blocking, prompt))\n\n\n# ---------------------------------------------------------------------------\n# TGI / vLLM remote server (OpenAI compatible)\n# ---------------------------------------------------------------------------\nclass TGIClient(LLMClientBase):\n    def __init__(self, *, layer: str):\n        self.base_url = cfg(\"TGI_BASE_URL\", layer=layer, default=\"http://localhost:8000\")\n        self.model = cfg(\"TGI_MODEL_NAME\", layer=layer, default=\"local-model\")\n        self.timeout = cfg(\"TGI_TIMEOUT_S\", layer=layer, cast=int, default=60)\n\n    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:  # noqa: D401\n        payload = {\"model\": self.model, \"messages\": messages, **kwargs}\n        async with aiohttp.ClientSession() as sess:\n            async with sess.post(f\"{self.base_url}/v1/chat/completions\", json=payload, timeout=self.timeout) as resp:\n                resp.raise_for_status()\n                data = await resp.json()\n                return data[\"choices\"][0][\"message\"][\"content\"].strip()\n\n\n# ---------------------------------------------------------------------------\n# Factory\n# ---------------------------------------------------------------------------\nProviderType = Literal[\"azure\", \"openai\", \"llama3\", \"hf_local\", \"tgi\", \"vllm\"]\n\n\ndef get_llm(layer: str | None = None) -> LLMClientBase:  # noqa: D401\n    layer = (layer or cfg(\"ACTIVE_LAYER\", default=\"FASTAPI\")).upper()\n    provider: ProviderType = cfg(\"LLM_PROVIDER\", layer=layer, default=\"azure\").lower()  # type: ignore[assignment]\n    logger.info(\"LLM factory – layer=%s provider=%s\", layer, provider)\n    # Bail out early if the layer says \"no LLM\"\n\n    if not cfg(\"LLM_ENABLED\", layer=layer, cast=bool, default=True):\n        class _Dummy(LLMClientBase):\n            async def chat(self, messages, **kw):      # noqa: D401\n                return \"⚠️  LLM disabled for this layer.\"\n        return _Dummy()\n\n    if provider == \"azure\":\n        return AzureOpenAIClient(layer=layer)\n    if provider == \"openai\":\n        return OpenAIClient(layer=layer)\n    if provider == \"llama3\":\n        return Llama3Client(layer=layer)\n    if provider == \"hf_local\":\n        return HFLocalClient(layer=layer)\n    if provider in {\"tgi\", \"vllm\"}:\n        return TGIClient(layer=layer)\n\n    raise ValueError(f\"Unsupported LLM provider '{provider}'.\")\n\n\n__all__ = [\n    \"LLMClientBase\",\n    \"AzureOpenAIClient\",\n    \"OpenAIClient\",\n    \"Llama3Client\",\n    \"HFLocalClient\",\n    \"TGIClient\",\n    \"get_llm\",\n]\n"
  },
  {
    "path": "app/llm/prompt_templates.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## ai-building-blocks-agent/app/llm/prompt_templates.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n\n# Example prompt templates for the LLM\n# Adjust these as needed for your domain and instructions.\n\nBASE_SYSTEM_PROMPT_DOCS_ONLY = \"\"\"You are a helpful AI assistant. You must only use the provided documents to answer questions.\n\n1. If the answer is not in the documents, say \"I don't know.\" Do not use any external knowledge.\n2. Return answers in HTML format only (no triple backticks).\n3. If the user's message is clearly continuing a previous request (e.g., \"yes\", \"please proceed\", \"go ahead\"), \n   and you have already suggested a function call or next step, do not say \"I don't know.\" \n   Instead, proceed with the indicated function or next step (i.e., multi-turn continuation).\n\"\"\"\n\nBASE_SYSTEM_PROMPT_GENERAL = \"\"\"You are a helpful AI assistant. You must only use the provided documents or retrieved data to answer questions.\n\n1. If the answer is not in the documents or results, say \"I don't know.\"\n2. Never use your own knowledge, training data, or general internet information.\n3. Return answers in HTML format only (no triple backticks).\n4. If the user's message is clearly continuing a previous request (e.g., \"yes\", \"go ahead\"), and you previously suggested a function call or next step, continue with that — otherwise, say \"I don't know.\"\n\"\"\"\n\n\n\nBASE_SYSTEM_PROMPT_EVENT = \"\"\"\nYou are an AI assistant specialized in explaining Cisco event data.\nYou must only use the provided event documents to answer questions.\n\nWhen the user requests event details, display them in a well-structured HTML format as follows:\n1. Please output pure HTML without wrapping it in Markdown fences (no triple backticks).\n2. Always show the <strong>Event ID</strong>, <strong>Detected event</strong>, <strong>Zone</strong>, \n   <strong>Date/Time</strong>, <strong>Camera</strong>, <strong>Building</strong>, <strong>Floor</strong>, \n   and <strong>Location</strong>.\n3. If 'recommended_actions' are present, list them under a <strong>Recommended Actions</strong> heading \n   with <ul> and <li>.\n4. If 'urls_for_further_action' are present, display them as clickable links under a \n   <strong>Additional URLs</strong> heading. Each link should be configured to open in a new browser\n   window (or tab) in fullscreen or 100% mode. For example:\n   <a href=\"LINK_URL\"\n      target=\"_blank\" \n      onclick=\"window.open('LINK_URL','_blank','fullscreen=yes'); return false;\">\n      LINK_NAME\n   </a>\n5. If 'extra_notes' are present, display them under a <strong>Notes</strong> heading in bullet points.\n6. Make the layout easy to read: separate major sections with <hr> or <p>.\n7. If the information is not in the documents, say \"I don't know.\"\n8. Do not wrap any output in code fences, and do not use triple backticks. All content must be valid HTML.  \n\"\"\"\n\nBASE_SYSTEM_PROMPT_LOB = \"\"\"\nYou are an AI assistant specialized in explaining LOB (healthcare) data.\nYou must only use the provided LOB documents to answer questions.\n\nWhen responding:\n1. Please output pure HTML without wrapping it in Markdown fences (no triple backticks).\n2. If the user requests a summary or data, display it in a well-structured HTML format \n   such as a table or list. \n3. If the user specifically asks to draft an email or letter, create a concise, \n   professional email (or letter) with these elements:\n   - **Subject** (for email)\n   - **Greeting** (e.g., \"Dear Alice Johnson,\" or \"Hello Bob,\")\n   - **Body** with relevant details (appointments, instructions, etc.)\n   - **Closing**, like \"Best regards,\" and a signature line\n   - If relevant data (like appointment times, reasons, or instructions) is found in \n     your provided LOB documents, incorporate it. If not, say \"I don't know.\"\n4. If the question asks for other details not in the documents, respond with \"I don't know.\"\n5. Make sure the final output is pure HTML—no Markdown code fences or triple backticks.\n\n\"\"\"\n\nHTML_MERAKI_INVENTORY_PROMPT = \"\"\"\nYou just called a Cisco function (Meraki) and have JSON with devices in the organization's inventory.\nPlease parse each device object and display the following fields (if available) in an HTML table:\n\n- <strong>Serial</strong>\n- <strong>MAC</strong>\n- <strong>Name</strong>\n- <strong>Model</strong>\n- <strong>Network ID</strong>\n- <strong>Product Type</strong>\n- <strong>Claimed At</strong>\n- <strong>License Expiration Date</strong>\n- <strong>Tags</strong>\n- <strong>Country Code</strong>\n\n**Requirements**:\n1. Only output valid HTML (no Markdown fences). \n2. Use a simple `<table>` with `<thead>` and `<tbody>`.\n3. Each device = one row. \n4. If a field is missing, leave it blank.\n5. Make it look professional and easy to read.\n6. Do NOT provide any other summary or explanation text, just the HTML table.\n\"\"\"\nHTML_MERAKI_APS_WITH_MESSAGE_PROMPT = \"\"\"\nYou just called a Cisco function that returned JSON with two keys: \n'message' (a short status string) and 'access_points' (an array of devices).\n\nPlease produce valid HTML:\n1. Display 'message' in an HTML paragraph: <p>{{message}}</p>\n2. Below that, build an HTML table for the 'access_points' array with columns:\n   - <strong>Serial</strong>\n   - <strong>Model</strong>\n   - <strong>MAC</strong>\n   - <strong>Network ID</strong>\n   - <strong>Product Type</strong>\n   - etc. (any relevant fields)\n3. No Markdown backticks or code fences; pure HTML only.\n4. If a field is missing for a device, leave it blank.\n5. No extra commentary or summary—just the HTML.\n\"\"\"\n\nFUNCTIONS_LLM_PROMPT = \"\"\"\nYou are a meta-assistant that must pick exactly **one** function from the\nJSON list below to satisfy the user’s request.\n\nReturn **only** a JSON object on a single line, with this shape:\n\n{\n  \"name\": \"<function name>\",\n  \"arguments\": { ... }\n}\n\nImportant rules:\n• Do NOT wrap the JSON in markdown fences.\n• Do NOT add any extra keys or text.\n• If no function is relevant, respond with ordinary text instead.\n\"\"\"\n\n\nUSER_PROMPT_TEMPLATE = \"\"\"User: {user_query}\"\"\""
  },
  {
    "path": "app/llm/utils.py",
    "content": "#!/usr/bin/env python3\n# ./ai-building-blocks-agent/app/llm/utils.py\nfrom __future__ import annotations\nimport os\n\n# Map the suffix in ENABLE_<SUFFIX> to the slug used by your scaffolder/json files\n_ALIAS = {\n    \"CATALYST_CENTER\"     : \"catalyst_center\",\n    \"CATALYST_SD_WAN\"     : \"catalyst_sd_wan\",\n    \"CISCO_SPACES\"        : \"spaces\",\n    \"CISCO_WEBEX\"         : \"webex\",\n    \"MERAKI\"              : \"meraki\",\n    \"NEXUS_HYPERFABRIC\"   : \"nexus_hyperfabric\",\n    # add new platforms here ⬆\n}\n\ndef enabled_platforms() -> list[str]:\n    \"\"\"\n    Scan env for ENABLE_<PLATFORM>=true and return a list of platform slugs.\n    Falls back to ['meraki'] if none are set.\n    \"\"\"\n    out: list[str] = []\n    for env_key, value in os.environ.items():\n        if not env_key.startswith(\"ENABLE_\"):\n            continue\n        if value.strip().lower() != \"true\":\n            continue\n        suffix = env_key[len(\"ENABLE_\"):]\n        slug   = _ALIAS.get(suffix, suffix.lower())\n        out.append(slug)\n    return out or [\"meraki\"]      # sensible default\n"
  },
  {
    "path": "retrievers/__init__.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## ai-building-blocks-agent/retrievers/__init__.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n \n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n\nfrom app.config import cfg\n\n\n# ════════════════════════════════════════════════════════════════════════\n# 1️⃣  Helper that every retriever can safely import\n# ════════════════════════════════════════════════════════════════════════\ndef default_pool_size(layer: str, backend: str, *, fallback: int = 4) -> int:\n    \"\"\"\n    Resolution order (highest-to-lowest):\n\n        <LAYER>_<BACKEND>_RETRIEVER_WORKERS\n        <LAYER>_RETRIEVER_WORKERS\n        RETRIEVER_WORKERS\n        fallback (default 4)\n    \"\"\"\n    layer_uc   = layer.upper()\n    backend_uc = backend.upper()\n\n    # first: layer+backend specific\n    val = cfg(f\"{backend_uc}_RETRIEVER_WORKERS\", layer=layer_uc, cast=int, default=None)\n    if val is not None:\n        return val\n\n    # second: layer-wide\n    val = cfg(\"RETRIEVER_WORKERS\", layer=layer_uc, cast=int, default=None)\n    if val is not None:\n        return val\n\n    # third: global\n    val = cfg(\"RETRIEVER_WORKERS\", cast=int, default=fallback)\n    return val\n\n\n# ════════════════════════════════════════════════════════════════════════\n# 2️⃣  Now import the retriever implementations\n#     (they are free to `from retrievers import default_pool_size`)\n# ════════════════════════════════════════════════════════════════════════\nfrom .azure_search_retriever import AzureSearchRetriever   # noqa: E402\nfrom .chroma_retriever       import ChromaRetriever        # noqa: E402\nfrom .elastic_retriever      import ElasticRetriever       # noqa: E402\nfrom .null_retriever         import NullRetriever          # noqa: E402\n\n\n__all__ = [\n    \"default_pool_size\",\n    \"AzureSearchRetriever\",\n    \"ChromaRetriever\",\n    \"ElasticRetriever\",\n    \"NullRetriever\",\n]"
  },
  {
    "path": "retrievers/azure_search_retriever.py",
    "content": "#! /usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n# ai-building-blocks-agent/retrievers/azure_search_retriever.py\n# Copyright (c) 2025 Jeff Teeter, Ph.D. – Cisco Systems, Inc.\n# Apache-2.0  (see LICENSE) – Provided “AS IS”, no warranties.\n################################################################################\n\"\"\"\nDomain-first retriever for **Azure AI Search** (formerly Cognitive Search).\n\nEnvironment variables (layer-aware – replace <LAYER> with FASTAPI / DOMAIN / …):\n\n    <LAYER>_AZURE_ENDPOINT             https://<svc>.search.windows.net\n    <LAYER>_AZURE_KEY                  <admin-key-or-query-key>\n    <LAYER>_AZURE_API_VERSION          2024-11-01-preview  (default)\n\n    # Index names\n    <LAYER>_AZURE_DOMAIN_INDEX         domain-index            (required)\n    <LAYER>_AZURE_PLATFORM_INDEX       platform-summaries-index\n    <LAYER>_AZURE_API_DOCS_INDEX       api-docs-index\n    <LAYER>_AZURE_EVENTS_INDEX         events-index\n\n    # Embedding creds (Azure OpenAI)\n    <LAYER>_AZURE_OPENAI_ENDPOINT\n    <LAYER>_AZURE_OPENAI_KEY\n    <LAYER>_AZURE_OPENAI_EMBEDDING_DEPLOYMENT\n\"\"\"\n\nimport logging\nimport re\nfrom typing import Dict, List, Optional\n\nimport requests\n\nfrom app.config import cfg\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom retrievers import default_pool_size \n_logger = logging.getLogger(__name__)\n\n\ndef _sanitize_filename(text: str, max_len: int = 100) -> str:  # noqa: D401\n    \"\"\"Remove nasty chars so Windows/macOS don’t choke if files ever get saved.\"\"\"\n    return re.sub(r\"[\\\\/:*?\\\"<>|]\", \"_\", text)[:max_len].rstrip()\n\n\nclass AzureSearchRetriever:\n    \"\"\"Lightweight wrapper around the *vector + semantic* hybrid query API.\"\"\"\n\n    def __init__(self, *, layer: str | None = None, top_k: int | None = None):\n        self.layer = (layer or cfg(\"ACTIVE_LAYER\", default=\"FASTAPI\")).upper()\n\n        # ------------------------------------------------------------------ #\n        # Connection & auth\n        # ------------------------------------------------------------------ #\n        self.search_endpoint = cfg(\"AZURE_ENDPOINT\", layer=self.layer)\n        self.search_key = cfg(\"AZURE_KEY\", layer=self.layer)\n        self.api_version = cfg(\n            \"AZURE_API_VERSION\", layer=self.layer, default=\"2024-11-01-preview\"\n        )\n\n        if not (self.search_endpoint and self.search_key):\n            raise RuntimeError(\n                f\"[{self.layer}] Missing AZURE_ENDPOINT or AZURE_KEY – check your .env\"\n            )\n\n        # ------------------------------------------------------------------ #\n        # Index names\n        # ------------------------------------------------------------------ #\n        self.domain_index = cfg(\n            \"AZURE_DOMAIN_INDEX\", layer=self.layer, default=\"domain-index\"\n        )\n        self.platform_summaries_index = cfg(\n            \"AZURE_PLATFORM_INDEX\", layer=self.layer, default=\"\"\n        )\n        self.api_docs_index = cfg(\"AZURE_API_DOCS_INDEX\", layer=self.layer, default=\"\")\n        self.events_index = cfg(\"AZURE_EVENTS_INDEX\", layer=self.layer, default=\"\")\n\n        # ------------------------------------------------------------------ #\n        # Embedding (Azure OpenAI)\n        # ------------------------------------------------------------------ #\n        self.openai_endpoint = cfg(\"AZURE_OPENAI_ENDPOINT\", layer=self.layer)\n        self.openai_key = cfg(\"AZURE_OPENAI_KEY\", layer=self.layer)\n        self.embedding_deployment = cfg(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", layer=self.layer)\n        self.openai_api_version = cfg(\n            \"AZURE_OPENAI_API_VERSION\", layer=self.layer, default=\"2023-05-15\"\n        )\n\n        if not all((self.openai_endpoint, self.openai_key, self.embedding_deployment)):\n            raise RuntimeError(\n                f\"[{self.layer}] Missing Azure OpenAI embedding creds – \"\n                \"AZURE_OPENAI_ENDPOINT / AZURE_OPENAI_KEY / AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"\n            )\n\n        # Misc\n        self.vector_field = cfg(\n            \"AZURE_VECTOR_COLUMNS\", layer=self.layer, default=\"embedding\"\n        )\n        self.top_k = int(\n            top_k\n            if top_k is not None\n            else cfg(\"AZURE_TOP_K\", layer=self.layer, cast=int, default=5)\n        )\n        # ────────────── concurrent-embed pool size ────────────────\n        self.pool_size = default_pool_size(self.layer, \"azure\", fallback=4)\n        # One informative line at construction time\n        print(\n            f\"[{self.layer}/AzureRetriever]  ⚙  embed-workers = {self.pool_size}\",\n            flush=True,\n        )\n\n    # ------------------------------------------------------------------ #\n    # Internal helpers\n    # ------------------------------------------------------------------ #\n    def _embed(self, text: str) -> List[float]:\n        body = {\n            \"input\": text,                 # single string is fine\n            \"encoding_format\": \"float\",    # <-- THE important line\n            # \"dimensions\": 1536           # only if you want to shorten vectors\n    }\n        url = (\n            f\"{self.openai_endpoint}/openai/deployments/\"\n            f\"{self.embedding_deployment}/embeddings\"\n            f\"?api-version={self.openai_api_version}\"\n        )\n        resp = requests.post(\n            url,\n            headers={\"api-key\": self.openai_key, \"Content-Type\": \"application/json\"},\n            json=body,\n            timeout=30,\n        )\n        if resp.status_code >= 400:\n            print(\"Azure returned:\", resp.text)   \n        resp.raise_for_status()\n        return resp.json()[\"data\"][0][\"embedding\"]\n\n    def _query(self, index: str, payload: Dict) -> List[Dict]:\n        url = (\n            f\"{self.search_endpoint}/indexes('{index}')/docs/search\"\n            f\"?api-version={self.api_version}\"\n        )\n        try:\n            resp = requests.post(\n                url,\n                headers={\"api-key\": self.search_key, \"Content-Type\": \"application/json\"},\n                json=payload,\n                timeout=60,\n            )\n            resp.raise_for_status()\n            return resp.json().get(\"value\", [])\n        except requests.RequestException as exc:  # noqa: BLE001\n            _logger.error(\"Azure AI Search error on %s → %s\", index, exc)\n            if exc.response is not None:\n                _logger.error(\"Response text: %s\", exc.response.text)\n            return []\n        \n    def _embed_many(self, queries: list[str]) -> list[list[float]]:\n        \"\"\"\n        Parallel embeds with `self.pool_size` workers.\n        \"\"\"\n        with ThreadPoolExecutor(max_workers=self.pool_size) as pool:\n            futs = {pool.submit(self._embed, q): i for i, q in enumerate(queries)}\n            vecs = [None] * len(queries)                         # pre-allocate\n            for fut in as_completed(futs):\n                vecs[futs[fut]] = fut.result()\n        return vecs\n\n    # ------------------------------------------------------------------ #\n    # Public retrieval methods\n    # ------------------------------------------------------------------ #\n    def retrieve_domain_info(self, query: str) -> List[Dict]:\n        vec = self._embed(query)\n        payload = {\n            \"search\": query,\n            \"queryType\": \"semantic\",\n            \"semanticConfiguration\": \"domain-index-semantic-config\",\n            \"top\": self.top_k,\n            \"vectorQueries\": [\n                {\n                    \"kind\": \"vector\",\n                    \"fields\": self.vector_field,\n                    \"vector\": vec,\n                    \"k\": self.top_k,\n                }\n            ],\n            \"select\": \"id,content,metadata\",\n        }\n        return self._query(self.domain_index, payload)\n\n    def retrieve_platform_summaries(self, query: str) -> List[Dict]:\n        \"\"\"\n        Search the *function-definitions* index.  Behaviour can be tuned with\n        two env-vars:\n\n        • FASTAPI_AZURE_PLATFORM_SEMCONF   » name of the semantic-config\n          (set it **empty** to disable semantic ranking).\n        • FASTAPI_AZURE_PLATFORM_SELECT    » comma-sep list of fields to return\n          (must exist in the index doc-schema).\n        \"\"\"\n        if not self.platform_summaries_index:\n            return []\n\n        semconf = cfg(\n            \"AZURE_PLATFORM_SEMCONF\",\n            layer=self.layer,\n            default=\"platform-summaries-semantic-config\",\n        )\n        select = cfg(\n            \"AZURE_PLATFORM_SELECT\",\n            layer=self.layer,\n            default=\"platform,name,content\",     # no doc_type by default\n        )\n\n        vec = self._embed(query)\n        payload: Dict = {\n            \"search\": query,\n            \"top\": self.top_k,\n            \"vectorQueries\": [\n                {\n                    \"kind\": \"vector\",\n                    \"fields\": self.vector_field,\n                    \"vector\": vec,\n                    \"k\": self.top_k,\n                }\n            ],\n            \"select\": select,\n        }\n        if semconf:\n            payload.update(\n                {\n                    \"queryType\": \"semantic\",\n                    \"semanticConfiguration\": semconf,\n                }\n            )\n\n        return self._query(self.platform_summaries_index, payload)\n\n\n \n\n    def retrieve_event_info(self, query: str, event_type: Optional[str] = None) -> List[Dict]:\n        if not self.events_index:\n            return []\n        vec = self._embed(query)\n        payload = {\n            \"search\": query,\n            \"queryType\": \"semantic\",\n            \"semanticConfiguration\": \"events-semantic-config\",\n            \"top\": self.top_k,\n            \"vectorQueries\": [\n                {\n                    \"kind\": \"vector\",\n                    \"fields\": self.vector_field,\n                    \"vector\": vec,\n                    \"k\": self.top_k,\n                }\n            ],\n            \"select\": \"event_id,event_name,event_type,content,additional_info\",\n        }\n        if event_type:\n            payload[\"filter\"] = f\"event_type eq '{event_type}'\"\n        return self._query(self.events_index, payload)\n\n \n    # ------------------------------------------------------------------ #\n    # Generic vector query – API-compatible with ChromaRetriever\n    # ------------------------------------------------------------------ #\n    def query(\n        self,\n        text: str,\n        *,\n        k: int = 128,\n        filter: Optional[dict] = None,\n    ) -> List[Dict]:\n        \"\"\"\n        Wrapper used by dynamic.build_functions_for_llm().\n        Just redirect to the trusted retrieve_platform_summaries().\n        \"\"\"\n        # honour the simple platform filter, if present\n        if filter and \"platform\" in filter:\n            clause = filter[\"platform\"]\n            if isinstance(clause, dict) and \"$in\" in clause:\n                platforms = clause[\"$in\"]\n                if platforms and self.platform_summaries_index:\n                    # form Azure $filter clause\n                    self._extra_filter = \" or \".join(\n                        [f\"platform eq '{p}'\" for p in platforms]\n                    )\n        else:\n            self._extra_filter = None\n\n        # call the original method (it already embeds + does semantic search)\n        hits = self.retrieve_platform_summaries(text)\n\n        # keep only top-k (retrieve_platform_summaries already uses self.top_k,\n        # but dynamic.py may pass k < self.top_k)\n        return hits[:k]"
  },
  {
    "path": "retrievers/chroma_retriever.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## ai-building-blocks-agent/retrievers/chroma_retriever.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\n\"\"\"\n\n\"\"\"\nVery thin wrapper around a *single* Chroma collection.\n\n• query(text, k)               – generic top‑k vector search\n• retrieve_event_info(text)    – alias of query(text, 5)\n• retrieve_domain_info(text)   – alias of query(text, 5)\n• retrieve_api_docs(text, pls) – same, but filtered by platform meta\n• FunctionRetriever            – exposes .query exactly as dynamic.py expects\n\"\"\"\n \nimport os\nimport logging\nfrom pathlib import Path\nfrom typing import Sequence, List, Dict, Any\n\nimport chromadb\nfrom chromadb.config import Settings\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nfrom retrievers import default_pool_size          # ← safe (defined early)\n\nlog = logging.getLogger(__name__)\n\n\nclass ChromaRetriever:\n    # ────────────────────────────────────────────────────────────────────\n    # construction\n    # ────────────────────────────────────────────────────────────────────\n    def __init__(\n        self,\n        *,\n        layer: str = \"FASTAPI\",\n        collection_name: str | None = None,\n    ) -> None:\n        self.layer = layer.upper()\n        base_path  = os.getenv(f\"{self.layer}_CHROMA_DB_PATH\", \"./chroma_dbs/fastapi\")\n        coll_name  = (\n            collection_name\n            or os.getenv(f\"{self.layer}_CHROMA_COLLECTION_PLATFORM\", \"platform-summaries-index\")\n        )\n\n        self.path = Path(base_path).expanduser().resolve() / coll_name\n        if not self.path.exists():\n            raise FileNotFoundError(\n                f\"Chroma collection directory '{self.path}' not found.\"\n            )\n\n        client = chromadb.PersistentClient(\n            path=str(self.path),\n            settings=Settings(anonymized_telemetry=False),\n        )\n        self.col = client.get_or_create_collection(coll_name)\n\n        # ── concurrency knobs ─────────────────────────────────────────\n        self.pool_size = default_pool_size(self.layer, \"chroma\", fallback=4)\n        print(\n            f\"[{self.layer}/ChromaRetriever]  ⚙  workers = {self.pool_size}\",\n            flush=True,\n        )\n\n    # ────────────────────────────────────────────────────────────────────\n    # core query helpers\n    # ────────────────────────────────────────────────────────────────────\n    def _query_one(\n        self,\n        text: str,\n        *,\n        k: int,\n        where_clause: Dict[str, Any] | None,\n    ) -> List[dict]:\n        res = self.col.query(\n            query_texts=[text],\n            n_results=k,\n            include=[\"documents\", \"distances\", \"metadatas\"],\n            where=where_clause,\n        )\n        docs  = res.get(\"documents\", [[]])[0]\n        metas = res.get(\"metadatas\", [[]])[0]\n        dists = (res.get(\"distances\") or [[None]])[0]\n\n        return [\n            {\"content\": doc, **(meta or {}), \"distance\": dist}\n            for doc, meta, dist in zip(docs, metas, dists)\n        ]\n\n    # public – single text ------------------------------------------------\n    def query(\n        self,\n        text: str,\n        *,\n        k: int = 5,\n        filter: Dict[str, Any] | None = None,\n    ) -> List[dict]:\n        return self._query_one(text, k=k, where_clause=filter)\n\n    # public – many texts (parallel) -------------------------------------\n    def query_many(\n        self,\n        texts: Sequence[str],\n        *,\n        k: int = 5,\n        filter: Dict[str, Any] | None = None,\n    ) -> List[List[dict]]:\n        if not texts:\n            return []\n\n        with ThreadPoolExecutor(max_workers=self.pool_size) as pool:\n            futs = {\n                pool.submit(self._query_one, t, k=k, where_clause=filter): idx\n                for idx, t in enumerate(texts)\n            }\n            results: List[List[dict]] = [None] * len(texts)  # type: ignore\n            for fut in as_completed(futs):\n                results[futs[fut]] = fut.result()\n        return results\n\n    # convenience aliases -------------------------------------------------\n    def retrieve_domain_info(self, text: str) -> List[dict]:\n        return self.query(text, k=5)\n\n\n\n\n# ------------------------------------------------------------------------------\n# adapter that dynamic.py expects ----------------------------------------------\nclass FunctionRetriever:\n    def __init__(self, collection_name: str) -> None:\n        self._inner = ChromaRetriever(collection_name=collection_name)\n\n    def query(self, *args, **kwargs):\n        return self._inner.query(*args, **kwargs)\n"
  },
  {
    "path": "retrievers/elastic_retriever.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## ai-building-blocks-agent/retrievers/elastic_retriever.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n \n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n\nimport os\nfrom typing import List\nfrom elasticsearch import Elasticsearch\n#from app.llm.llm_factory import AzureOpenAIClient\n\nclass ElasticRetriever:\n    \"\"\"\n    A retriever that uses Elasticsearch for vector-based (and/or hybrid) retrieval.\n    \"\"\"\n\n    def __init__(self):\n        self.elastic_host = os.getenv(\"ELASTIC_HOST\", \"http://localhost:9200\")\n        self.elastic_user = os.getenv(\"ELASTIC_USERNAME\", \"elastic\")\n        self.elastic_pass = os.getenv(\"ELASTIC_PASSWORD\", \"changeme\")\n        self.elastic_index = os.getenv(\"ELASTIC_INDEX\", \"cisco_docs\")\n        self.elastic_vector_field = os.getenv(\"ELASTIC_VECTOR_FIELD\", \"embedding\")\n        # For hybrid, you might store the text in a field named \"content\" and the vector in \"embedding\".\n        self.es = Elasticsearch(\n            self.elastic_host,\n            basic_auth=(self.elastic_user, self.elastic_pass),\n            verify_certs=False\n        )\n\n        # For embeddings\n        #self.llm_client = AzureOpenAIClient()\n\n    def retrieve_documents(self, user_input: str, top_k: int = 5) -> List[str]:\n        \"\"\"\n        1) Get embedding for user_input\n        2) Query Elasticsearch (k-NN or hybrid)\n        3) Return doc chunks\n        \"\"\"\n        embedding = self.llm_client.get_embedding(user_input)\n\n        # EXAMPLE: a simple vector query in ES 8.x\n        # If you want strictly vector search:\n        # (Note: the field \"embedding\" must be mapped as a dense_vector or similar.)\n        query = {\n            \"size\": top_k,\n            \"query\": {\n                \"knn\": {\n                    self.elastic_vector_field: {\n                        \"vector\": embedding,\n                        \"k\": top_k\n                    }\n                }\n            }\n        }\n\n        # If you want a hybrid approach (BM25 + vector), you can do something like:\n        # query = {\n        #   \"size\": top_k,\n        #   \"query\": {\n        #       \"bool\": {\n        #           \"should\": [\n        #               { \"match\": { \"content\": user_input } },\n        #               {\n        #                   \"script_score\": {\n        #                       \"query\": {\"match_all\": {}},\n        #                       \"script\": {\n        #                           \"source\": f\"cosineSimilarity(params.queryVector, '{self.elastic_vector_field}') + 1.0\",\n        #                           \"params\": {\"queryVector\": embedding}\n        #                       }\n        #                   }\n        #               }\n        #           ]\n        #       }\n        #   }\n        # }\n\n        response = self.es.search(index=self.elastic_index, body=query)\n\n        docs = []\n        hits = response[\"hits\"][\"hits\"]\n        for hit in hits:\n            # If your document content is stored in e.g. \"content\" field:\n            doc_text = hit[\"_source\"].get(\"content\", \"\")\n            docs.append(doc_text.strip())\n\n        return docs\n"
  },
  {
    "path": "retrievers/null_retriever.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## ai-building-blocks-agent/retrievers/null_retriever.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n \n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n\nclass NullRetriever:\n    \"\"\"\n    A retriever that always returns an empty list (i.e., no documents).\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def retrieve_documents(self, query: str, top_k: int) -> list:\n        \"\"\"\n        Always returns an empty list.\n        :param query: The search query.\n        :param top_k: The number of documents to 'retrieve'—in this case, none.\n        :return: Empty list.\n        \"\"\"\n        return []\n"
  },
  {
    "path": "scripts/__init__.py",
    "content": "#suite-cisco-ai-building-blocks/ai-building-blocks-agent/scripts/__init__.py\nimport sys, types\n "
  },
  {
    "path": "scripts/index_functions.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## AI-Building-Blocks-Agent/scripts/index_functions.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n\n\"\"\"\nIndex every *diet* function schema (or a single platform) into the vector-store\nselected for the FASTAPI layer.  Supports Chroma, Azure AI Search, and\nElasticsearch.\n\nExamples\n--------\n# one platform\npython -m scripts.index_functions --platform meraki\n\n# everything under app/llm/function_definitions\npython -m scripts.index_functions --all\n\"\"\"\n# ── stdlib ────────────────────────────────────────────────────────────────\nimport argparse\nimport importlib\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom types import ModuleType\n# ── third-party ───────────────────────────────────────────────────────────\nfrom dotenv import load_dotenv\n# ── internal ──────────────────────────────────────────────────────────────\nfrom scripts.utils.paths import ensure_abs_env   # new helper\n\n# ══════════════════════════════════════════════════════════════════════════\n# 0.  Environment & basic paths\n# ══════════════════════════════════════════════════════════════════════════\nload_dotenv()\n\nREPO_ROOT: Path = Path(__file__).resolve().parents[1]            # ai-building-blocks-agent/\nLLM_DIR     = REPO_ROOT / \"app\" / \"llm\"\nDIET_DIR    = LLM_DIR / \"function_definitions\"\nFULL_DIR    = LLM_DIR / \"openapi_specs\"\n\nBACKEND     = os.getenv(\"FASTAPI_VECTOR_BACKEND\", \"chroma\").lower()\n\n# ── guarantee absolute Chroma path (only when using Chroma) ───────────────\nif BACKEND == \"chroma\":\n    # layer-scoped DB path (repo-relative default)\n    ensure_abs_env(\"FASTAPI_CHROMA_DB_PATH\", \"chroma_dbs/fastapi\")\n\n# ══════════════════════════════════════════════════════════════════════════\n# 1.  Import the correct indexer implementation\n# ══════════════════════════════════════════════════════════════════════════\nINDEXER_MODULES: dict[str, str] = {\n    \"chroma\":  \"db_scripts.indexers.chroma_indexer\",\n    \"azure\":   \"db_scripts.indexers.azure_indexer\",\n    \"elastic\": \"db_scripts.indexers.elastic_indexer\",\n}\n\ntry:\n    Indexer = getattr(\n        importlib.import_module(INDEXER_MODULES[BACKEND]),\n        \"PlatformFunctionIndexer\"\n    )\nexcept KeyError as exc:\n    raise RuntimeError(\n        f\"Unsupported backend '{BACKEND}'. \"\n        f\"Choose from: {', '.join(INDEXER_MODULES)}.\"\n    ) from exc\n\n# ══════════════════════════════════════════════════════════════════════════\n# 2.  Index a single platform\n# ══════════════════════════════════════════════════════════════════════════\ndef index_one(platform: str) -> None:\n    diet_path = DIET_DIR  / f\"{platform}.json\"\n    full_path = FULL_DIR  / f\"full_{platform}.json\"\n\n    if not diet_path.exists():\n        raise FileNotFoundError(f\"Missing diet schema: {diet_path}\")\n    if not full_path.exists():\n        raise FileNotFoundError(f\"Missing full OpenAPI spec: {full_path}\")\n\n    diet_fns  = json.loads(diet_path.read_text())\n    full_spec = json.loads(full_path.read_text())\n\n    # Azure OpenAI: function names must be ≤ 64 characters\n    for fn in diet_fns:\n        if len(fn.get(\"name\", \"\")) > 64:\n            orig = fn[\"name\"]\n            fn[\"name\"] = orig[:64]\n            print(f\"[WARN] Truncated long function name: {orig!r} → {fn['name']!r}\")\n\n    # collection / index name selected from env for the chosen backend\n    if BACKEND == \"chroma\":\n        index_name = os.getenv(\"FASTAPI_CHROMA_COLLECTION_PLATFORM\",\n                               \"platform-summaries-index\")\n    elif BACKEND == \"azure\":\n        index_name = os.getenv(\"FASTAPI_AZURE_PLATFORM_INDEX\",\n                               \"platform-summaries-index\")\n    else:  # elastic\n        index_name = os.getenv(\"FASTAPI_ELASTIC_PLATFORM_INDEX\",\n                               \"platform-summaries-index\")\n\n    indexer = Indexer(index_name=index_name, layer_name=\"FASTAPI\")\n    print(f\"→ indexing {platform} …\", flush=True)\n    indexer.index_functions(platform, diet_fns, full_spec)\n    print(f\"✓ {platform}: {len(diet_fns)} functions indexed via {BACKEND}\")\n\n# ══════════════════════════════════════════════════════════════════════════\n# 3.  CLI entry-point\n# ══════════════════════════════════════════════════════════════════════════\ndef cli() -> None:\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--platform\", help=\"meraki, catalyst, …\")\n    ap.add_argument(\"--all\", action=\"store_true\",\n                    help=\"index every diet_*.json found\")\n    args = ap.parse_args()\n\n    if args.platform and args.all:\n        raise SystemExit(\"--platform and --all are mutually exclusive\")\n\n    if args.all:\n        platforms = [p.stem for p in DIET_DIR.glob(\"*.json\")]\n    elif args.platform:\n        platforms = [args.platform]\n    else:\n        raise SystemExit(\"Specify --platform <name> or --all\")\n\n    for plat in platforms:\n        index_one(plat)\n\nif __name__ == \"__main__\":\n    cli()\n"
  },
  {
    "path": "scripts/observability_setup.py",
    "content": "#suite-cisco-ai-building-blocks/ai-building-blocks-agent/scripts/observability_setup.py\nimport os, subprocess\n\n# 1. Define the content for Dockerfile (FastAPI app image)\ndockerfile_content = \"\"\"\\\nFROM python:3.11-slim\nWORKDIR /app\n# Install FastAPI app dependencies (including opentelemetry and prometheus)\nRUN pip install fastapi uvicorn[standard] prometheus-fastapi-instrumentator prometheus-client \\\\\n    opentelemetry-sdk opentelemetry-exporter-otlp-proto-http \\\\\n    opentelemetry-instrumentation-fastapi\nCOPY . .\nENV OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://tempo:4318}\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\"\"\"\n\n# 2. Define docker-compose.yaml content with FastAPI app, Prometheus, Tempo, Grafana\ncompose_content = \"\"\"\\\nservices:\n  fastapi-app:\n    build: .\n    container_name: fastapi-app\n    ports:\n      - \"8001:8000\"\n    environment:\n      - OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4318\n      - OTEL_SERVICE_NAME=fastapi-app\n    depends_on:\n      - tempo\n\n  prometheus:\n    image: prom/prometheus:latest\n    container_name: prometheus\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro\n    # <— removed depends_on here\n\n  tempo:\n    image: grafana/tempo:latest\n    container_name: tempo\n    command:\n      - \"-config.file=/etc/tempo.yaml\"\n    volumes:\n      - ./tempo.yaml:/etc/tempo.yaml:ro\n    ports:\n      - \"3200:3200\"\n      - \"4318:4318\"\n    depends_on:\n      - prometheus\n\n  grafana:\n    image: grafana/grafana:latest\n    container_name: grafana\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro\n    depends_on:\n      - prometheus\n      - tempo\n\"\"\"\n\n# 3. Prometheus configuration to scrape FastAPI and Tempo\nprom_config = \"\"\"\\\nglobal:\n  scrape_interval: 5s\nscrape_configs:\n  - job_name: 'fastapi-app'\n    scrape_interval: 5s\n    static_configs:\n      - targets: ['fastapi-app:8000']\n  - job_name: 'tempo'\n    scrape_interval: 15s\n    static_configs:\n      - targets: ['tempo:3200']\n\"\"\"\n\n# 4. Tempo configuration (OTLP receiver, in-memory storage)\ntempo_config = \"\"\"\\\n# tempo.yaml — Tempo v2.7+ all-in-one\ntarget: all\n\nserver:\n  http_listen_port: 3200   # Grafana queries this\n  grpc_listen_port: 9095   # internal gRPC (won’t conflict)\n\n# ── Add this back so Tempo will talk OTLP on 4317/4318 ───────────────────────\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: \"0.0.0.0:4317\"\n      http:\n        endpoint: \"0.0.0.0:4318\"\n\ndistributor:\n  receivers:\n    otlp:\n      protocols:\n        grpc: {}\n        http: {}\n\ningester:\n  lifecycler:\n    ring:\n      kvstore:\n        store: inmemory\n      replication_factor: 1\n\nstorage:\n  trace:\n    backend: local\n    local:\n      path: /tmp/tempo/traces\n\"\"\"\n\n\n# 5. Grafana data sources provisioning (Prometheus & Tempo)\ngrafana_datasources = \"\"\"\\\napiVersion: 1\ndatasources:\n- name: Prometheus\n  type: prometheus\n  access: proxy\n  url: http://prometheus:9090\n  isDefault: true\n- name: Tempo\n  type: tempo\n  access: proxy\n  url: http://tempo:3200\n  isDefault: false\n\"\"\"\n# This provisioning file tells Grafana to automatically add Prometheus and Tempo as data sources.\n\n# Create the files in the current directory\nwith open(\"Dockerfile\", \"w\") as f:\n    f.write(dockerfile_content)\nwith open(\"docker-compose.yaml\", \"w\") as f:\n    f.write(compose_content)\nwith open(\"prometheus.yml\", \"w\") as f:\n    f.write(prom_config)\nwith open(\"tempo.yaml\", \"w\") as f:\n    f.write(tempo_config)\nwith open(\"grafana-datasources.yml\", \"w\") as f:\n    f.write(grafana_datasources)\n\n# 6. Build and start all containers using Docker Compose\nsubprocess.run([\"docker\", \"compose\", \"up\", \"-d\"], check=True)\nprint(\"Docker Compose services are up and running.\")\n"
  },
  {
    "path": "scripts/platform_scaffolder.py",
    "content": "#!/usr/bin/env python3\n\"\"\"\nUnified Platform Scaffolder – phase-1\nAUTO-GENERATES:\n\n* app/llm/function_definitions/<platform>.json\n* app/llm/openapi_specs/full_<platform>.json\n* app/llm/platform_clients/<platform>_client.py\n* app/llm/function_dispatcher/<platform>_dispatcher.py\n* app/llm/unified_service/<platform>_service.py\n\"\"\"\nfrom __future__ import annotations\nimport sys\nfrom pathlib import Path\n\nREPO_ROOT = Path(__file__).resolve().parents[1]\nif str(REPO_ROOT) not in sys.path:\n    sys.path.insert(0, str(REPO_ROOT))\n\nimport argparse\nimport json\nimport logging\nimport re\nimport textwrap\nfrom typing import Dict, List\n\nfrom dotenv import load_dotenv\n\n# ───────── helpers ─────────────────────────────────────────────────────\nfrom scripts.utils.openapi_loader import load_spec\nfrom scripts.utils.sdk_loader      import load_client\nfrom scripts.utils.dietify         import dietify_schema\n\n# ───────── constants ───────────────────────────────────────────────────\nload_dotenv()\n\nROOT = REPO_ROOT\nLLM_DIR = ROOT / \"app\" / \"llm\"\n\nOUT_DIRS = {\n    \"diet\":    LLM_DIR / \"function_definitions\",\n    \"full\":    LLM_DIR / \"openapi_specs\",\n    \"client\":  LLM_DIR / \"platform_clients\",\n    \"disp\":    LLM_DIR / \"function_dispatcher\",\n    \"service\": LLM_DIR / \"unified_service\",\n}\n\nfor p in OUT_DIRS.values():\n    p.mkdir(parents=True, exist_ok=True)\n\nlogging.basicConfig(level=logging.INFO, format=\"%(levelname)s | %(message)s\")\nlog = logging.getLogger(\"scaffolder\")\n\n# ╭─────────────────────────────────────────────────────────────────────╮\n# │ 1 ─ package initialisation helpers                                 │\n# ╰─────────────────────────────────────────────────────────────────────╯\nDISPATCHER_INIT = OUT_DIRS[\"disp\"] / \"__init__.py\"\nFUNCS_INIT = OUT_DIRS[\"diet\"] / \"__init__.py\"\n\n# 1-a  dispatcher registry (smart Meraki fallback)\nif not DISPATCHER_INIT.exists():\n    DISPATCHER_INIT.write_text(textwrap.dedent(\"\"\"\\\n        \\\"\\\"\\\"\n        Decorator-based dispatcher registry **plus** smart Meraki fallback.\n        AUTO-GENERATED – DO NOT EDIT MANUALLY.\n        \\\"\\\"\\\"\n        from __future__ import annotations\n        import importlib\n        import os\n        from pathlib import Path\n        from typing import Any, Callable, Dict\n        from meraki import DashboardAPI\n\n        _registry: Dict[str, Callable[..., Any]] = {}\n\n        def register(name: str):\n            def _decorator(fn: Callable[..., Any]):\n                _registry[name] = fn\n                return fn\n            return _decorator\n\n        # auto-import sub-dispatchers so their @register executes\n        _pkg_path = Path(__file__).parent\n        for _p in _pkg_path.glob('*_dispatcher.py'):\n            if _p.name != '__init__.py':\n                importlib.import_module(f'{__name__}.{_p.stem}')\n\n        # Meraki SDK fallback\n        def _call_meraki(fname: str, kwargs: Dict[str, Any]):\n            api_key = os.getenv('MERAKI_DASHBOARD_API_KEY')\n            if not api_key:\n                raise ValueError('Meraki dispatch failed: missing MERAKI_DASHBOARD_API_KEY')\n\n            dash = DashboardAPI(api_key=api_key,\n                                suppress_logging=True,\n                                print_console=True)\n\n            if hasattr(dash, fname):\n                return getattr(dash, fname)(**kwargs)\n\n            for attr in dir(dash):\n                if attr.startswith('_'):\n                    continue\n                sub = getattr(dash, attr)\n                if hasattr(sub, fname):\n                    return getattr(sub, fname)(**kwargs)\n\n            raise AttributeError(f'No Meraki SDK method {fname!r} found')\n\n        def dispatch_function_call(name: str, arguments: Dict[str, Any]):\n            if name in _registry:\n                return _registry[name](**arguments)\n            return _call_meraki(name, arguments)\n\n        __all__ = ['dispatch_function_call', 'register']\n    \"\"\"), encoding=\"utf-8\")\n    log.info(\"✓ %s\", DISPATCHER_INIT.relative_to(ROOT))\n\n# 1-b  function-definitions loader\nif not FUNCS_INIT.exists():\n    FUNCS_INIT.write_text(textwrap.dedent(f\"\"\"\\\n        # Auto-generated – DO NOT EDIT\n        # {FUNCS_INIT.relative_to(ROOT)}\n        \\\"\\\"\\\"\n        Loads every *.json in this folder into FUNCTION_DEFINITIONS\n            {{ '<platform>': [{{…}}, … ] , … }}\n        \\\"\\\"\\\"\n        from __future__ import annotations\n        import json\n        from pathlib import Path\n        from typing import Dict, List, Any\n\n        _DIR = Path(__file__).parent\n        FUNCTION_DEFINITIONS: Dict[str, List[Dict[str, Any]]] = {{}}\n\n        for _fp in _DIR.glob('*.json'):\n            try:\n                FUNCTION_DEFINITIONS[_fp.stem] = json.loads(_fp.read_text(encoding='utf-8'))\n            except Exception as exc:\n                print(f'[function_definitions] ⚠️  skipped {{_fp.name}}: {{exc}}')\n\n        __all__ = ['FUNCTION_DEFINITIONS']\n    \"\"\"), encoding=\"utf-8\")\n    log.info(\"✓ %s\", FUNCS_INIT.relative_to(ROOT))\n\n# 1-c  empty __init__.py for every other folder\nfor folder in OUT_DIRS.values():\n    init_py = folder / \"__init__.py\"\n    if not init_py.exists():\n        init_py.write_text(\n            f\"# Auto-generated – DO NOT EDIT\\n# {init_py.relative_to(ROOT)}\\n\",\n            encoding=\"utf-8\",\n        )\n        log.info(\"✓ %s\", init_py.relative_to(ROOT))\n\n# ╭─────────────────────────────────────────────────────────────────────╮\n# │ 2 ─ utility functions                                              │\n# ╰─────────────────────────────────────────────────────────────────────╯\ndef _write_json(p: Path, obj, *, pretty: bool = False):\n    p.write_text(\n        json.dumps(obj, indent=2 if pretty else None, separators=(\",\", \":\")),\n        encoding=\"utf-8\",\n    )\n    log.info(\"✓ %s\", p.relative_to(ROOT))\n\n_identifier_rx = re.compile(r\"[^0-9A-Za-z_]\")\ndef _py_identifier(raw: str, seen: Dict[str, int]) -> str:\n    \"\"\"\n    Convert *raw* into a valid Python identifier and guarantee uniqueness\n    within a dispatcher file using the *seen* registry.\n    \"\"\"\n    ident = _identifier_rx.sub(\"_\", raw)\n    if ident[0].isdigit():\n        ident = f\"op_{ident}\"\n    # prevent collisions if two different raw names normalise identically\n    if ident in seen:\n        seen[ident] += 1\n        ident = f\"{ident}_{seen[ident]}\"\n    else:\n        seen[ident] = 0\n    return ident\n\n\n# ── helpers ────────────────────────────────────────────────────────────────\ndef _emit_client_stub(platform: str, sdk_module: str) -> None:\n    \"\"\"\n    Write app/llm/platform_clients/<platform>_client.py\n    – searches both root *and* first-level sub-clients for attributes.\n    \"\"\"\n    sdk_cls = load_client(sdk_module)                # e.g. meraki.DashboardAPI\n\n    # ── Meraki-specific env guard\n    extra_imports, extra_init = [], []\n    if platform.lower() == \"meraki\":\n        extra_imports.append(\"import os\")\n        extra_init.extend([\n            \"api_key = os.getenv('CISCO_MERAKI_API_KEY')\",\n            \"if not api_key:\",\n            \"    raise ValueError('Missing CISCO_MERAKI_API_KEY environment variable')\",\n            \"kwargs['api_key'] = api_key\",\n            \"\",\n        ])\n\n    lines: list[str] = [\n        f\"# {(OUT_DIRS['client'] / f'{platform}_client.py').relative_to(ROOT)}\",\n        \"# Auto-generated – DO NOT EDIT\",\n        f\"import {sdk_module} as _sdk\",\n        *extra_imports,\n        \"\",\n        f\"class {platform.capitalize()}Client:\",\n        f\"    \\\"\\\"\\\"Thin wrapper around `{sdk_cls.__name__}` with fuzzy attribute lookup.\\\"\\\"\\\"\",\n        \"\",\n        \"    def __init__(self, **kwargs):\",\n        *[f\"        {l}\" for l in extra_init],\n        f\"        self._sdk = _sdk.{sdk_cls.__name__}(**kwargs)\",\n\n        \"\",\n        \"    def __getattr__(self, item):\",\n        \"        # ① direct attribute on DashboardAPI\",\n        \"        if hasattr(self._sdk, item):\",\n        \"            return getattr(self._sdk, item)\",\n        \"\",\n        \"        # ② first-level sub-clients (organizations, networks, …)\",\n        \"        for name in dir(self._sdk):\",\n        \"            if name.startswith('_'):\",\n        \"                continue\",\n        \"            sub = getattr(self._sdk, name)\",\n        \"            if hasattr(sub, item):\",\n        \"                return getattr(sub, item)\",\n        \"\",\n        \"        raise AttributeError(f\\\"{self.__class__.__name__} has no attribute {item!r}\\\")\",\n        \"\",\n    ]\n\n    fp = OUT_DIRS['client'] / f'{platform}_client.py'\n    fp.write_text('\\n'.join(lines), encoding='utf-8')\n    log.info('✓ %s', fp.relative_to(ROOT))\n\n\n\n\n\ndef _emit_unified_service(platform: str):\n    code = textwrap.dedent(f\"\"\"\n        # Auto-generated – DO NOT EDIT\n        from app.llm.platform_clients.{platform}_client import {platform.capitalize()}Client\n\n        class {platform.capitalize()}ServiceClient:\n            \\\"\\\"\\\"Generic call-through service used by FastAPI.\\\"\\\"\\\"\n\n            def __init__(self, **sdk_kwargs):\n                self.client = {platform.capitalize()}Client(**sdk_kwargs)\n\n            def call(self, function_name: str, **kwargs):\n                try:\n                    method = getattr(self.client, function_name)\n                except AttributeError:\n                    raise ValueError(\n                        f\"No such method '{{function_name}}' on {platform.capitalize()}Client\"\n                    )\n                return method(**kwargs)\n    \"\"\").strip()\n\n    fp = OUT_DIRS[\"service\"] / f\"{platform}_service.py\"\n    fp.write_text(f\"# {fp.relative_to(ROOT)}\\n\\n{code}\", encoding=\"utf-8\")\n    log.info(\"✓ %s\", fp.relative_to(ROOT))\n\n\n \n\n# ╭─────────────────────────────────────────────────────────────────────╮\n# │ 3 ─ per-platform scaffolding                                       │\n# ╰─────────────────────────────────────────────────────────────────────╯\ndef scaffold_one(\n    platform: str,\n    sdk_module: str,\n    spec_path: Path,\n    include_http: set[str] | None,\n    name_re: re.Pattern | None,\n) -> None:\n\n    full_spec = load_spec(spec_path)\n\n    _write_json(OUT_DIRS[\"full\"] / f\"{platform}.json\", full_spec, pretty=True)\n    _write_json(OUT_DIRS[\"full\"] / f\"full_{platform}.json\", full_spec, pretty=True)\n\n    diet_fns: List[dict] = []\n    safe_name_seen: Dict[str, int] = {}\n\n    for path, path_item in full_spec.get(\"paths\", {}).items():\n        for verb, op in path_item.items():\n            if include_http and verb.upper() not in include_http:\n                continue\n\n            op_id = op.get(\"operationId\") or f\"{verb}_{path}\"\n            if name_re and not name_re.search(op_id):\n                continue\n\n            schema = {\n                \"name\": op_id,\n                \"description\": op.get(\"summary\") or op.get(\"description\", \"\"),\n                \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []},\n            }\n            for p in op.get(\"parameters\", []):\n                # skip any unresolved $ref or otherwise anonymous parameter\n                if \"name\" not in p:\n                    log.warning(\"Skipping parameter without name: %r\", p)\n                    continue\n\n                name = p[\"name\"]\n                schema[\"parameters\"][\"properties\"][name] = {\n                    \"type\": p.get(\"schema\", {}).get(\"type\", \"string\"),\n                    \"description\": p.get(\"description\", \"\"),\n                }\n                if p.get(\"required\"):\n                    schema[\"parameters\"][\"required\"].append(name)\n\n\n            diet_fns.append(dietify_schema(schema))\n\n    _write_json(OUT_DIRS[\"diet\"] / f\"{platform}.json\", diet_fns)\n\n    _emit_client_stub(platform, sdk_module)\n    _emit_unified_service(platform)\n\n    disp_fp = OUT_DIRS[\"disp\"] / f\"{platform}_dispatcher.py\"\n    lines: List[str] = [\n        f\"# {disp_fp.relative_to(ROOT)}\",\n        \"from app.llm.function_dispatcher import register\",\n        f\"from app.llm.platform_clients.{platform}_client import {platform.capitalize()}Client\",\n        \"\",\n    ]\n    for fn in diet_fns:\n        safe_name = _py_identifier(fn[\"name\"], safe_name_seen)\n        lines.append(\n            f\"@register('{fn['name']}')\\n\"\n            f\"def {safe_name}(**kwargs):\\n\"\n            f\"    return {platform.capitalize()}Client().{safe_name}(**kwargs)\\n\"\n        )\n\n    disp_fp.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n    log.info(\"✓ %s\", disp_fp.relative_to(ROOT))\n\n# ╭─────────────────────────────────────────────────────────────────────╮\n# │ 4 ─ CLI                                                            │\n# ╰─────────────────────────────────────────────────────────────────────╯\ndef _parse_cli():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--platform\")\n    ap.add_argument(\"--sdk-module\")\n    ap.add_argument(\"--openapi-spec\")\n    ap.add_argument(\"--include-http-methods\")\n    ap.add_argument(\"--name-pattern\")\n    ap.add_argument(\"--all\", action=\"store_true\",\n                    help=\"Scaffold for every full_*.json already present\")\n    return ap.parse_args()\n\ndef main():\n    args = _parse_cli()\n\n    if args.all and (args.platform or args.sdk_module or args.openapi_spec):\n        raise SystemExit(\"--all cannot be used with other flags\")\n\n    if args.all:\n        platforms = [\n            p.stem.replace(\"full_\", \"\") for p in OUT_DIRS[\"full\"].glob(\"full_*.json\")\n        ]\n    else:\n        if not (args.platform and args.sdk_module and args.openapi_spec):\n            raise SystemExit(\"Need --platform, --sdk-module, and --openapi-spec\")\n        platforms = [args.platform]\n\n    include_http = (\n        {m.upper() for m in args.include_http_methods.split(\",\")}\n        if args.include_http_methods\n        else None\n    )\n    name_re = re.compile(args.name_pattern) if args.name_pattern else None\n\n    for plat in platforms:\n        spec = (\n            Path(args.openapi_spec)\n            if args.openapi_spec\n            else OUT_DIRS[\"full\"] / f\"{plat}.json\"\n        )\n        log.info(\"⏳ Scaffolding %s …\", plat)\n        scaffold_one(plat, args.sdk_module, spec, include_http, name_re)\n\n    log.info(\"✅ DONE – all artefacts generated\")\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "path": "scripts/reset_automated_folders.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n###########################################################################################\n## suite-cisco-ai-building-blocks/ai-building-blocks-agent/tools/reset_automated_folders.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n###########################################################################################\n\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis script can either:\n1. Wipe and recreate all auto-generated LLM folders (function_definitions, openapi_specs,\n   platform_clients, function_dispatcher, unified_service).\n2. Remove all artifacts for a single platform across those same folders.\n\nLaunch it from anywhere; it always operates on the folders inside the\nai-building-blocks-agent repo and prints what it deleted / rebuilt.\n\"\"\"\n\nimport sys\nimport os\nimport shutil\nfrom pathlib import Path\n\n# ── repo root discovery so script works from any cwd ──────────────────────\nREPO_ROOT = Path(__file__).resolve().parents[1]         # …/ai-building-blocks-agent\nif str(REPO_ROOT) not in sys.path:\n    sys.path.insert(0, str(REPO_ROOT))\n\n# ── target folders (absolute paths) ───────────────────────────────────────\nFOLDERS_TO_RESET = [\n    REPO_ROOT / \"app\" / \"llm\" / \"function_definitions\",\n    REPO_ROOT / \"app\" / \"llm\" / \"openapi_specs\",\n    REPO_ROOT / \"app\" / \"llm\" / \"platform_clients\",\n    REPO_ROOT / \"app\" / \"llm\" / \"function_dispatcher\",\n    REPO_ROOT / \"app\" / \"llm\" / \"unified_service\",\n]\n\ndef reset_folders(paths: list[Path]) -> None:\n    \"\"\"Delete each folder if present, then recreate it empty.\"\"\"\n    for folder in paths:\n        if folder.exists():\n            print(f\"Deleting  → {folder.relative_to(REPO_ROOT)}\")\n            shutil.rmtree(folder)\n        else:\n            print(f\"Not found  → {folder.relative_to(REPO_ROOT)} (skip delete)\")\n\n        folder.mkdir(parents=True, exist_ok=True)\n        print(f\"Recreated → {folder.relative_to(REPO_ROOT)}\\n\")\n\n\nif __name__ == \"__main__\":\n    # Discover available platforms by looking at *_client.py in platform_clients\n    clients_dir = REPO_ROOT / \"app\" / \"llm\" / \"platform_clients\"\n    platform_files = list(clients_dir.glob(\"*_client.py\"))\n    platforms = sorted(\n        p.stem[:-len(\"_client\")]\n        for p in platform_files\n        if p.stem.endswith(\"_client\")\n    )\n\n    if not platforms:\n        print(\"No platforms found in platform_clients; resetting all folders.\")\n        reset_folders(FOLDERS_TO_RESET)\n        print(\"✅ All automated folders have been reset.\")\n        sys.exit(0)\n\n    # Prompt user for choice\n    print(\"Select platform to reset:\")\n    for idx, name in enumerate(platforms, start=1):\n        print(f\"  {idx}. {name}\")\n    all_option = len(platforms) + 1\n    print(f\"  {all_option}. All\")\n\n    choice = input(f\"Select [1-{all_option}]: \").strip()\n    try:\n        sel = int(choice)\n    except ValueError:\n        print(f\"Invalid selection: {choice}\")\n        sys.exit(1)\n\n    if sel == all_option:\n        # Reset all folders as before\n        reset_folders(FOLDERS_TO_RESET)\n        print(\"✅ All automated folders have been reset.\")\n    elif 1 <= sel <= len(platforms):\n        plat = platforms[sel - 1]\n        print(f\"→ Removing artifacts for platform '{plat}' …\\n\")\n        for folder in FOLDERS_TO_RESET:\n            rel = folder.relative_to(REPO_ROOT)\n            fn = folder.name\n            if fn == \"platform_clients\":\n                target = folder / f\"{plat}_client.py\"\n                if target.exists():\n                    print(f\"Deleting  → {rel}/{target.name}\")\n                    target.unlink()\n                else:\n                    print(f\"Not found  → {rel}/{target.name} (skip)\")\n            elif fn == \"function_definitions\":\n                target = folder / f\"{plat}.json\"\n                if target.exists():\n                    print(f\"Deleting  → {rel}/{target.name}\")\n                    target.unlink()\n                else:\n                    print(f\"Not found  → {rel}/{target.name} (skip)\")\n            elif fn == \"openapi_specs\":\n                for name in (f\"{plat}.json\", f\"full_{plat}.json\"):\n                    target = folder / name\n                    if target.exists():\n                        print(f\"Deleting  → {rel}/{target.name}\")\n                        target.unlink()\n                    else:\n                        print(f\"Not found  → {rel}/{target.name} (skip)\")\n            elif fn == \"function_dispatcher\":\n                target = folder / f\"{plat}_dispatcher.py\"\n                if target.exists():\n                    print(f\"Deleting  → {rel}/{target.name}\")\n                    target.unlink()\n                else:\n                    print(f\"Not found  → {rel}/{target.name} (skip)\")\n            elif fn == \"unified_service\":\n                target = folder / f\"{plat}_service.py\"\n                if target.exists():\n                    print(f\"Deleting  → {rel}/{target.name}\")\n                    target.unlink()\n                else:\n                    print(f\"Not found  → {rel}/{target.name} (skip)\")\n        print(f\"\\n✅ Platform '{plat}' artifacts have been removed.\")\n    else:\n        print(f\"Invalid selection: {choice}\")\n        sys.exit(1)\n"
  },
  {
    "path": "scripts/utils/__init__.py",
    "content": "#suite-cisco-ai-building-blocks/ai-building-blocks-agent/scripts/utils/__init__.py\n"
  },
  {
    "path": "scripts/utils/dietify.py",
    "content": "def dietify_schema(full_schema: dict) -> dict:\n    \"\"\"\n    Return a *diet* version of an OpenAPI function schema:\n    – keep only primitive params (string / integer / number / boolean / array)\n    – drop examples, defaults, complex objects to save tokens\n    \"\"\"\n    keep = {\"string\", \"integer\", \"number\", \"boolean\", \"array\"}\n\n    full_parameters = full_schema.get(\"parameters\", {}).get(\"properties\", {})\n    slimmed = {\n        k: v\n        for k, v in full_parameters.items()\n        if v.get(\"type\") in keep\n    }\n\n    diet = full_schema.copy()\n    diet[\"parameters\"][\"properties\"] = slimmed\n    return diet\n"
  },
  {
    "path": "scripts/utils/openapi_loader.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## AI-Building-Blocks-Agent/scripts/utils/openapi_loader.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n\"\"\"\nopenapi_loader.py\n\nTiny helper that loads an OpenAPI document into a Python ``dict``.\n\n• Accepts **JSON** (``*.json``) **and YAML** (``*.yml`` / ``*.yaml``).  \n• Requires ``PyYAML`` for YAML files – install once with ``pip install pyyaml``.\n\"\"\"\n\nfrom pathlib import Path\nimport json\nfrom typing import Any, Dict\n\n\ndef _load_yaml(text: str) -> Dict[str, Any]:\n    try:\n        import yaml  # type: ignore\n    except ModuleNotFoundError as exc:\n        raise RuntimeError(\n            \"YAML spec detected but PyYAML is not installed.\\n\"\n            \"Fix:  pip install pyyaml\"\n        ) from exc\n    return yaml.safe_load(text)\n\n\ndef load_spec(path: Path) -> dict:\n    \"\"\"\n    Read *path* and return the parsed OpenAPI spec as a dictionary.\n\n    Parameters\n    ----------\n    path : pathlib.Path\n        File ending in .json, .yaml or .yml.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the path doesn’t exist.\n    ValueError\n        If the file extension is unrecognised.\n    RuntimeError\n        If a YAML file is supplied but PyYAML is missing.\n    \"\"\"\n    if not path.exists():\n        raise FileNotFoundError(path)\n\n    ext = path.suffix.lower()\n    text = path.read_text(encoding=\"utf-8\")\n\n    if ext == \".json\":\n        return json.loads(text)\n    if ext in {\".yaml\", \".yml\"}:\n        return _load_yaml(text)\n\n    raise ValueError(f\"Unsupported file extension {ext!r} for {path}\")\n"
  },
  {
    "path": "scripts/utils/paths.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## AI-Building-Blocks-Agent/scripts/utils/paths.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n\"\"\"\nPath-handling helpers shared across layers.\n\n* ``ensure_abs_env(var, default_rel)``  \n  Makes sure *var* exists and is an **absolute** path, resolving any relative\n  value against the repo root (parent of ``ai-building-blocks-agent``).\n\n* ``get_chroma_root(layer_env: str, default_rel: str = \"chroma_dbs\")``  \n  Returns the absolute path for *this layer’s* Chroma collections, based solely\n  on the per-layer env-var (no more legacy ``CHROMA_DB_ROOT``).\n\n* ``get_dynamic_cache_dir()``  \n  Ensures and returns the directory pointed to by\n  ``PLATFORM_DYNAMIC_CACHE_PATH`` (or its default).\n\"\"\"\n\nfrom pathlib import Path\nimport os\nfrom typing import Final\n\n# ────────────────────────────────────────────────────────────────────────────\nREPO_ROOT: Final[Path] = Path(__file__).resolve().parents[2]  # ../../\n\n# ╔═════════════════════════════════════════════════════════════════════════╗\n# 1.  Generic helper\n# ╚═════════════════════════════════════════════════════════════════════════╝\ndef ensure_abs_env(var: str, default_rel: str) -> Path:\n    raw = os.getenv(var, default_rel)\n    p   = Path(raw).expanduser()\n    if not p.is_absolute():\n        p = (REPO_ROOT / p).resolve()\n    os.environ[var] = str(p)\n    return p\n\n# ╔═════════════════════════════════════════════════════════════════════════╗\n# 2.  Chroma collections (per layer)\n# ╚═════════════════════════════════════════════════════════════════════════╝\ndef get_chroma_root(layer_env: str, default_rel: str = \"chroma_dbs\") -> Path:\n    \"\"\"\n    Resolve the folder that stores *layer-specific* Chroma collections.\n\n    Parameters\n    ----------\n    layer_env:\n        The env-var name, e.g. ``FASTAPI_CHROMA_DB_PATH``.\n    default_rel:\n        Fallback relative path (resolved under the repo root).\n\n    Returns\n    -------\n    pathlib.Path\n        Absolute path to the collection directory.\n    \"\"\"\n    return ensure_abs_env(layer_env, default_rel)\n\n# ╔═════════════════════════════════════════════════════════════════════════╗\n# 3.  Platform-function dynamic cache\n# ╚═════════════════════════════════════════════════════════════════════════╝\ndef get_dynamic_cache_dir() -> Path:\n    cache = ensure_abs_env(\n        \"PLATFORM_DYNAMIC_CACHE_PATH\",\n        \"ai-building-blocks-agent/app/platform_dynamic_cache\",\n    )\n    Path(cache).mkdir(parents=True, exist_ok=True)\n    return Path(cache)\n"
  },
  {
    "path": "scripts/utils/sdk_loader.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n################################################################################\n## AI-Building-Blocks-Agent/scripts/utils/sdk_loader.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\n\n\"\"\"\nUtility to locate the main *client* class in a vendor SDK so that the scaffolder\ncan wrap it automatically.\n\nMatching rules — in order of preference — are:\n\n1.  Class name ends with **DashboardAPI**        (e.g. ``DashboardAPI``)\n2.  Class name ends with **Client**              (e.g. ``MerakiClient``)\n3.  Class name ends with **API**                 (e.g. ``DNACenterAPI``)\n4.  Class name ends with **Session**             (e.g. ``ManagerSession``)\n\nThe comparison is case-insensitive.  The first match encountered is returned.\n\"\"\"\n\nimport importlib\nimport inspect\nfrom types import ModuleType\nfrom typing import Type, Any\n\n\ndef load_client(module_name: str) -> Type[Any]:\n    \"\"\"\n    Dynamically import *module_name* and return the first class that looks like\n    an SDK entry-point according to the rules above.\n\n    Parameters\n    ----------\n    module_name : str\n        The fully-qualified name of the SDK module, e.g. ``\"meraki\"``,\n        ``\"dnacentersdk\"``.\n\n    Raises\n    ------\n    RuntimeError\n        If no suitable client class is found.\n\n    Examples\n    --------\n    >>> load_client(\"meraki\").__name__\n    'DashboardAPI'\n    >>> load_client(\"dnacentersdk\").__name__\n    'DNACenterAPI'\n    \"\"\"\n    mod: ModuleType = importlib.import_module(module_name)\n\n    for attr in dir(mod):\n        obj = getattr(mod, attr)\n        if not inspect.isclass(obj):\n            continue\n\n        lower_name = attr.lower()\n        if lower_name.endswith(\"dashboardapi\") or lower_name.endswith(\"client\") or lower_name.endswith(\"api\") or lower_name.endswith(\"session\"):\n            return obj\n\n    raise RuntimeError(f\"No SDK client found in {module_name!r}\")\n"
  },
  {
    "path": "static/index.html",
    "content": "\n<!--\n################################################################################\n## ai-building-blocks-agent/static/index.html\n## Copyright (c) 2025 Cisco Systems\n## Licensed under the Apache License, Version 2.0  (see LICENSE)\n################################################################################\nDISCLAIMER: USE AT YOUR OWN RISK – see repository README for details.\n-->\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\" />\n  <title>Cisco AI Building Blocks AI Agent</title>\n  <link rel=\"icon\" type=\"image/x-icon\" href=\"/assets/favicon.ico\" />\n  <style>\n    body{font-family:Arial,Helvetica,sans-serif;margin:0;background:#f6f6f6;color:#333}\n    header{display:flex;align-items:center;padding:10px 20px;background:#005073;color:#fff}\n    header img{height:40px;margin-right:15px}\n    header h1{font-size:1.5rem;margin:0}\n    .container{width:90%;margin:40px auto;background:#fff;border-radius:8px;box-shadow:0 0 10px rgba(0,0,0,.1)}\n    #messages{padding:20px;height:65vh;overflow-y:auto;line-height:1.4;border-bottom:1px solid #ccc}\n    #messages div{margin-bottom:10px}\n    #messages .user{color:#007DAA;font-weight:bold}\n    #messages .assistant{color:#333}\n    .assistant-label{font-weight:bold;color:#007DAA;margin-bottom:5px}\n    table{width:100%;border-collapse:collapse}\n    th,td{border:1px solid #ccc;padding:8px;text-align:left}\n    th{background:#005073!important;color:#fff!important}\n    .collapsible{background:#f9f9f9;color:#333;cursor:pointer;padding:10px;border:1px solid #ccc;text-align:left;font-size:16px;margin-bottom:5px}\n    .collapsible.active{background:#e9e9e9}\n    .content{display:none;background:#f1f1f1;border:1px solid #ccc;margin-bottom:10px;padding:0 15px}\n    #input-area{display:flex;padding:10px 20px;align-items:center;gap:10px;background:#f1f1f1}\n    #user-input{flex:1;padding:8px;font-size:1rem}\n    button{background:#005073;color:#fff;border:none;padding:8px 16px;font-size:1rem;border-radius:4px;cursor:pointer}\n    button:hover{background:#007DAA}\n    pre{white-space:pre-wrap;margin:10px 0;padding:10px;background:#f6f6f6;border:1px solid #ccc}\n  </style>\n</head>\n<body>\n<header>\n  <img src=\"/assets/Cisco_Logo_White.png\" alt=\"Cisco Logo\" />\n  <h1>Cisco AI Building Blocks AI Agent<h1>\n</header>\n\n<div class=\"container\">\n  <div id=\"messages\"></div>\n  <div id=\"input-area\">\n    <input type=\"text\" id=\"user-input\" placeholder=\"Type a message...\" autocomplete=\"off\" />\n    <button id=\"send-btn\">Send</button>\n    <button id=\"clear-btn\">🧹</button>\n    <button id=\"toggle-btn\">Toggle View</button>\n    <button id=\"download-btn\">Download JSON</button>\n  </div>\n</div>\n\n<script>\n/* ---------- DOM refs --------------------------------------------------- */\nconst messagesDiv = document.getElementById('messages');\nconst userInput    = document.getElementById('user-input');\nconst sendBtn      = document.getElementById('send-btn');\nconst clearBtn     = document.getElementById('clear-btn');\nconst toggleBtn    = document.getElementById('toggle-btn');\nconst downloadBtn  = document.getElementById('download-btn');\n\n/* ---------- Helpers ---------------------------------------------------- */\nconst API_ROOT = window.location.pathname.replace(/\\/$/, ''); // \"\" or \"/some/prefix\"\n\nfunction removeTripleBackticks(str){return String(str).replaceAll(\"```\",\"\");}\n\n/** Convert raw text|json|markdown to nice HTML (collapsibles, tables, etc.). */\nfunction parseResponseToHtml(responseText){\n  responseText = removeTripleBackticks(responseText);\n  const trimmed = String(responseText).trim();\n\n  /* A. Raw HTML – show as-is */\n  if(trimmed.startsWith(\"<\") || /<(strong|p|h[1-6]|table|div|ul|li)\\b/i.test(trimmed)){\n    return trimmed;\n  }\n\n  /* B. JSON (object or array) */\n  if(/^[\\[{]/.test(trimmed)){\n    try{\n      const json = JSON.parse(trimmed);\n      return Array.isArray(json) ? createHtmlTable(json) : createCollapsibleHtml(json);\n    }catch{ /* fallthrough */ }\n  }\n\n  /* C. Markdown lite */\n  if(trimmed.includes(\"**\") || trimmed.includes(\"- \")){ return formatMarkdownAsHtml(trimmed); }\n\n  /* D. Plain text fallback */\n  return `<p>${trimmed}</p>`;\n}\n\nfunction createCollapsibleHtml(obj, lvl=0){\n  let html=\"\";\n  for(const [k,v] of Object.entries(obj)){\n    if(typeof v===\"object\" && v!==null){\n      html += `<button class=\"collapsible\">${'&nbsp;'.repeat(lvl*2)}${k}</button>`;\n      html += `<div class=\"content\">${createCollapsibleHtml(v, lvl+1)}</div>`;\n    }else{\n      html += `<p>${'&nbsp;'.repeat(lvl*2)}<strong>${k}:</strong> ${v}</p>`;\n    }\n  }\n  return html;\n}\n\nfunction createHtmlTable(arr){\n  if(!Array.isArray(arr)||!arr.length){return \"<p>No data.</p>\";}\n  const heads = Object.keys(arr[0]);\n  let h=`<table><thead><tr>${heads.map(h=>`<th>${h}</th>`).join(\"\")}</tr></thead><tbody>`;\n  arr.forEach(r=>{\n    h+=\"<tr>\"+heads.map(c=>`<td>${r[c]??\"\"}</td>`).join(\"\")+\"</tr>\";\n  });\n  return h+\"</tbody></table>\";\n}\n\nfunction formatMarkdownAsHtml(md){\n  return md.split(\"\\n\").map(line=>{\n    if(line.startsWith(\"**\"))          return `<p><strong>${line.replace(/\\*\\*/g,\"\")}</strong></p>`;\n    else if(line.startsWith(\"-\"))      return `<li>${line.slice(1).trim()}</li>`;\n    else                               return `<p>${line}</p>`;\n  }).join(\"\");\n}\n\n/* ---------- Chat logic -------------------------------------------------- */\nlet isHtmlView = true;\nconst chatHistory = [];\n\nasync function sendMessage(){\n  const query = userInput.value.trim();\n  if(!query) return;\n\n  /* user bubble */\n  messagesDiv.insertAdjacentHTML(\"beforeend\", `<div class=\"user\">User: ${query}</div>`);\n  userInput.value = \"\";\n\n  try{\n    const res = await fetch(`${API_ROOT}/chat`, {\n      method:\"POST\",\n      headers:{'Content-Type':'application/json'},\n      body:JSON.stringify({message:query})\n    });\n    const data  = await res.json();\n\n    const label = data.label ?? \"Assistant\";\n    const html  = parseResponseToHtml(data.response ?? \"No response.\");\n    const json  = JSON.stringify(data,null,2);\n    chatHistory.push({html,json});\n\n    const msgHtml = `<div class=\"assistant\">\n                       <div class=\"assistant-label\">${label}:</div>\n                       ${isHtmlView ? html : `<pre>${json}</pre>`}\n                     </div>`;\n    messagesDiv.insertAdjacentHTML(\"beforeend\", msgHtml);\n    messagesDiv.scrollTop = messagesDiv.scrollHeight;\n  }catch(err){ console.error(\"Fetch error:\", err); }\n}\n\n/* ---------- UI bindings ------------------------------------------------- */\nsendBtn  .addEventListener(\"click\", sendMessage);\nuserInput.addEventListener(\"keypress\", e=>{if(e.key===\"Enter\") sendMessage();});\n\nclearBtn .addEventListener(\"click\", ()=>{\n  messagesDiv.innerHTML=\"\"; userInput.value=\"\"; chatHistory.length=0;\n});\n\ntoggleBtn.addEventListener(\"click\",()=>{\n  isHtmlView = !isHtmlView;\n  document.querySelectorAll('.assistant').forEach((div,i)=>{\n    const c = chatHistory[i];\n    if(!c) return;\n    div.innerHTML = isHtmlView\n      ? `<div class=\"assistant-label\">${JSON.parse(c.json).label ?? \"Assistant\"}:</div>${c.html}`\n      : `<pre>${c.json}</pre>`;\n  });\n});\n\ndownloadBtn.addEventListener(\"click\",()=>{\n  const blob = new Blob([chatHistory.map(c=>c.json).join(\"\\n\")],{type:\"application/json\"});\n  const url  = URL.createObjectURL(blob);\n  const a    = document.createElement(\"a\");\n  a.href=url; a.download=\"chat_history.json\"; a.click(); URL.revokeObjectURL(url);\n});\n\n/* ---------- collapsible handler ---------------------------------------- */\ndocument.addEventListener(\"click\",e=>{\n  if(e.target.classList.contains(\"collapsible\")){\n    e.target.classList.toggle(\"active\");\n    const c=e.target.nextElementSibling;\n    if(c){c.style.display = c.style.display===\"block\"?\"none\":\"block\";}\n  }\n});\n</script>\n</body>\n</html>\n"
  },
  {
    "path": "tools/__init__.py",
    "content": "#stub"
  },
  {
    "path": "tools/compare_diet_full.py",
    "content": "#!/usr/bin/env python3\n###########################################################################################\n## suite-cisco-ai-building-blocks/ai-building-blocks-agent/tools/compare_diet_full.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n###########################################################################################\nfrom __future__ import annotations\n\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n\n\"\"\"\nQuick utility for **spot-querying the FASTAPI layer’s Chroma DB**.\n\nRun it when you want to inspect which diet-function records are stored for a\nparticular pattern (e.g. any function whose name starts with “getOrganization”).\nIt connects to the collection defined by FASTAPI_CHROMA_DB_PATH /\nFASTAPI_CHROMA_COLLECTION_PLATFORM, filters the documents, and dumps the hits\nas JSONL right next to this script in ai-building-blocks-agent/tools/.\n\"\"\"\nimport os\nimport json\nfrom pathlib import Path\n\n# ── 1. Locate your diet file and full spec ───────────────────────────────\nROOT       = Path(__file__).resolve().parents[1]\nDIET_PATH  = ROOT / \"app\" / \"llm\" / \"function_definitions\" / \"meraki.json\"\nFULL_PATH  = ROOT / \"app\" / \"llm\" / \"openapi_specs\" / \"full_meraki.json\"\n\n# ── 2. Load them ─────────────────────────────────────────────────────────\ndiet      = json.loads(DIET_PATH.read_text(encoding=\"utf-8\"))\nfull_spec = json.loads(FULL_PATH.read_text(encoding=\"utf-8\"))\n\n# ── 3. Extract operationIds ──────────────────────────────────────────────\ndiet_ids = {fn[\"name\"] for fn in diet}\n\nfull_ids = {\n    op[\"operationId\"]\n    for path_item in full_spec.get(\"paths\", {}).values()\n    for op in path_item.values()\n    if \"operationId\" in op\n}\n\n# ── 4. Compute differences ────────────────────────────────────────────────\nmissing_in_diet = sorted(full_ids - diet_ids)\nextra_in_diet   = sorted(diet_ids - full_ids)\n\nreport = {\n    \"missing_in_diet\": missing_in_diet,\n    \"extra_in_diet\":   extra_in_diet,\n}\n\n# ── 5. Write report to disk ──────────────────────────────────────────────\nOUTFILE = \"diet_vs_full_report.json\"\nPath(OUTFILE).write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n\nprint(f\"✅ Report written to {OUTFILE}\")\nprint(f\" • missing_in_diet: {len(missing_in_diet)} items\")\nprint(f\" • extra_in_diet:   {len(extra_in_diet)} items\")\n"
  },
  {
    "path": "tools/jaeger_collector.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n####################################################################################\n## suite-cisco-ai-building-blocks/ai-building-blocks-agent/tools/jaeger_collector.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n####################################################################################\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n\n\n\"\"\"\nUtility that ensures a Jaeger all-in-one OpenTelemetry collector is running.\nIf the container is missing it will be created; if it exists but is stopped\nit will be started.  The container is given a restart policy so it survives\nhost reboots and unexpected exits.\n\nRequires:  pip install docker\n\"\"\"\nfrom typing import Final\nimport docker\nfrom docker.errors import NotFound, APIError\n\nCONTAINER_NAME: Final = \"jaeger\"\nIMAGE: Final = \"jaegertracing/all-in-one:1.57\"\nPORTS: Final = {\"4317/tcp\": 4317, \"16686/tcp\": 16686}\nRESTART_POLICY: Final = {\"Name\": \"unless-stopped\"}\n\ndef main() -> None:\n    client = docker.from_env()\n\n    try:\n        container = client.containers.get(CONTAINER_NAME)\n        status = container.attrs[\"State\"][\"Status\"]\n        if status != \"running\":\n            print(f\"Starting existing container '{CONTAINER_NAME}' (was {status})…\")\n            container.start()\n        else:\n            print(f\"Container '{CONTAINER_NAME}' is already running.\")\n    except NotFound:\n        print(f\"Container '{CONTAINER_NAME}' not found. Creating it…\")\n        container = client.containers.run(\n            IMAGE,\n            name=CONTAINER_NAME,\n            detach=True,\n            ports=PORTS,\n            restart_policy=RESTART_POLICY,\n        )\n        print(f\"Started new container '{CONTAINER_NAME}' ({container.short_id}).\")\n    except APIError as exc:\n        print(f\"Docker API error: {exc.explanation}\")\n        raise SystemExit(1)\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "path": "tools/list_chroma_collections.py",
    "content": "#!/usr/bin/env python3\n###########################################################################################\n## suite-cisco-ai-building-blocks/ai-building-blocks-agent/tools/list_chroma_collections.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n###########################################################################################\nfrom __future__ import annotations\n\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n \n\"\"\"\nSpot-query the FASTAPI layer’s Chroma DB and dump matching diet-function\nrecords to a JSONL file for manual inspection.\n\nCLI flags\n---------\n--pattern   prefix (or regex) to match against `metadata[\"name\"]`\n--platform  restrict to a single platform (omit for all)\n--outfile   where to write the JSONL (default: <pattern>_schemas.jsonl)\n\nExample usage\n-------------\n# (run these from ai-building-blocks-agent/tools)\n\n# 1. Default: find any function whose name starts with “getOrganization”\npython3 list_chroma_collections.py --pattern getOrganization\n\n# 2. Same pattern but restrict to the Meraki platform\npython3 list_chroma_collections.py --pattern getOrganization --platform meraki\n\n# 3. Look for functions that start with “client”, show all platforms,\n#    and write the results to client_funcs.jsonl\npython3 list_chroma_collections.py --pattern client --outfile client_funcs.jsonl\n\n\"\"\"\n\n\n# ── standard libs ─────────────────────────────────────────────────────────\nimport argparse\nimport json\nimport os\nimport re\nimport sys\nfrom pathlib import Path\n\n# ── repo-root on sys.path so imports work from any cwd ────────────────────\nREPO_ROOT = Path(__file__).resolve().parents[1]          # …/ai-building-blocks-agent\nif str(REPO_ROOT) not in sys.path:\n    sys.path.insert(0, str(REPO_ROOT))\n\n# ── third-party ───────────────────────────────────────────────────────────\nfrom dotenv import load_dotenv\nimport chromadb\nfrom chromadb.config import Settings\n\n# ── internal helper to canonicalise paths ────────────────────────────────\nfrom scripts.utils.paths import ensure_abs_env\n\n# ╔══════════════════════════════════════════════════════════════════════╗\n# 1.  Parse CLI flags\n# ╚══════════════════════════════════════════════════════════════════════╝\nap = argparse.ArgumentParser(description=\"Dump matching diet-function records \"\n                                         \"from the FASTAPI Chroma DB.\")\nap.add_argument(\"--pattern\", default=\"getOrganization\",\n                help=\"prefix (or full regex) to match (default: getOrganization)\")\nap.add_argument(\"--platform\", default=None,\n                help=\"platform filter (e.g. meraki); omit for all\")\nap.add_argument(\"--outfile\", default=None,\n                help=\"output path; default is <pattern>_schemas.jsonl \"\n                     \"in the same folder as this script\")\nargs = ap.parse_args()\n\n# ╔══════════════════════════════════════════════════════════════════════╗\n# 2.  Resolve env-vars + open Chroma collection\n# ╚══════════════════════════════════════════════════════════════════════╝\nload_dotenv(REPO_ROOT.parent / \".env\")\n\nDB_ROOT = ensure_abs_env(\"FASTAPI_CHROMA_DB_PATH\", \"chroma_dbs/fastapi\")\nCOL_NAME = os.getenv(\"FASTAPI_CHROMA_COLLECTION_PLATFORM\",\n                     \"function-definitions-index\")\nCOL_DIR = DB_ROOT / COL_NAME\nclient = chromadb.PersistentClient(\n    path=str(COL_DIR),\n    settings=Settings(anonymized_telemetry=False),\n)\ncol = client.get_or_create_collection(COL_NAME)\nprint(f\"Collection '{COL_NAME}' has {col.count()} vectors\")\n\n# ╔══════════════════════════════════════════════════════════════════════╗\n# 3.  Fetch and filter\n# ╚══════════════════════════════════════════════════════════════════════╝\nregex = re.compile(rf\"{args.pattern}\")\nhits = []\n\ndata = col.get()   # pulls all ids/docs/metas\nfor _id, meta, doc in zip(data[\"ids\"], data[\"metadatas\"], data[\"documents\"]):\n    if args.platform and meta.get(\"platform\") != args.platform:\n        continue\n    name = meta.get(\"name\", \"\")\n    if regex.match(name):\n        hits.append({\n            \"id\": _id,\n            \"name\": name,\n            \"metadata\": meta,\n            \"schema\": json.loads(doc),\n        })\n\nprint(f\"Found {len(hits)} matching schemas \"\n      f\"for pattern '{args.pattern}'\"\n      f\"{' in platform ' + args.platform if args.platform else ''}\")\n\n# ╔══════════════════════════════════════════════════════════════════════╗\n# 4.  Write JSONL next to this script (or custom path)\n# ╚══════════════════════════════════════════════════════════════════════╝\nscript_dir = Path(__file__).parent.resolve()\noutfile = Path(args.outfile) if args.outfile else \\\n          script_dir / f\"{args.pattern}_schemas.jsonl\"\n\nwith outfile.open(\"w\", encoding=\"utf-8\") as fh:\n    for rec in hits:\n        fh.write(json.dumps(rec) + \"\\n\")\n\nprint(f\"Wrote {len(hits)} records → {outfile.resolve()}\")\n"
  },
  {
    "path": "tools/telemetry_stack.py",
    "content": "#!/usr/bin/env python3\nimport os\nfrom dotenv import load_dotenv\nimport docker\nimport subprocess\n\n# ── 0) Load .env overrides first ────────────────────────────\nload_dotenv(override=True)\n\n# ── 1) OTEL exporter defaults ───────────────────────────────\n# signal-specific HTTP/protobuf traces exporter\nos.environ.setdefault(\"OTEL_TRACES_EXPORTER\",        \"otlp\")\nos.environ.setdefault(\"OTEL_METRICS_EXPORTER\",       \"none\")\nos.environ.setdefault(\"OTEL_EXPORTER_OTLP_ENDPOINT\", \"http://localhost:4319\")\nos.environ.setdefault(\"OTEL_EXPORTER_OTLP_PROTOCOL\", \"http/protobuf\")\nos.environ.setdefault(\"OTEL_EXPORTER_OTLP_INSECURE\", \"true\")\nos.environ.setdefault(\"OTEL_SERVICE_NAME\",           \"ai-building-blocks-agent\")\n\n# ── 1a) Prep host storage dirs ─────────────────────────────\nfor sub in (\"traces\", \"metrics-generator/traces\", \"metrics-generator/wal\"):\n    os.makedirs(f\"tempo-data/{sub}\", exist_ok=True)\n\n# fix ownership & perms so Tempo can mkdir & write inside container\nsubprocess.run([\"sudo\", \"chown\", \"-R\", f\"{os.getuid()}:{os.getgid()}\", \"tempo-data\"], check=True)\nsubprocess.run([\"chmod\", \"-R\", \"777\", \"tempo-data\"], check=True)\n\n# ── 2) Write out tempo.yaml with debug logging ──────────────\ntempo_config = \"\"\"\\\nserver:\n  # main HTTP (query, metrics) and gRPC ports\n  http_listen_port: 4318\n  grpc_listen_port: 4317\n\n  log_level: debug\n  log_format: logfmt\n\n  # force‐log *every* HTTP request on the query/metrics server\n  http_server_settings:\n    log_requests: true\n\ndistributor:\n  receivers:\n    otlp:\n      protocols:\n        http:\n          endpoint: 0.0.0.0:4319\n          # force‐log *every* OTLP-HTTP span post\n          http_server_settings:\n            log_requests: true\n        grpc:\n          endpoint: 0.0.0.0:4320\n\ningester:\n  lifecycler:\n    ring:\n      kvstore:\n        store: memberlist\n      replication_factor: 1\n\nstorage:\n  trace:\n    backend: local\n    local:\n      path: /var/tempo/traces\n\"\"\"\n\nwith open(\"tempo.yaml\", \"w\") as f:\n    f.write(tempo_config)\n\n# ── 3) Prometheus & Grafana provisioning ────────────────────\nprom_config = \"\"\"\\\nglobal:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: prometheus\n    static_configs:\n      - targets: ['localhost:9090']\n\n  - job_name: tempo-server\n    metrics_path: /metrics\n    static_configs:\n      - targets: ['tempo:4318']\n\"\"\"\nwith open(\"prometheus.yml\", \"w\") as f:\n    f.write(prom_config)\n\ngrafana_ds = \"\"\"\\\napiVersion: 1\ndatasources:\n- name: Prometheus\n  type: prometheus\n  access: proxy\n  url: http://prometheus:9090\n  isDefault: true\n\n- name: Tempo\n  type: tempo\n  access: proxy\n  url: http://tempo:4318\n  jsonData:\n    serviceGraphDatasource: Prometheus\n\"\"\"\nos.makedirs(\"grafana_provisioning\", exist_ok=True)\nwith open(\"grafana_provisioning/datasources.yaml\", \"w\") as f:\n    f.write(grafana_ds)\n\n# ── 4) Launch via Docker API ────────────────────────────────\nclient = docker.from_env()\nnet = \"observability-net\"\ntry:\n    client.networks.get(net)\nexcept docker.errors.NotFound:\n    client.networks.create(net, driver=\"bridge\")\n\n# tear down old\nfor name in (\"prometheus\", \"tempo\", \"grafana\"):\n    try:\n        client.containers.get(name).remove(force=True)\n    except docker.errors.NotFound:\n        pass\n\n# Prometheus\nclient.containers.run(\n    \"prom/prometheus:latest\",\n    name=\"prometheus\",\n    detach=True,\n    ports={\"9090/tcp\": 9090},\n    volumes={os.path.abspath(\"prometheus.yml\"): {\"bind\": \"/etc/prometheus/prometheus.yml\", \"mode\": \"ro\"}},\n    command=[\"--config.file=/etc/prometheus/prometheus.yml\", \"--web.enable-remote-write-receiver\"],\n    network=net,\n)\n\n# Tempo v2.7.1 with debug logs\nclient.containers.run(\n    \"grafana/tempo:2.7.1\",\n    name=\"tempo\",\n    detach=True,\n    network_mode=\"host\",\n    volumes={\n        os.path.abspath(\"tempo-debug.yaml\"): {\"bind\": \"/etc/tempo.yaml\", \"mode\": \"ro\"},\n        os.path.abspath(\"tempo-data\"):    {\"bind\": \"/var/tempo\",         \"mode\": \"rw\"},\n    },\n    command=[\"-config.file=/etc/tempo.yaml\", \"--log.level=debug\"],\n)\n\n\n# Grafana\nclient.containers.run(\n    \"grafana/grafana:latest\",\n    name=\"grafana\",\n    detach=True,\n    ports={\"3000/tcp\": 3000},\n    volumes={os.path.abspath(\"grafana_provisioning\"): {\"bind\": \"/etc/grafana/provisioning/datasources\", \"mode\": \"ro\"}},\n    environment={\n        \"GF_SECURITY_ADMIN_USER\":     \"admin\",\n        \"GF_SECURITY_ADMIN_PASSWORD\": \"cisco101\",\n    },\n    network=net,\n)\n\nprint(\"✅ telemetry stack started.\")\nprint(\"  • Prometheus → http://localhost:9090\")\nprint(\"  • Tempo (OTLP HTTP) → http://localhost:4319/v1/traces\")\nprint(\"  • Grafana → http://localhost:3000 (admin/cisco101)\")\n"
  },
  {
    "path": "tools/test_chroma_retriever.py",
    "content": "#!/usr/bin/env python3\n###########################################################################################\n## suite-cisco-ai-building-blocks/ai-building-blocks-agent/tools/test_chroma_retriever.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n###########################################################################################\nfrom __future__ import annotations\n\n\"\"\"\nSmoke-test the Chroma **FunctionRetriever**.\n\nIt works from any directory: the script locates the repo root, fixes\n`FASTAPI_CHROMA_DB_PATH` (turning a relative value into an absolute path),\nthen runs one or two demo searches and prints the distance-sorted results.\n\nCLI flags\n---------\n--term        search term for the vector query        (default: \"organization\")\n--platform    restrict results to a single platform   (omit for no filter)\n--top-k       how many hits to show                   (default: 5)\n\nExample usage\n-------------\n# ── launch from the tools/ folder ──────────────────────────────────────\npython3 test_chroma_retriever.py                       # default: term=\"organization\"\npython3 test_chroma_retriever.py --term client --top-k 10\npython3 test_chroma_retriever.py --term site --platform meraki\n\n# ── launch from the repo root (or anywhere) using -m ───────────────────\npython -m ai_building_blocks_agent.tools.test_chroma_retriever \\\n       --term site --platform meraki\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nfrom pathlib import Path\n\n# ── ensure repo root is importable ─────────────────────────────────────────\nREPO_ROOT = Path(__file__).resolve().parents[1]          # …/ai-building-blocks-agent\nif str(REPO_ROOT) not in sys.path:\n    sys.path.insert(0, str(REPO_ROOT))\n\n# ── canonicalise Chroma DB path (relative → absolute) ─────────────────────\nfrom scripts.utils.paths import ensure_abs_env\nensure_abs_env(\"FASTAPI_CHROMA_DB_PATH\", \"chroma_dbs/fastapi\")\n\nfrom retrievers.chroma_retriever import FunctionRetriever\n\n\ndef print_results(results: list[dict]) -> None:\n    for i, r in enumerate(results, 1):\n        print(f\"  {i:>2}. \"\n              f\"name={r.get('name'):<60} \"\n              f\"platform={r.get('platform'):<10} \"\n              f\"distance={r.get('distance'):.4f}\")\n\n\ndef main() -> None:\n    # ── CLI flags ─────────────────────────────────────────────────────────\n    ap = argparse.ArgumentParser(description=\"Quick query against Chroma.\")\n    ap.add_argument(\"--term\",      default=\"organization\",\n                    help=\"vector query term (default: 'organization')\")\n    ap.add_argument(\"--platform\",  default=None,\n                    help=\"platform filter, e.g. meraki (omit for no filter)\")\n    ap.add_argument(\"--top-k\",     type=int, default=5,\n                    help=\"how many top hits to display (default: 5)\")\n    args = ap.parse_args()\n\n    collection = os.getenv(\"FASTAPI_CHROMA_COLLECTION_PLATFORM\",\n                           \"function-definitions-index\")\n    print(f\"\\nInitializing FunctionRetriever against collection \"\n          f\"'{collection}' …\\n\")\n    retriever = FunctionRetriever(collection_name=collection)\n\n    # ── query without platform filter ─────────────────────────────────────\n    print(f\"1) Top-{args.top_k} schemas for query '{args.term}':\")\n    hits = retriever.query(args.term, k=args.top_k)\n    print_results(hits)\n    print(f\"  → Retrieved {len(hits)} entries\\n\")\n\n    # ── optional platform-specific query ──────────────────────────────────\n    if args.platform:\n        print(f\"2) Top-{args.top_k} schemas for '{args.term}' \"\n              f\"on platform '{args.platform}':\")\n        hits = retriever.query(\n            args.term,\n            k=args.top_k,\n            filter={\"platform\": {\"$in\": [args.platform]}}\n        )\n        print_results(hits)\n        print(f\"  → Retrieved {len(hits)} entries\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "path": "tools/test_otel_http.py",
    "content": "#!/usr/bin/env python3\nimport logging\nlogging.basicConfig(\n    format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\",\n    level=logging.DEBUG,\n)\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n\n# 1) Hit Tempo's HTTP OTLP endpoint internally on the Docker network\nexporter = OTLPSpanExporter(endpoint=\"http://tempo:4319/v1/traces\")\n\n# 2) Immediate delivery\nprovider = TracerProvider(resource=Resource({\"service.name\": \"docker-http-test\"}))\nprovider.add_span_processor(SimpleSpanProcessor(exporter))\ntrace.set_tracer_provider(provider)\n\n# 3) Emit a single span\ntracer = trace.get_tracer(\"test_http\")\nwith tracer.start_as_current_span(\"hello-http-span\"):\n    print(\"✔️  started & ended hello-http-span\")\n\n# 4) Flush & exit\ntrace.get_tracer_provider().shutdown()\n"
  }
]