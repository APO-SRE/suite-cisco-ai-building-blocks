[
  {
    "path": "README.md",
    "content": "Thought for 5 seconds\n\n\n![Cisco AI Building Blocks Database Banner](assets/cisco_ai%20building_blocks_domain_project.png)\n\n# Cisco AI Building Blocks — Database Project\n\n*Vector-index pipelines for every AI layer*\n\n> **DISCLAIMER — USE AT YOUR OWN RISK**\n> The software in this repository is provided **“as is”** without warranty of any kind, either express or implied.\n> It is intended for demonstration and development. **Do not** deploy in production until you have reviewed, tested, and hardened the code for your own requirements. By using this project you acknowledge that you have read and accepted these terms.\n\n---\n\n## 1  What this repo does\n\nThis repository houses the **data-engineering side** of the Cisco AI Building Blocks Suite.\nIt contains the scripts and utilities that:\n\n| 🏗️ Task                                                                                     | 📁 Where / How                    |\n| -------------------------------------------------------------------------------------------- | --------------------------------- |\n| Chunk raw files into manageable text chunks                                                  | `scripts/utils/chunking.py`       |\n| Embed each chunk with your choice of encoder (Azure OpenAI, Hugging Face, etc.)              | `scripts/utils/pipeline_utils.py` |\n| Write the embeddings to your chosen vector backend (Azure Cognitive Search, Chroma, Elastic) | `scripts/indexers/*`              |\n| Do all of the above per **layer** (FASTAPI, EVENTS, DOMAIN, AGENTIC)                         | `scripts/process_*.py`            |\n\nThe resulting vector indices are then consumed by the runtime agent in the companion project: **[Cisco AI Building Blocks Agent](https://github.com/APO-SRE/ai-building-blocks-agent)**.\n\n---\n\n## 2  Layer-specific processors\n\n| Layer                  | Script                       | Typical content                                 |\n| ---------------------- | ---------------------------- | ----------------------------------------------- |\n| **FASTAPI**            | `scripts/process_docs.py`    | API docs & platform-level references            |\n| **EVENTS**             | `scripts/process_events.py`  | High-volume telemetry / event blobs             |\n| **DOMAIN**             | `scripts/process_domain.py`  | Domain-specific or industry data                |\n| **AGENTIC** (optional) | `scripts/process_agentic.py` | Long-form knowledge for chain-of-thought agents |\n\nEach script hard-codes its `LAYER` value and automatically reads the matching section in your **`.env`**.\n\n---\n\n## 3  Environment-variable driven\n\nAll configuration lives in **`.env`** (copy `example.env` to get started).\nVariables are grouped by layer:\n\n```\n# FASTAPI ───────────────────────────────────────────\nFASTAPI_VECTOR_ENABLED=true\nFASTAPI_VECTOR_BACKEND=chroma\nFASTAPI_EMBEDDING_PROVIDER=huggingface\nFASTAPI_CHROMA_RECREATE_INDEX=false\n…\n\n# DOMAIN ────────────────────────────────────────────\nDOMAIN_VECTOR_ENABLED=true\nDOMAIN_VECTOR_BACKEND=azure\nDOMAIN_AZURE_SEARCH_INDEX_NAME=domain-index\n…\n```\n\nA full cheat-sheet is available in [here](example_environment_variables_guide.MD)\n---\n\n## 4  Project layout\n\n```\nai-building-blocks-database/\n├── assets/                         # banners & diagrams\n├── chroma_dbs/                     # local Chroma storage (git-ignored)\n├── domain_samples/                 # sample JSON for DOMAIN layer\n├── events/                         # raw event docs\n├── platform_summaries/             # FASTAPI layer docs\n├── scripts/\n│   ├── process_docs.py             # FASTAPI\n│   ├── process_events.py           # EVENTS\n│   ├── process_domain.py           # DOMAIN\n│   ├── process_agentic.py          # AGENTIC (optional)\n│   ├── indexers/                   # Azure / Chroma / Elastic drivers\n│   └── utils/                      # chunking, NLP, pipeline helpers\n└── .env / example.env              # configuration\n```\n\n---\n\n## 5  Quick start\n\n```bash\n# 1  Clone & create your .env\ngit clone https://github.com/APO-SRE/AI-Building-Blocks-Database.git\ncp example.env .env\n# edit keys, backend choices, and layer flags\n\n# 2  Create the indices you need\npython scripts/process_domain.py      # DOMAIN layer\npython scripts/process_events.py      # EVENTS layer\npython scripts/process_docs.py        # FASTAPI layer\n# (optional) python scripts/process_agentic.py\n```\n\nYou’ll see progress logs for chunking, embedding, and upsert operations.\nValidate the output in your vector backend’s UI or via the test queries provided in the Agent project.\n\n---\n\n## 6  How it works (nutshell)\n\n1. **Pipeline factory** decides which encoder & vector DB to spin up (driven by `.env`).\n2. **Chunker** splits source text (size tunable per layer).\n3. **Embedder** converts chunks to vectors in mini-batches (CPU/OMP threads also tunable).\n4. **Indexer** upserts to the target backend.\n\n   * Chroma collections live under `./chroma_dbs/<layer>/…`\n   * Azure & Elastic indices use the names in `.env`.\n\nAll heavy-lifting happens inside `pipeline_utils.embed_and_index()` so each `process_*.py` stays tiny (< 100 LOC).\n\n---\n\n## 7  Extending & automating\n\n* **Add a backend**: write a new class in `scripts/indexers/`, expose the env-vars, done.\n* **Add a layer**: copy an existing script, change `LAYER` and source paths.\n* **CI/CD**: hook the scripts into your pipeline to auto-refresh indices on doc updates.\n\n---\n\n## 8  License\n\nLicensed under the **Apache 2.0** license. See [`LICENSE`](LICENSE) for the full text.\n\n---\n\n*Last updated — May 2025*\n"
  },
  {
    "path": "pyproject.toml",
    "content": "[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"ai-building-blocks-agent\"\nversion = \"0.1.0\"\ndescription = \"Cisco AI Building Blocks Agent\"\nreadme = \"README.md\"\nrequires-python = \">=3.12\"\ndependencies = [\n  \"rich>=14.0.0\",\n  \"colorama\",\n]\n\n[project.scripts]\ncreate-platform = \"tools.create_platform:main\"\ncreate-index     = \"tools.create_index:main\"\n\n[tool.setuptools.packages.find]\nwhere   = [\".\"]\ninclude = [\"tools*\"]\n"
  },
  {
    "path": "scripts/__init__.py",
    "content": "\n#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/__init__.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\nfrom __future__ import annotations\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\""
  },
  {
    "path": "scripts/generate_openapi_sdks.py",
    "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n##############################################################################################\n## suite-cisco-ai-building-blocks/ai-building-blocks-database/scripts/generate_openapi_sdks.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n##############################################################################################\n\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis script lists OpenAPI v3 schema files in the source directory and invokes\nopenapi-python-client to generate Python SDK packages into the output directory.\nAfter each successful generation it will also update the agent’s sdk_map.json\nwith the actual package directory names.\n\"\"\"\n\nimport subprocess\nimport sys\nimport json\nfrom pathlib import Path\n\n\ndef main() -> None:\n    # --- locate all the relevant directories --------------------------------\n    script_dir      = Path(__file__).resolve().parent\n    db_root         = script_dir.parent                          # .../ai-building-blocks-database\n    source_dir      = db_root / \"source_open_api\"\n    output_base_dir = db_root / \"output_sdk\"\n\n    # the agent’s sdk_map.json lives alongside user_commands in the ai-building-blocks-agent tree\n    agent_root      = db_root.parent / \"ai-building-blocks-agent\"\n    sdk_map_file    = agent_root / \"app\" / \"llm\" / \"sdk_map.json\"\n\n    # ensure our output dir exists\n    output_base_dir.mkdir(parents=True, exist_ok=True)\n\n    # load or initialize SDK map\n    if sdk_map_file.exists():\n        sdk_map = json.loads(sdk_map_file.read_text(encoding=\"utf-8\"))\n    else:\n        sdk_map = {}\n\n    # --- gather all OpenAPI files -------------------------------------------\n    openapi_files = sorted(\n        p for p in source_dir.iterdir()\n        if p.is_file() and p.suffix.lower() in {\".json\", \".yaml\", \".yml\"}\n    )\n    if not openapi_files:\n        print(f\"No OpenAPI files found in {source_dir}\", file=sys.stderr)\n        sys.exit(1)\n\n    # --- user picks one or all -----------------------------------------------\n    print(\"Available OpenAPI schema files:\")\n    for idx, path in enumerate(openapi_files, start=1):\n        print(f\"  {idx}. {path.name}\")\n    print(f\"  0. Generate SDK for ALL files\")\n\n    choice = input(f\"Select [0-{len(openapi_files)}]: \").strip()\n    if choice == \"0\":\n        selected = openapi_files\n    else:\n        try:\n            i = int(choice)\n            if not (1 <= i <= len(openapi_files)):\n                raise ValueError\n            selected = [openapi_files[i-1]]\n        except ValueError:\n            print(\"Invalid selection.\", file=sys.stderr)\n            sys.exit(1)\n\n    # --- generate and map each one ------------------------------------------\n    added_mappings: list[str] = []\n    for spec in selected:\n        default_name = spec.stem\n        user_input   = input(f\"Enter SDK folder name [{default_name}]: \").strip()\n        sdk_name     = user_input or default_name\n\n        dest_dir = output_base_dir / sdk_name\n        dest_dir.mkdir(parents=True, exist_ok=True)\n\n        print(f\"\\n🔧 Generating SDK for '{spec.name}' → '{dest_dir}'\")\n        cmd = [\n            \"openapi-python-client\",\n            \"generate\",\n            \"--path\", str(spec),\n            \"--output-path\", str(dest_dir),\n            \"--meta\", \"poetry\",\n            \"--overwrite\"\n        ]\n        result = subprocess.run(cmd)\n        if result.returncode != 0:\n            print(f\"❌ Failed for {spec.name}; see details above.\", file=sys.stderr)\n            continue\n\n        # detect actual Python package directory inside the generated SDK\n        pkg_dir = next(\n            (d.name for d in dest_dir.iterdir()\n             if d.is_dir() and (d / \"__init__.py\").exists()),\n            sdk_name\n        )\n        print(f\"✅ Successfully generated SDK at: {dest_dir} (package '{pkg_dir}')\")\n\n        # update sdk_map.json if new\n        if sdk_name not in sdk_map or sdk_map[sdk_name] != pkg_dir:\n            sdk_map[sdk_name] = pkg_dir\n            added_mappings.append(f\"{sdk_name} → {pkg_dir}\")\n\n    # --- persist any changes to sdk_map.json --------------------------------\n    if added_mappings:\n        sdk_map_file.parent.mkdir(parents=True, exist_ok=True)\n        sdk_map_file.write_text(json.dumps(sdk_map, indent=2), encoding=\"utf-8\")\n        print(\"\\n🗺️  Updated SDK map with:\")\n        for mapping in added_mappings:\n            print(f\"  {mapping}\")\n\n    print(\"\\nAll done. Your SDKs live under:\")\n    print(f\"  {output_base_dir}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  {
    "path": "scripts/process_domain.py",
    "content": "\n#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/process_domain.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\nfrom __future__ import annotations\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\nimport os\nimport glob\nimport json\nimport hashlib\nimport logging\nfrom dotenv import load_dotenv\n\nfrom scripts.utils.chunking import chunk_file\nfrom scripts.utils import pipeline_utils\n\n# 1) Load .env & configure logging\nload_dotenv()\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\nlogger = logging.getLogger(\"process_domain\")\n\n# 2) Hard-code the layer name:\nLAYER = \"DOMAIN\"\n\n# 3) Determine backend & provider\nbackend  = os.getenv(f\"{LAYER}_VECTOR_BACKEND\",    \"azure\").lower()\nprovider = os.getenv(f\"{LAYER}_EMBEDDING_PROVIDER\", \"azure\").lower()\nlogger.info(f\"Layer={LAYER}, VECTOR_BACKEND={backend}, EMBEDDING_PROVIDER={provider}\")\n\n# 4) Decide concurrency prefix (for OMP_NUM_THREADS, etc.)\n_backend_prefix_map = {\n    \"azure\":   \"AZURE\",\n    \"chroma\":  \"CHROMA\",\n    \"elastic\": \"ELASTIC\",\n}\nprefix_key = _backend_prefix_map.get(backend, \"AZURE\")\n\n# 5) Pull concurrency from environment\nomp_var  = os.getenv(f\"{LAYER}_{prefix_key}_OMP_NUM_THREADS\") or os.getenv(\"OMP_NUM_THREADS\", \"1\")\nmkl_var  = os.getenv(f\"{LAYER}_{prefix_key}_MKL_NUM_THREADS\") or os.getenv(\"MKL_NUM_THREADS\", \"1\")\ncpus_var = os.getenv(f\"{LAYER}_{prefix_key}_NUM_CPUS\")       or os.getenv(\"NUM_CPUS\", \"1\")\n\nlogger.info(f\"Threads: OMP_NUM_THREADS={omp_var}, MKL_NUM_THREADS={mkl_var}, NUM_CPUS={cpus_var}\")\nos.environ[\"OMP_NUM_THREADS\"] = omp_var\nos.environ[\"MKL_NUM_THREADS\"] = mkl_var\nos.environ[f\"{LAYER}_OMP_NUM_THREADS\"] = cpus_var\nlogger.info(f\"Set {LAYER}_OMP_NUM_THREADS={cpus_var} for pipeline parallelism\")\n\n# 6) Check if vector indexing is enabled\nif os.getenv(f\"{LAYER}_VECTOR_ENABLED\", \"false\").lower() != \"true\":\n    logger.info(f\"{LAYER}_VECTOR_ENABLED is not true → skipping domain processing.\")\n    exit(0)\n\nlogger.info(f\"Using backend='{backend}' for domain data indexing…\")\n\n# 7) If using Azure, ensure index is created\nif backend == \"azure\":\n    from scripts.indexers.azure_indexer import AzureIndexer\n    idx_name = os.getenv(f\"{LAYER}_AZURE_INDEX\", f\"{LAYER.lower()}-index\")\n    logger.info(f\"Ensuring Azure Search index '{idx_name}'…\")\n    AzureIndexer(idx_name, layer_name=LAYER).create_index(recreate=None)\n    os.environ[f\"{LAYER}_AZURE_INDEX\"] = idx_name\n\ndef deterministic_id(*parts: str, algo: str = \"sha1\") -> str:\n    \"\"\"\n    Build a repeatable ID from any number of text fragments.\n\n    Example\n    -------\n    deterministic_id(\"domain\", \"/path/file.json\", \"42\", \"3\")  ->\n        '6a4e8f85d0c3715f824d4e0c890d503f1ca89e23'\n    \"\"\"\n    joined = \"§\".join(parts)          # unlikely to appear in file names\n    h      = hashlib.new(algo)\n    h.update(joined.encode())\n    return h.hexdigest()\n\n################################################################################\n# 8) Gather domain data from domain_samples/<folder_name>/...\n################################################################################\n\nfolder_name = os.getenv(\"DOMAIN_SAMPLES_INDEX_FOLDER_NAME\", \"\")  # e.g. \"airline\"\nif not folder_name:\n    logger.warning(\"No DOMAIN_SAMPLES_INDEX_FOLDER_NAME specified in .env; defaulting to 'domain_samples'\")\ndomain_samples_dir = os.path.join(\"domain_samples\", folder_name)  \n# e.g. domain_samples/airline\n\njson_files = glob.glob(os.path.join(domain_samples_dir, \"*.json\"))\nif not json_files:\n    logger.warning(f\"No .json files found in '{domain_samples_dir}'. Nothing to index.\")\n    exit(0)\n\nall_chunks = []\nCHUNK_SIZE    = int(os.getenv(f\"{LAYER}_{prefix_key}_CHUNK_SIZE\",    \"2000\"))\nCHUNK_OVERLAP = int(os.getenv(f\"{LAYER}_{prefix_key}_CHUNK_OVERLAP\", \"100\"))\n\nfor fp in json_files:\n    logger.info(f\"Loading domain data from: {fp}\")\n    try:\n        with open(fp, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n    except Exception as e:\n        logger.error(f\"Could not read/parse {fp}: {e}\")\n        continue\n\n    # data might be a list or single dict\n    records = data if isinstance(data, list) else [data]\n\n    # chunk each record\n    for rec_idx, record in enumerate(records):\n        # Convert everything to text\n        text_parts = []\n        for key, val in record.items():\n            if isinstance(val, str):\n                text_parts.append(val.strip())\n            elif isinstance(val, (list, dict)):\n                # Optionally flatten further\n                pass\n\n        combined_text = \"\\n\".join(text_parts).strip()\n        if not combined_text:\n            continue\n        for chunk_idx, chunk in enumerate(\n            chunk_file(combined_text, CHUNK_SIZE, CHUNK_OVERLAP)\n        ):\n            all_chunks.append({\n                \"id\": deterministic_id(\"domain\", fp, str(rec_idx), str(chunk_idx)),\n                \"content\":  chunk,\n                \"metadata\": record\n            })\n \n\nlogger.info(f\"Prepared {len(all_chunks)} domain-chunks from {len(json_files)} files in {domain_samples_dir}.\")\n\nif not all_chunks:\n    logger.info(\"No domain chunks found; nothing to index.\")\n    exit(0)\n\nlogger.info(f\"Embedding & indexing {len(all_chunks)} domain-chunks via {backend}…\")\n\npipeline_utils.index_documents(\n    docs=          all_chunks,\n    provider_name= provider,\n    backend_name=  backend,\n    layer_prefix=  LAYER\n)\n\nlogger.info(\"✅ Domain indexing complete.\")\n\n\n"
  },
  {
    "path": "scripts/process_events.py",
    "content": "\n#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/process_events.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\nfrom __future__ import annotations\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n\nimport os\nimport json\nimport uuid\nimport logging\nfrom dotenv import load_dotenv\n\nfrom scripts.utils.chunking import chunk_file\nfrom scripts.utils import pipeline_utils\n\n# 1) Load .env & configure logging\nload_dotenv()\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\nlogger = logging.getLogger(\"process_events\")\n\n# 2) Hard-code the layer\nLAYER = \"EVENTS\"\n\n# 3) Read which vector backend & embedding provider we’re using\nbackend  = os.getenv(f\"{LAYER}_VECTOR_BACKEND\",    \"azure\").lower()\nprovider = os.getenv(f\"{LAYER}_EMBEDDING_PROVIDER\", \"azure\").lower()\nlogger.info(f\"Layer={LAYER}, VECTOR_BACKEND={backend}, EMBEDDING_PROVIDER={provider}\")\n\n# 4) Decide concurrency prefix (for OMP_NUM_THREADS, etc.)\n_prefix_map = {\"azure\": \"AZURE\", \"chroma\": \"CHROMA\", \"elastic\": \"ELASTIC\"}\nprefix_key = _prefix_map.get(backend, \"AZURE\")\n\n# 5) Pull concurrency from environment\nomp_var  = os.getenv(f\"{LAYER}_{prefix_key}_OMP_NUM_THREADS\") or os.getenv(\"OMP_NUM_THREADS\", \"1\")\nmkl_var  = os.getenv(f\"{LAYER}_{prefix_key}_MKL_NUM_THREADS\") or os.getenv(\"MKL_NUM_THREADS\", \"1\")\ncpus_var = os.getenv(f\"{LAYER}_{prefix_key}_NUM_CPUS\")       or os.getenv(\"NUM_CPUS\", \"1\")\n\nlogger.info(f\"Threads: OMP_NUM_THREADS={omp_var}, MKL_NUM_THREADS={mkl_var}, NUM_CPUS={cpus_var}\")\n\n# 6) Apply them so that libraries & pipeline_utils pick them up\nos.environ[\"OMP_NUM_THREADS\"] = omp_var\nos.environ[\"MKL_NUM_THREADS\"] = mkl_var\nos.environ[f\"{LAYER}_OMP_NUM_THREADS\"] = cpus_var\nlogger.info(f\"Set {LAYER}_OMP_NUM_THREADS={cpus_var} for pipeline parallelism\")\n\n# 7) Only proceed if vector indexing is enabled\nif os.getenv(f\"{LAYER}_VECTOR_ENABLED\", \"false\").lower() != \"true\":\n    logger.info(f\"{LAYER}_VECTOR_ENABLED is not true → skipping.\")\n    exit(0)\n\nlogger.info(f\"Using backend={backend!r} for indexing…\")\n\n# 8) For Azure backend, explicitly ensure the Azure Search index exists\n#    (Chroma & Elastic get created/prompted automatically inside pipeline_utils.)\nif backend == \"azure\":\n    from scripts.indexers.azure_indexer import AzureIndexer\n    idx = os.getenv(f\"{LAYER}_AZURE_INDEX\", f\"{LAYER.lower()}-index\")\n    logger.info(f\"Ensuring Azure Search index '{idx}'…\")\n\n    AzureIndexer(idx, layer_name=LAYER).create_index(recreate=None)\n    os.environ[f\"{LAYER}_AZURE_INDEX\"] = idx\n\n# 9) Load & chunk your events JSON\nevents_path = \"events/sample_events.json\"\nif not os.path.exists(events_path):\n    logger.warning(f\"Missing {events_path}, nothing to do.\")\n    exit(0)\n\nwith open(events_path, \"r\", encoding=\"utf-8\") as f:\n    events_data = json.load(f)\nif not isinstance(events_data, list) or not events_data:\n    logger.warning(\"No events found in JSON, exiting.\")\n    exit(0)\n\n# 10) Convert each event to text + chunk\nCHUNK_SIZE    = int(os.getenv(f\"{LAYER}_{prefix_key}_CHUNK_SIZE\",    \"2000\"))\nCHUNK_OVERLAP = int(os.getenv(f\"{LAYER}_{prefix_key}_CHUNK_OVERLAP\", \"100\"))\n\nevent_chunks = []\nfor evt in events_data:\n    info = evt.get(\"additional_info\", {})\n    # Example short text from event\n    snippet = (\n        f\"Event: {evt.get('event_type','unknown')} \"\n        f\"in zone {info.get('zone_id','N/A')} at {info.get('timestamp','N/A')}.\"\n    )\n\n\n    for chunk in chunk_file(snippet, CHUNK_SIZE, CHUNK_OVERLAP):\n        event_chunks.append({\n            \"id\":             evt.get(\"id\"),\n            \"event_id\":       evt.get(\"event_id\"),\n            \"event_name\":     evt.get(\"event_name\"),\n            \"event_type\":     evt.get(\"event_type\"),\n            \"content\":        chunk,\n            \"additional_info\": info\n        })\n\nlogger.info(f\"Prepared {len(event_chunks)} event-chunks from {len(events_data)} events.\")\n\n\n\n \n\n# 11) Let pipeline_utils do embedding + indexing.\n#     For Chroma, it will:\n#       - Check if the collection name already exists\n#       - If so and if EVENTS_CHROMA_RECREATE_INDEX is not set to \"true\"\n#         it will prompt for (R)ecreate or (A)ppend\n#     Same pattern if you eventually implement the process_docs, process_domain, etc.\nlogger.info(f\"Embedding & indexing {len(event_chunks)} chunks via {backend}…\")\n\npipeline_utils.index_documents(\n    docs=          event_chunks,\n    provider_name= provider,\n    backend_name=  backend,\n    layer_prefix=  LAYER\n)\n\nlogger.info(\"✅ All done.\")\n"
  },
  {
    "path": "scripts/indexers/__init__.py",
    "content": "#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/indexers/__init__.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\nfrom __future__ import annotations\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n# ai-building-blocks-database/scripts/__init__.py\nimport sys, types\n\n# Make everything in this folder importable as \"db_scripts.…\"\nmodule = types.ModuleType(\"db_scripts\")\nmodule.__path__ = __path__          # type: ignore  # inherit the same path list\nsys.modules[\"db_scripts\"] = module\n\nfrom .azure_indexer import AzureIndexer\nfrom .chroma_indexer import ChromaIndexer\nfrom .elastic_indexer import ElasticIndexer\nfrom .base_indexer import BaseIndexer\n# from .base_indexer import BaseIndexer\n# from .chroma_indexer import ChromaIndexer\n# from .elastic_indexer import ElasticIndexer\n\n"
  },
  {
    "path": "scripts/indexers/azure_indexer.py",
    "content": "#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/indexers/azure_indexer.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\nfrom __future__ import annotations\n\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\nimport os, json, re, base64\nfrom dotenv import load_dotenv\n\nfrom azure.search.documents import SearchClient\nfrom azure.search.documents.indexes import SearchIndexClient\nfrom azure.search.documents.indexes.models import (\n    SearchIndex,\n    SearchableField,\n    SearchField,\n    ComplexField,\n    SearchFieldDataType,\n    VectorSearch,\n    VectorSearchProfile,\n    HnswParameters,\n    SemanticConfiguration,\n    SemanticPrioritizedFields,\n    SemanticField,\n    SemanticSearch,\n    CorsOptions\n)\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.core.exceptions import ResourceNotFoundError, HttpResponseError\nfrom ..utils.embedding import embed_text\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nCPU_WORKERS = int(os.getenv(\"FASTAPI_AZURE_NUM_CPUS\", \"4\") or 4)\n# show thread-pool size and default batch size once\nDEFAULT_BATCH = int(os.getenv(\"FASTAPI_AZURE_BATCH\", \"500\") or 500)\nprint(\n    f\"[AzureIndexer]  ⚙  embedding threads = {CPU_WORKERS}  |  \"\n    f\"default batch-upload = {DEFAULT_BATCH}\",\n    flush=True,\n)\n\nfrom .base_indexer import BaseIndexer\n_already_finalised: set[str] = set()\n\nload_dotenv()\n\nclass AzureIndexer(BaseIndexer):\n    def __init__(self, index_name: str, layer_name: str = \"\"):\n        super().__init__(index_name)\n        # store layer key for use in other methods\n        self.layer_key = layer_name.upper().strip() if layer_name else \"\"\n        self.docs_index_name = os.getenv(f\"{self.layer_key}_AZURE_DOCS_INDEX\")\n        self.platform_index_name = os.getenv(f\"{self.layer_key}_AZURE_PLATFORM_INDEX\")\n\n        endpoint = os.getenv(f\"{self.layer_key}_AZURE_ENDPOINT\")\n        key = os.getenv(f\"{self.layer_key}_AZURE_KEY\")\n        api_ver = os.getenv(f\"{self.layer_key}_AZURE_API_VERSION\", \"2024-07-01\")\n        if not endpoint or not key:\n            raise ValueError(f\"Missing AI Azure Search endpoint/key for layer '{layer_name}'\")\n\n        self.endpoint = endpoint\n        self.key = key\n        self.api_version = api_ver\n\n        # interactive recreate logic\n        raw_local = os.getenv(f\"{self.layer_key}_AZURE_RECREATE_INDEX\", None)\n        raw_global = os.getenv(\"AZURE_RECREATE_INDEX\", None)\n        raw = raw_local if raw_local is not None else raw_global\n        self.recreate_index = None if raw is None or raw.strip() == \"\" else raw.lower() == \"true\"\n\n        print(f\"[AzureIndexer] Layer='{layer_name}', Endpoint={endpoint}, IndexName={index_name}, Recreate? {self.recreate_index}\")\n\n        self.credential = AzureKeyCredential(self.key)\n        self.index_client = SearchIndexClient(\n            endpoint=self.endpoint,\n            credential=self.credential,\n            api_version=self.api_version\n        )\n        self.search_client = None\n\n    # ────────────────────────────────────────────────────────────────\n    #  Index bootstrap  — delete-once-per-run logic\n    # ────────────────────────────────────────────────────────────────\n    def create_index(self, recreate: bool | None = None):\n        \"\"\"\n        • If self.recreate_index is *True* we delete the index **only once per\n          process** (tracked in the _already_recreated set).\n        • If it is *False* we always append.\n        • If it is *None* we prompt the user exactly once; subsequent calls\n          reuse the remembered choice without asking again.\n        \"\"\"\n        global _already_finalised   # use the module-level guard\n\n        # ── 0️⃣  fast-path: decision already made in this process ──────\n        if self.index_name in _already_finalised:\n            # we have already *created* or *kept* the index in this run\n            self.recreate_index = False          # always append\n            recreate            = False          # caller override ignored\n            print(\n                f\"[AzureIndexer] add → '{self.index_name}' (decision cached)\",\n                flush=True,\n            )\n\n             \n        # allow caller to override the env-derived value\n        if recreate is not None:\n            self.recreate_index = recreate\n\n        # ── does the index exist? ────────────────────────────────────\n        print(f\"[AzureIndexer] Checking if index '{self.index_name}' exists…\")\n        try:\n            existing = self.index_client.get_index(self.index_name)\n            print(f\"[AzureIndexer] Index '{self.index_name}' found.\")\n        except ResourceNotFoundError:\n            existing = None\n            print(f\"[AzureIndexer] Index '{self.index_name}' does not exist.\")\n        except HttpResponseError as exc:\n            print(f\"[AzureIndexer] Error checking index: {exc}\")\n            raise\n\n        # ── ask the question only if we still don’t know what to do ──\n        if existing is not None and self.recreate_index is None:\n            resp = input(\n                f\"Index '{self.index_name}' already exists. (R)ecreate or \"\n                \"(A)ppend? [R/a]: \"\n            ).strip().lower()\n            self.recreate_index = resp.startswith(\"r\")\n\n        # ── delete once per run if requested ────────────────────────\n \n\n\n        if existing and self.recreate_index:\n            print(\n                f\"[AzureIndexer] recreate_index=True → deleting \"\n                f\"'{self.index_name}'…\"\n            )\n            self.index_client.delete_index(self.index_name)\n            existing = None\n \n\n        # ── create if missing (either new or just deleted) ──────────\n        if existing is None:\n            schema = self.build_index_schema(self.index_name)\n            self.index_client.create_index(schema)\n            print(f\"[AzureIndexer] Created new index '{self.index_name}'\")\n            # remember the decision so next platforms just append\n            _already_finalised.add(self.index_name)\n            self.recreate_index = False          # subsequent calls append\n        else:\n            print(f\"[AzureIndexer] Reusing existing index '{self.index_name}'\")\n            _already_finalised.add(self.index_name)   # mark as handled\n\n        # ── finally set up the SearchClient for uploads ─────────────\n        self.search_client = SearchClient(\n            endpoint=self.endpoint,\n            index_name=self.index_name,\n            credential=self.credential,\n            api_version=self.api_version,\n            logging_enable=True,\n        )\n\n\n    # ────────────────────────────────────────────────\n    #  Schema factory\n    # ────────────────────────────────────────────────\n    def build_index_schema(self, index_name: str) -> SearchIndex:\n        # read embedding dimension from env or default\n        dim_env = os.getenv(f\"{self.layer_key}_AZURE_DIM\") or os.getenv(\"AZURE_DIM\")\n        try:\n            EMBEDDING_DIM = int(dim_env)\n        except (TypeError, ValueError):\n            EMBEDDING_DIM = 1536\n\n        # normalize for comparison\n        idx = index_name.lower()\n        docs_idx = (self.docs_index_name or \"\").lower()\n        plat_idx = (self.platform_index_name or \"\").lower()\n\n\n        if idx == \"function-definitions-index\":\n            fields = [\n                SearchableField(\n                    name=\"id\", type=SearchFieldDataType.String,\n                    key=True, filterable=True, searchable=True\n                ),\n                SearchableField(\n                    name=\"platform\", type=SearchFieldDataType.String,\n                    filterable=True, searchable=True, facetable=True\n                ),\n                SearchableField(\n                    name=\"name\", type=SearchFieldDataType.String,\n                    filterable=True, searchable=True\n                ),\n                SearchableField(\n                    name=\"content\", type=SearchFieldDataType.String,\n                    searchable=True\n                ),\n                SearchField(\n                    name=\"embedding\",\n                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n                    searchable=True,\n                    filterable=False,\n                    sortable=False,\n                    facetable=False,\n                    vector_search_dimensions=EMBEDDING_DIM,\n                    vector_search_profile_name=\"fnDefHnswProfile\",\n                ),\n            ]\n\n            semantic_config = SemanticConfiguration(\n                name=\"fnDefSemanticConfig\",\n                prioritized_fields=SemanticPrioritizedFields(\n                    title_field=SemanticField(field_name=\"name\"),\n                    content_fields=[SemanticField(field_name=\"content\")],\n                    keywords_fields=[SemanticField(field_name=\"platform\")],\n                ),\n            )\n            vector_search = VectorSearch(\n                algorithms=[\n                    {\n                        \"name\": \"fnDefHnsw\",\n                        \"kind\": \"hnsw\",\n                        \"parameters\": HnswParameters(\n                            m=4,\n                            ef_construction=400,\n                            ef_search=500,\n                            metric=\"cosine\"\n                        )\n                    }\n                ],\n                profiles=[\n                    VectorSearchProfile(\n                        name=\"fnDefHnswProfile\",\n                        algorithm_configuration_name=\"fnDefHnsw\"\n                    )\n                ]\n            )\n\n\n            cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n\n            return SearchIndex(\n                name=index_name,\n                fields=fields,\n                vector_search=vector_search,\n                semantic_search=SemanticSearch(configurations=[semantic_config]),\n                cors_options=cors_options,\n            )\n\n################################################################################\n# In azure_indexer.py -> AzureIndexer.build_index_schema()\n################################################################################\n\n        if index_name.startswith(\"domain-\"):\n            fields = [\n                SearchableField(\n                    name=\"id\",\n                    type=SearchFieldDataType.String,\n                    key=True,\n                    filterable=True,\n                    searchable=True\n                ),\n                SearchableField(\n                    name=\"content\",\n                    type=SearchFieldDataType.String,\n                    searchable=True\n                ),\n                SearchField(\n                    name=\"embedding\",\n                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n                    searchable=True,\n                    filterable=False,\n                    sortable=False,\n                    facetable=False,\n                    vector_search_dimensions=EMBEDDING_DIM,\n                    vector_search_profile_name=\"domainHnswProfile\"   # Referencing the profile name below\n                ),\n                SearchableField(\n                    name=\"metadata\",\n                    type=SearchFieldDataType.String,\n                    searchable=True,\n                    filterable=False\n                )\n            ]\n\n            semantic_config_name = f\"{index_name}-semantic-config\"\n            semantic_config = SemanticConfiguration(\n                name=semantic_config_name,\n                prioritized_fields=SemanticPrioritizedFields(\n                    title_field=None,\n                    content_fields=[SemanticField(field_name=\"content\")],\n                    keywords_fields=[]\n                )\n            )\n\n            # Notice we rename both the \"name\" and the \"algorithm_configuration_name\"\n            # to \"domainHnsw\", so they match exactly.\n            vector_search = VectorSearch(\n                algorithms=[\n                    {\n                        \"name\": \"domainHnsw\",    # must match algorithm_configuration_name\n                        \"kind\": \"hnsw\",\n                        \"parameters\": HnswParameters(\n                            m=4,\n                            ef_construction=400,\n                            ef_search=500,\n                            metric=\"cosine\"\n                        )\n                    }\n                ],\n                profiles=[\n                    VectorSearchProfile(\n                        name=\"domainHnswProfile\",                # used by the field\n                        algorithm_configuration_name=\"domainHnsw\" # must match algorithms[].name\n                    )\n                ]\n            )\n\n            cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n\n            return SearchIndex(\n                name=index_name,\n                fields=fields,\n                vector_search=vector_search,\n                semantic_search=SemanticSearch(configurations=[semantic_config]),\n                cors_options=cors_options\n            )\n\n\n        # events-index\n        elif index_name == \"events-index\":\n            fields = [\n                SearchableField(\n                    name=\"id\",\n                    type=SearchFieldDataType.String,\n                    key=True,\n                    filterable=True,\n                    searchable=True\n                ),\n                SearchableField(\n                    name=\"event_id\",\n                    type=SearchFieldDataType.String,\n                    filterable=True,\n                    searchable=True\n                ),\n                SearchableField(\n                    name=\"event_name\",\n                    type=SearchFieldDataType.String,\n                    searchable=True\n                ),\n                SearchableField(\n                    name=\"event_type\",\n                    type=SearchFieldDataType.String,\n                    filterable=True,\n                    searchable=True\n                ),\n                SearchableField(\n                    name=\"content\",\n                    type=SearchFieldDataType.String,\n                    searchable=True\n                ),\n                SearchField(\n                    name=\"embedding\",\n                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n                    searchable=True,\n                    filterable=False,\n                    sortable=False,\n                    facetable=False,\n                    vector_search_dimensions=EMBEDDING_DIM,\n                    vector_search_profile_name=\"myHnswProfile\"\n                ),\n                ComplexField(\n                    name=\"additional_info\",\n                    fields=[\n                        SearchableField(\n                            name=\"zone_id\",\n                            type=SearchFieldDataType.String,\n                            filterable=True,\n                            searchable=True\n                        ),\n                        SearchField(\n                            name=\"timestamp\",\n                            type=SearchFieldDataType.DateTimeOffset,\n                            filterable=True,\n                            sortable=True,\n                            facetable=False\n                        ),\n                        SearchableField(\n                            name=\"camera_id\",\n                            type=SearchFieldDataType.String,\n                            filterable=True,\n                            searchable=True\n                        ),\n                        SearchableField(\n                            name=\"building\",\n                            type=SearchFieldDataType.String,\n                            filterable=True,\n                            searchable=True\n                        ),\n                        SearchableField(\n                            name=\"floor\",\n                            type=SearchFieldDataType.String,\n                            filterable=True,\n                            searchable=True\n                        ),\n                        SearchableField(\n                            name=\"location\",\n                            type=SearchFieldDataType.String,\n                            filterable=True,\n                            searchable=True\n                        ),\n                        SearchableField(\n                            name=\"cisco_ai\",\n                            type=SearchFieldDataType.String,\n                            filterable=True,\n                            searchable=True\n                        ),\n                        SearchField(\n                            name=\"recommended_actions\",\n                            type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n                            searchable=True,\n                            filterable=False,\n                            sortable=False,\n                            facetable=False\n                        ),\n                        SearchField(\n                            name=\"urls_for_further_action\",\n                            type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n                            searchable=True,\n                            filterable=False,\n                            sortable=False,\n                            facetable=False\n                        ),\n                        SearchField(\n                            name=\"extra_notes\",\n                            type=SearchFieldDataType.Collection(SearchFieldDataType.String),\n                            searchable=True,\n                            filterable=False,\n                            sortable=False,\n                            facetable=False\n                        )\n                    ]\n                )\n            ]\n\n            semantic_config = SemanticConfiguration(\n                name=\"myEventsSemanticConfig\",\n                prioritized_fields=SemanticPrioritizedFields(\n                    title_field=SemanticField(field_name=\"event_name\"),\n                    content_fields=[SemanticField(field_name=\"content\")],\n                    keywords_fields=[\n                        SemanticField(field_name=\"event_type\"),\n                        SemanticField(field_name=\"event_id\")\n                    ]\n                )\n            )\n\n            vector_search = VectorSearch(\n                algorithms=[\n                    {\n                        \"name\": \"myHnsw\",\n                        \"kind\": \"hnsw\",\n                        \"parameters\": HnswParameters(\n                            m=4,\n                            ef_construction=400,\n                            ef_search=500,\n                            metric=\"cosine\"\n                        )\n                    }\n                ],\n                profiles=[\n                    VectorSearchProfile(\n                        name=\"myHnswProfile\",\n                        algorithm_configuration_name=\"myHnsw\"\n                    )\n                ]\n            )\n\n            cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n\n            return SearchIndex(\n                name=index_name,\n                fields=fields,\n                vector_search=vector_search,\n                semantic_search=SemanticSearch(configurations=[semantic_config]),\n                cors_options=cors_options\n            )\n\n \n        elif idx == docs_idx:\n          \n            fields = [\n                SearchableField(name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True, searchable=True),\n                SearchableField(name=\"title\", type=SearchFieldDataType.String, searchable=True),\n                SearchableField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n                SearchableField(name=\"platform\", type=SearchFieldDataType.String, filterable=True, searchable=True),\n                SearchableField(name=\"doc_type\", type=SearchFieldDataType.String, filterable=True, searchable=True),\n                #SearchableField(name=\"summary\", type=SearchFieldDataType.String, searchable=True, filterable=False),\n                #SearchableField(name=\"keywords\", type=SearchFieldDataType.String, searchable=True, filterable=False),\n                SearchField(\n                    name=\"embedding\",\n                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n                    searchable=True,\n                    vector_search_dimensions=EMBEDDING_DIM,\n                    vector_search_profile_name=\"apiDocsHnswProfile\"\n                )\n            ]\n\n            semantic_config = SemanticConfiguration(\n                name=\"apiDocsSemanticConfig\",\n                prioritized_fields=SemanticPrioritizedFields(\n                    title_field=SemanticField(field_name=\"title\"),\n                    content_fields=[SemanticField(field_name=\"content\")],\n                    keywords_fields=[SemanticField(field_name=\"platform\")]\n                )\n            )\n\n            vector_search = VectorSearch(\n                algorithms=[{\n                    \"name\": \"apiDocsHnsw\",\n                    \"kind\": \"hnsw\",\n                    \"parameters\": HnswParameters(m=4, ef_construction=400, ef_search=500, metric=\"cosine\")\n                }],\n                profiles=[VectorSearchProfile(name=\"apiDocsHnswProfile\", algorithm_configuration_name=\"apiDocsHnsw\")]\n            )\n\n            cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n\n            return SearchIndex(\n                name=index_name,\n                fields=fields,\n                vector_search=vector_search,\n                semantic_search=SemanticSearch(configurations=[semantic_config]),\n                cors_options=cors_options\n            )\n\n        \n        elif idx == plat_idx:\n\n            fields = [\n                SearchableField(\n                    name=\"id\",\n                    type=SearchFieldDataType.String,\n                    key=True,\n                    filterable=True,\n                    searchable=True\n                ),\n                SearchableField(\n                    name=\"platform\",\n                    type=SearchFieldDataType.String,\n                    filterable=True,\n                    searchable=True\n                ),\n                SearchableField(\n                    name=\"doc_type\",\n                    type=SearchFieldDataType.String,\n                    filterable=True,\n                    searchable=True\n                ),\n                SearchableField(\n                    name=\"content\",\n                    type=SearchFieldDataType.String,\n                    searchable=True\n                ),\n                SearchField(\n                    name=\"embedding\",\n                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n                    searchable=True,\n                    filterable=False,\n                    sortable=False,\n                    facetable=False,\n                    vector_search_dimensions=EMBEDDING_DIM,\n                    vector_search_profile_name=\"platformHnswProfile\"\n                )\n            ]\n\n            semantic_config = SemanticConfiguration(\n                name=\"platformSummariesSemanticConfig\",\n                prioritized_fields=SemanticPrioritizedFields(\n                    title_field=None,\n                    content_fields=[SemanticField(field_name=\"content\")],\n                    keywords_fields=[\n                        SemanticField(field_name=\"platform\"),\n                        SemanticField(field_name=\"doc_type\")\n                    ]\n                )\n            )\n\n            vector_search = VectorSearch(\n                algorithms=[{\n                    \"name\": \"platformHnsw\",\n                    \"kind\": \"hnsw\",\n                    \"parameters\": HnswParameters(\n                        m=4, ef_construction=400, ef_search=500, metric=\"cosine\"\n                    )\n                }],\n                profiles=[VectorSearchProfile(\n                    name=\"platformHnswProfile\",\n                    algorithm_configuration_name=\"platformHnsw\"\n                )]\n            )\n\n            cors_options = CorsOptions(allowed_origins=[\"*\"], max_age_in_seconds=60)\n\n            return SearchIndex(\n                name=index_name,\n                fields=fields,\n                vector_search=vector_search,\n                semantic_search=SemanticSearch(configurations=[semantic_config]),\n                cors_options=cors_options\n            )\n\n        else:\n            # if you hit here, you’ll immediately get a clear error instead of None\n            raise ValueError(f\"[AzureIndexer] No schema defined for index '{index_name}'\")\n        \n\n    def index_documents(self, docs: list, batch_size: int = 500):\n        if not self.search_client:\n            self.search_client = SearchClient(\n                endpoint=self.endpoint,\n                index_name=self.index_name,\n                credential=self.credential,\n                api_version=self.api_version,\n                logging_enable=True\n            )\n\n        total = len(docs)\n\n        print(\n            f\"[AzureIndexer]  ⚙  upload thread-pool = \"\n            f\"{min(4, (total + batch_size - 1)//batch_size)}  |  \"\n            f\"batch size = {batch_size}\",\n            flush=True,\n        )\n        def _push(batch):\n            return self.search_client.upload_documents(documents=batch)\n\n        batches = [docs[i : i + batch_size] for i in range(0, total, batch_size)]\n        with ThreadPoolExecutor(max_workers=min(4, len(batches))) as pool:\n            futs = {pool.submit(_push, b): idx for idx, b in enumerate(batches, 1)}\n            for fut in as_completed(futs):\n                idx = futs[fut]\n                try:\n                    fut.result()\n                    print(f\"[AzureIndexer] ✓ batch {idx}/{len(batches)}\", flush=True)\n                except Exception as exc:        # noqa: BLE001\n                    print(f\"[AzureIndexer] ✗ batch {idx} – {exc}\", flush=True)\n        print(f\"[AzureIndexer] All {total} documents uploaded.\")\n\n\n\nclass PlatformFunctionIndexer(AzureIndexer):\n    SAFE_KEY_CHARS = re.compile(r\"[^A-Za-z0-9_\\-=]\")\n\n    @staticmethod\n    def _safe_key(text: str) -> str:\n        \"\"\"\n        Return a key that satisfies Azure Search rules:\n        • only A-Z a-z 0-9 _ - =\n        • length ≤ 500 (Azure limit is 1024, we keep head-room)\n        \"\"\"\n        cleaned = PlatformFunctionIndexer.SAFE_KEY_CHARS.sub(\"_\", text)\n        if len(cleaned) <= 500:\n            return cleaned\n        # fallback: URL-safe base64, then trim to 500 chars\n        return base64.urlsafe_b64encode(text.encode()).decode()[:500]\n\n    def index_functions(\n        self,\n        platform: str,\n        diet_list: list[dict],\n        full_spec: dict,   # kept for parity\n    ):\n \n        # ── 1️⃣ ensure the index exists ──────────────────────────────\n        self.create_index()\n\n        # ── 2️⃣ build docs with a mini progress counter ──────────────\n        total = len(diet_list)\n\n        print(\n            f\"[{platform}] embedding {total} functions \"\n            f\"with {CPU_WORKERS} thread(s)…\",\n            flush=True,\n        )\n        def _prepare(fn: dict) -> dict:\n            key = self._safe_key(f\"{platform}-{fn['name']}\")\n            vec = fn.get(\"embedding\") or embed_text(fn[\"name\"])[0]\n            return {\n                \"id\":       key,\n                \"platform\": platform,\n                \"name\":     fn[\"name\"],\n                \"content\":  json.dumps(fn),\n                \"embedding\": vec,\n            }\n\n        docs: list[dict] = []\n        with ThreadPoolExecutor(max_workers=CPU_WORKERS) as pool:\n            for i, fut in enumerate(\n                as_completed(pool.submit(_prepare, f) for f in diet_list), start=1\n            ):\n                docs.append(fut.result())\n                if i % 100 == 0 or i == total:\n                    print(f\"   • processed {i}/{total}\", flush=True)\n\n\n\n        # ── 3️⃣ upload the batch ────────────────────────────────────────\n        self.index_documents(docs)        \n   \n\n\n        \n\n\n\n"
  },
  {
    "path": "scripts/indexers/base_indexer.py",
    "content": "#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/indexers/base_indexer.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\nfrom __future__ import annotations\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n\nclass BaseIndexer:\n    def __init__(self, index_name: str):\n        self.index_name = index_name\n    \n    def create_index(self):\n        \"\"\"Create the index if it doesn't exist.\"\"\"\n        raise NotImplementedError\n    \n    def index_documents(self, docs: list):\n        \"\"\"Index a list of documents (dicts with text/content).\"\"\"\n        raise NotImplementedError"
  },
  {
    "path": "scripts/indexers/chroma_indexer.py",
    "content": "#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/indexers/chroma_indexer.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\nfrom __future__ import annotations\n\n\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n\nimport base64\nimport json\nimport os\nimport re\nimport shutil\nimport uuid\nfrom pathlib import Path\nfrom threading import Lock\nfrom typing import Any, Dict, List\n\nimport chromadb\nfrom chromadb import PersistentClient                      # type: ignore\nfrom chromadb.config import Settings                       # type: ignore\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom ..utils.embedding import embed_text\nfrom .base_indexer import BaseIndexer\n\n# ───────────────────────────── runtime tuning ──────────────────────────────\nCPU_WORKERS  = int(os.getenv(\"FASTAPI_CHROMA_NUM_CPUS\", \"4\") or 4)\nDEFAULT_BATCH = int(os.getenv(\"FASTAPI_CHROMA_BATCH\", \"500\") or 500)\n\nprint(\n    f\"[ChromaIndexer]  ⚙  embedding threads = {CPU_WORKERS}  |  \"\n    f\"default batch size (upsert) = {DEFAULT_BATCH}\",\n    flush=True,\n)\n\n# ───────────────────────────── helpers ─────────────────────────────────────\n\nSAFE_ID_CHARS = re.compile(r\"[^A-Za-z0-9_\\-=]\")\n\ndef _safe_id(text: str) -> str:\n    cleaned = SAFE_ID_CHARS.sub(\"_\", text)\n    return cleaned if len(cleaned) <= 500 else base64.urlsafe_b64encode(text.encode()).decode()[:500]\n\ndef dedupe_ids(ids: list[str]) -> list[str]:\n    \"\"\"Append a counter to any duplicate IDs to make them unique.\"\"\"\n    seen: dict[str, int] = {}\n    out: list[str] = []\n    for _id in ids:\n        if _id not in seen:\n            seen[_id] = 0\n            out.append(_id)\n        else:\n            seen[_id] += 1\n            out.append(f\"{_id}_{seen[_id]}\")   # e.g. foo, foo_1, foo_2\n    return out\n\n# ───────────────────────────── once-per-process guard ──────────────────────\n_handled: set[str] = set()\n_lock = Lock()            # serialises the decision phase\n\n\n# ═══════════════════════════════════════════════════════════════════════════\n#  BASE CHROMA INDEXER  (mirrors Azure recreation logic)\n# ═══════════════════════════════════════════════════════════════════════════\nclass ChromaIndexer(BaseIndexer):\n    \"\"\"\n    Persistent Chroma collection with single-prompt recreate behaviour.\n    \"\"\"\n\n    # ────────────────────────────────────────────────────────────────\n    #  constructor  – decide recreate / append exactly once\n    # ────────────────────────────────────────────────────────────────\n    def __init__(self, index_name: str, layer_name: str = \"\") -> None:\n        super().__init__(index_name)\n\n        self.collection_name = index_name\n        self.layer_key       = layer_name.upper().strip() if layer_name else \"\"\n\n        base_path = os.getenv(f\"{self.layer_key}_CHROMA_DB_PATH\", \"./chroma_dbs\")\n        self.db_dir = Path(base_path).expanduser().resolve() / self.collection_name\n\n        recreate_env = os.getenv(f\"{self.layer_key}_CHROMA_RECREATE_INDEX\", \"\").strip().lower()\n\n        # ── decision happens only once per process per collection ──────────\n        with _lock:\n            if self.collection_name not in _handled:\n                exists = self.db_dir.is_dir() and any(self.db_dir.iterdir())\n\n                # ① choose behaviour\n                if exists:\n                    if recreate_env == \"true\":\n                        choice = \"r\"\n                        print(f\"[ChromaIndexer] {self.collection_name}: env forces **recreate**\")\n                    elif recreate_env == \"false\":\n                        choice = \"a\"\n                        print(f\"[ChromaIndexer] {self.collection_name}: env forces append\")\n                    else:\n                        ans = input(\n                            f\"Collection '{self.collection_name}' exists. \"\n                            \"(R)ecreate or (A)ppend? [R/a]: \"\n                        ).strip().lower()\n                        choice = \"r\" if ans.startswith(\"r\") else \"a\"\n                else:\n                    choice = \"a\"     # nothing to recreate on first run\n\n                # ② carry out choice\n                if choice == \"r\" and self.db_dir.exists():\n                    print(f\"[ChromaIndexer] Deleting {self.db_dir} …\")\n                    shutil.rmtree(self.db_dir, ignore_errors=True)\n\n                self.db_dir.mkdir(parents=True, exist_ok=True)\n                _handled.add(self.collection_name)   # remember\n\n        print(f\"[ChromaIndexer] DB directory → {self.db_dir}\")\n\n        # ── open (or create) the collection ───────────────────────────────\n        self.client: PersistentClient = chromadb.PersistentClient(\n            path=str(self.db_dir),\n            settings=Settings(anonymized_telemetry=False),\n        )\n        self.collection = self.client.get_or_create_collection(self.collection_name)\n        print(f\"[ChromaIndexer] Collection ready – {self.collection.count()} vectors\")\n\n    def create_index(self) -> None:          # ← add this method\n        \"\"\"\n        Compatibility shim — the collection is fully initialised in __init__,\n        so this is just a harmless no-op for callers that still invoke it.\n        \"\"\"\n        pass\n    # ────────────────────────────────────────────────────────────────\n    #  generic bulk-index helper\n    # ────────────────────────────────────────────────────────────────\n    def index_documents(self, docs: List[Dict[str, Any]]) -> None:\n        if not docs:\n            return\n\n        ids, contents, embeddings, metadatas = [], [], [], []\n        for d in docs:\n            ids.append(d.get(\"id\") or str(uuid.uuid4()))\n            contents.append(d.get(\"content\", \"\"))\n            embeddings.append(d.get(\"embedding\"))\n            metadatas.append({k: v for k, v in d.items() if k not in {\"id\", \"content\", \"embedding\"}})\n\n        payload: Dict[str, Any] = dict(ids=ids, documents=contents, metadatas=metadatas)\n        if any(e is not None for e in embeddings):\n            payload[\"embeddings\"] = embeddings\n\n        self.collection.add(**payload)\n        print(f\"[ChromaIndexer] Inserted {len(docs)} documents\")\n\n    # thin helper\n    def add(\n        self,\n        *,\n        documents: List[str],\n        ids: List[str],\n        metadatas: List[dict] | None = None,\n        embeddings: List[List[float]] | None = None,\n    ) -> None:\n        self.collection.upsert(\n            ids=ids,\n            documents=documents,\n            metadatas=metadatas or [{} for _ in ids],\n            embeddings=embeddings,\n        )\n\n\n# ═══════════════════════════════════════════════════════════════════════════\n#  PLATFORM-SPECIFIC FUNCTION INDEXER\n# ═══════════════════════════════════════════════════════════════════════════\nclass PlatformFunctionIndexer(ChromaIndexer):\n    \"\"\"\n    Diet-function indexer for Chroma that now prints incremental progress\n    identical to AzureIndexer.\n    \"\"\"\n\n    def index_functions(\n        self,\n        platform: str,\n        diet_list: List[Dict[str, Any]],\n        full_spec: Dict[str, Any],   # kept for parity / future use\n    ) -> None:\n        # 1 ▸ make sure collection exists / recreate logic already handled\n        self.create_index()\n\n        total = len(diet_list)\n        print(\n            f\"[{platform}] embedding {total} functions with \"\n            f\"{CPU_WORKERS} thread(s)…\",\n            flush=True,\n        )\n\n        # ---------- worker used by the thread-pool ----------\n        def _prep(fn: dict):\n            key  = _safe_id(f\"{platform}-{fn['name']}\")\n            vec  = fn.get(\"embedding\") or embed_text(fn[\"name\"])[0]\n            meta = {\"platform\": platform, \"name\": fn[\"name\"]}\n            return key, json.dumps(fn), meta, vec\n\n        ids, docs, metas, vecs = [], [], [], []\n\n        # use as_completed so we can count finished jobs\n        from concurrent.futures import ThreadPoolExecutor, as_completed\n\n        with ThreadPoolExecutor(max_workers=CPU_WORKERS) as pool:\n            futures = [pool.submit(_prep, fn) for fn in diet_list]\n\n            for i, fut in enumerate(as_completed(futures), start=1):\n                key, doc, meta, vec = fut.result()\n                ids.append(key)\n                docs.append(doc)\n                metas.append(meta)\n                vecs.append(vec)\n\n                if i % 100 == 0 or i == total:          # Azure-style ticks\n                    print(f\"   • processed {i}/{total}\", flush=True)\n\n        # ---------- bulk upsert ----------\n        print(f\"[ChromaIndexer] Upserting {len(ids)} docs into '{self.collection_name}'\")\n        ids = dedupe_ids(ids)          # ← NEW\n        self.collection.upsert(\n            ids=ids,\n            documents=docs,\n            metadatas=metas,\n            embeddings=vecs,\n)\n        print(\n            f\"[ChromaIndexer]  ✓ finished — {len(ids)} vectors written \"\n            f\"with {CPU_WORKERS} worker(s)\",\n            flush=True,\n        )\n        "
  },
  {
    "path": "scripts/indexers/elastic_indexer.py",
    "content": "#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/indexers/elastic_indexer.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\nfrom __future__ import annotations\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\nimport os, json\nfrom elasticsearch import Elasticsearch\nfrom .base_indexer import BaseIndexer\nfrom ..utils.embedding import embed_text\nclass ElasticIndexer(BaseIndexer):\n    def __init__(self, index_name: str):\n        super().__init__(index_name)\n        self.host = os.getenv(\"ELASTIC_HOST\", \"http://localhost:9200\")\n        self.user = os.getenv(\"ELASTIC_USER\", \"elastic\")\n        self.password = os.getenv(\"ELASTIC_PASSWORD\", \"changeme\")\n        self.client = Elasticsearch(self.host, http_auth=(self.user, self.password))\n    \n    def create_index(self):\n        if not self.client.indices.exists(index=self.index_name):\n            self.client.indices.create(index=self.index_name, body={\n                \"mappings\": {\n                    \"properties\": {\n                        \"content\": {\"type\": \"text\"},\n                    }\n                }\n            })\n            print(f\"Created Elasticsearch index {self.index_name}.\")\n        else:\n            print(f\"Elasticsearch index {self.index_name} already exists.\")\n    \n    def index_documents(self, docs: list):\n        for doc in docs:\n            self.client.index(index=self.index_name, document=doc)\n        print(f\"Elasticsearch: Indexed {len(docs)} documents in {self.index_name}.\")\n\nclass PlatformFunctionIndexer(ElasticIndexer):\n    def index_functions(self, platform: str, diet_list: list[dict], full_spec: dict):\n        docs = [\n            {\"id\": f\"{platform}:{fn['name']}\", \"content\": json.dumps(fn)}\n            for fn in diet_list\n        ]\n        self.index_documents(docs) \n\n"
  },
  {
    "path": "scripts/utils/__init__.py",
    "content": "\n#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/utils/__init__.py\n## Copyright (c) 2025 Jeff Teeter\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\nfrom __future__ import annotations\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\""
  },
  {
    "path": "scripts/utils/chunking.py",
    "content": "#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/utils/chunking.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\nfrom __future__ import annotations\n\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\nimport os\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef chunk_file(filepath_or_text, chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Processes either a file path or raw text, splits into manageable chunks, \n    and returns them as a list of strings.\n\n    Args:\n        filepath_or_text (str): Path to the file or raw text to process.\n        chunk_size (int): Max size of each chunk (chars).\n        chunk_overlap (int): Overlap size (chars) between chunks.\n\n    Returns:\n        List[str]: List of text chunks.\n    \"\"\"\n    text = \"\"\n    if os.path.isfile(filepath_or_text):\n        with open(filepath_or_text, 'r', encoding='utf-8') as f:\n            text = f.read()\n    else:\n        text = filepath_or_text\n\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n    chunks = splitter.split_text(text)\n    return chunks\n"
  },
  {
    "path": "scripts/utils/embedding.py",
    "content": "#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/utils/embedding.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\nfrom __future__ import annotations\n\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis script is provided 'as is' without warranty of any kind. By using this code,\nyou agree to the terms stated in the LICENSE and above disclaimers.\n\nThis module focuses solely on generating embeddings from text, without\nsummaries/keywords. Summaries & keywords are handled by other code if needed.\n\"\"\"\nimport os\nimport openai\nfrom typing import List, Union\nfrom sentence_transformers import SentenceTransformer\nfrom threading import Lock\n_hf_models = {}  # Cache loaded SentenceTransformer models\n_hf_lock = Lock()  \n\ndef embed_text(text: Union[str, List[str]], layer: str = \"FASTAPI\") -> List[List[float]]:\n    \"\"\"\n    Generate embedding vectors for given text(s), based on environment variables\n    indicating whether the layer uses Azure or Hugging Face embeddings, etc.\n\n    Args:\n        text (str or List[str]): The text(s) to embed.\n        layer (str): The layer name, e.g. \"DOMAIN\", \"FASTAPI\", \"EVENTS\", \"AGENTIC\".\n\n    Returns:\n        List[List[float]]: A list of embedding vectors (one per input string).\n    \"\"\"\n\n    # 1) Ensure input is a list\n    if isinstance(text, str):\n        texts = [text]\n    else:\n        texts = text\n\n    # 2) Decide embedding provider\n    #    e.g., \"AGENTIC_EMBEDDING_PROVIDER\" => \"azure\" or \"huggingface\" or \"elastic\"\n    provider_var = f\"{layer.upper()}_EMBEDDING_PROVIDER\"\n    provider = os.getenv(provider_var, \"huggingface\").lower()\n\n    if provider == \"azure\":\n        return _embed_azure(texts, layer)\n    elif provider == \"huggingface\":\n        return _embed_huggingface(texts, layer)\n    elif provider == \"elastic\":\n        # If you implement Elasticsearch-based embedding logic, you'd do that here.\n        raise NotImplementedError(\"Elastic embedding not implemented in this script.\")\n    else:\n        raise ValueError(f\"[embedding.py] Unknown provider '{provider}' for layer '{layer}'\")\n\ndef _embed_azure(texts: List[str], layer: str) -> List[List[float]]:\n    \"\"\"\n    Use Azure OpenAI to embed a list of texts. The code reads environment\n    variables like <LAYER>_AZURE_OPENAI_ENDPOINT, <LAYER>_AZURE_OPENAI_KEY,\n    <LAYER>_AZURE_EMBEDDING_DEPLOYMENT, etc.\n    \"\"\"\n    # 1) Compose env var names\n    layer_key = layer.upper()\n    azure_endpoint = os.getenv(f\"{layer_key}_AZURE_OPENAI_ENDPOINT\", \"\").strip(\"/\")\n    azure_key = os.getenv(f\"{layer_key}_AZURE_OPENAI_KEY\", \"\")\n    azure_deployment = os.getenv(f\"{layer_key}_AZURE_OPENAI_EMBEDDING_DEPLOYMENT\",\"\")\n\n    if not azure_endpoint or not azure_key or not azure_deployment:\n        raise ValueError(f\"[embedding.py] Missing Azure env vars for layer '{layer}': \"\n                         f\"{layer_key}_AZURE_OPENAI_ENDPOINT, _KEY, or _EMBEDDING_DEPLOYMENT not set.\")\n\n    # 2) Set openai config\n    openai.api_type = \"azure\"\n    openai.api_base = azure_endpoint\n    openai.api_key = azure_key\n    # Possibly read a version like <LAYER>_AZURE_OPENAI_API_VERSION if you want\n    openai.api_version = os.getenv(f\"{layer_key}_AZURE_OPENAI_API_VERSION\", \"2024-10-21\")\n\n    # 3) Make the request\n    resp = openai.Embedding.create(input=texts, engine=azure_deployment)\n    # 4) Extract embeddings\n    vectors = [item[\"embedding\"] for item in resp[\"data\"]]\n    return vectors\n\ndef _embed_huggingface(texts: list[str], layer: str) -> list[list[float]]:\n    \"\"\"\n    Embeds *texts* with a local / HF SentenceTransformer model.\n    Model name comes from  <LAYER>_HUGGINGFACE_MODEL  (env).\n    The model is loaded exactly **once** per process, guarded by a lock so\n    multi-threaded pipelines don’t race when instantiating it.\n    \"\"\"\n    layer_key  = layer.upper()\n    model_name = os.getenv(f\"{layer_key}_HUGGINGFACE_MODEL\", \"\").strip()\n    if not model_name:\n        raise ValueError(\n            f\"[embedding.py] No Hugging Face model specified for layer '{layer}'. \"\n            f\"Set {layer_key}_HUGGINGFACE_MODEL in your env.\"\n        )\n\n    # ── lazy-load the model, but only one thread does the heavy work ─────────\n    with _hf_lock:\n        if model_name not in _hf_models:\n            print(f\"[embedding.py] Loading HF model '{model_name}' for layer '{layer}'…\")\n            from sentence_transformers import SentenceTransformer\n            _hf_models[model_name] = SentenceTransformer(model_name, device=\"cpu\")\n\n    model = _hf_models[model_name]\n\n    # encode() is internally parallelised by SentenceTransformers/torch\n    ndarray = model.encode(texts, convert_to_numpy=True)\n    return [vec.tolist() for vec in ndarray]\n\n"
  },
  {
    "path": "scripts/utils/nlp_tasks.py",
    "content": "#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/utils/nlp_tasks.py\n## Copyright (c) 2025 Jeff Teeter, Ph.D.\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n################################################################################\nfrom __future__ import annotations\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\nimport os\nimport logging\nimport yake\n\n# Summaries\nSUMMARIZER_TYPE = os.getenv(\"API_VECTOR_SUMMARIZER_TYPE\", \"sumy\").strip().lower()\n# Keywords\nKEYWORD_EXTRACTOR_TYPE = os.getenv(\"API_VECTOR_KEYWORD_TYPE\", \"keybert\").strip().lower()\n\n# Summaries\n_summarizer_pipeline = None\ndef summarize_text_sumy(text: str) -> str:\n    if not text.strip():\n        return \"\"\n    from sumy.parsers.plaintext import PlaintextParser\n    from sumy.nlp.tokenizers import Tokenizer\n    from sumy.summarizers.lex_rank import LexRankSummarizer\n    from sumy.nlp.stemmers import Stemmer\n    from sumy.utils import get_stop_words\n\n    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n    stemmer = Stemmer(\"english\")\n    summarizer = LexRankSummarizer(stemmer)\n    summarizer.stop_words = get_stop_words(\"english\")\n\n    max_sents = 5\n    doc_sents = len(list(parser.document.sentences))\n    if doc_sents < max_sents:\n        max_sents = doc_sents\n    summary_sents = summarizer(parser.document, max_sents)\n    summary_str = \" \".join(str(s) for s in summary_sents)\n    return summary_str.strip()\n\ndef summarize_text_transformer(text: str) -> str:\n    global _summarizer_pipeline\n    if not _summarizer_pipeline:\n        from transformers import pipeline\n        logging.info(\"[nlp_tasks] Loading DistilBART summarizer pipeline...\")\n        _summarizer_pipeline = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=-1)\n    if not text.strip():\n        return \"\"\n    out = _summarizer_pipeline(text, max_length=120, min_length=30, do_sample=False)\n    return out[0]['summary_text'].strip() if out else \"\"\n\ndef summarize_text(text: str) -> str:\n    if SUMMARIZER_TYPE in [\"none\", \"\", \"null\"]:\n        return \"\"\n    if SUMMARIZER_TYPE in [\"sumy\", \"gensim\", \"lexrank\"]:\n        # Gensim is deprecated, use sumy\n        return summarize_text_sumy(text)\n    elif SUMMARIZER_TYPE in [\"transformers\", \"transformer\", \"hf\", \"bart\"]:\n        return summarize_text_transformer(text)\n    else:\n        logging.warning(f\"[nlp_tasks] Unrecognized SUMMARIZER_TYPE '{SUMMARIZER_TYPE}', defaulting to sumy.\")\n        return summarize_text_sumy(text)\n\n# Keywords\n_kw_model_keybert = None\ndef extract_keywords_keybert(text: str) -> list:\n    global _kw_model_keybert\n    if not _kw_model_keybert:\n        from sentence_transformers import SentenceTransformer\n        from keybert import KeyBERT\n        model_name = os.getenv('API_VECTOR_EMBEDDINGS_MODEL', 'all-MiniLM-L6-v2')\n        emb_model = SentenceTransformer(model_name)\n        _kw_model_keybert = KeyBERT(emb_model)\n        logging.info(f\"[nlp_tasks] Initialized KeyBERT with model: {model_name}\")\n    results = _kw_model_keybert.extract_keywords(text, top_n=10)\n    return [r[0] for r in results]\n\ndef extract_keywords_yake(text: str) -> list:\n    import yake\n    extractor = yake.KeywordExtractor(lan=\"en\", n=3, top=10)\n    results = extractor.extract_keywords(text)\n    return [kw for (kw, score) in results]\n\ndef extract_keywords(text: str) -> list:\n    # Check if user wants keywords\n    enable_keywords = os.getenv(\"API_VECTOR_ENABLE_KEYWORDS\", \"false\").lower() == \"true\"\n    if not enable_keywords or not text.strip():\n        return []\n    if KEYWORD_EXTRACTOR_TYPE == \"yake\":\n        return extract_keywords_yake(text)\n    else:\n        # default to keybert\n        return extract_keywords_keybert(text)\n"
  },
  {
    "path": "scripts/utils/pipeline_utils.py",
    "content": "#!/usr/bin/env python3\n################################################################################\n## ai-building-blocks-database/scripts/utils/pipeline_utils.py\n## Copyright (c) 2025 ...\n## Cisco Systems, Inc.\n## Licensed under the Apache License, Version 2.0 (see LICENSE)\n## Distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.\n#################################################################################\nfrom __future__ import annotations\n\"\"\"\nDISCLAIMER: USE AT YOUR OWN RISK\n\nThis software is provided \"as is\", without any express or implied warranties, including, but not limited to,\nthe implied warranties of merchantability and fitness for a particular purpose. In no event shall the authors or\ncontributors be liable for any direct, indirect, incidental, special, exemplary, or consequential damages\n(including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits;\nor business interruption) however caused and on any theory of liability, whether in contract, strict liability,\nor tort (including negligence or otherwise) arising in any way out of the use of this software.\n\nThis script is provided for demonstration and development purposes only and is not intended for use in production\nenvironments. You are solely responsible for any modifications or adaptations made for your specific use case.\n\nBy using this code, you agree that you have read, understood, and accept these terms.\n\"\"\"\n\nimport os\nimport uuid\nimport shutil\nimport chromadb\nfrom dotenv import load_dotenv\nfrom chromadb.config import Settings\nfrom typing import List, Union\nimport json\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.search.documents import SearchClient\nload_dotenv()\n\n################################################################################\n# Embedding providers (Azure-OpenAI, OpenAI, HuggingFace, Cohere)\n################################################################################\nclass EmbeddingProviderBase:\n    def embed(self, texts):\n        raise NotImplementedError\n\nclass OpenAIEmbedding(EmbeddingProviderBase):\n    def __init__(self, prefix):\n        import openai\n        key = os.getenv(f\"{prefix}_OPENAI_API_KEY\")\n        if not key:\n            raise RuntimeError(f\"{prefix}_OPENAI_API_KEY not set\")\n        openai.api_key = key\n        self.model = os.getenv(f\"{prefix}_OPENAI_EMBEDDING_MODEL\",\n                               \"text-embedding-ada-002\")\n\n    def embed(self, texts):\n        import openai\n        resp = openai.Embedding.create(input=texts, model=self.model)\n        return [d[\"embedding\"] for d in resp[\"data\"]]\n\nclass AzureOpenAIEmbedding(EmbeddingProviderBase):\n    def __init__(self, prefix):\n        import openai\n        api_key    = os.getenv(f\"{prefix}_AZURE_OPENAI_KEY\")\n        endpoint   = os.getenv(f\"{prefix}_AZURE_OPENAI_ENDPOINT\")\n        deployment = os.getenv(f\"{prefix}_AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n        api_version= os.getenv(f\"{prefix}_AZURE_OPENAI_API_VERSION\", \"2023-05-15\")\n        if not (api_key and endpoint and deployment):\n            raise RuntimeError(\n                f\"[pipeline_utils] Azure OpenAI creds missing for {prefix}. \"\n                \"Make sure you have set \"\n                f\"{prefix}_AZURE_OPENAI_KEY, \"\n                f\"{prefix}_AZURE_OPENAI_ENDPOINT, \"\n                f\"{prefix}_AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"\n            )\n        openai.api_type    = \"azure\"\n        openai.api_base    = endpoint\n        openai.api_version = api_version\n        openai.api_key     = api_key\n        self.deployment    = deployment\n\n    def embed(self, texts):\n        import openai\n        resp = openai.Embedding.create(input=texts, engine=self.deployment)\n        return [d[\"embedding\"] for d in resp[\"data\"]]\n\nclass HuggingFaceEmbedding(EmbeddingProviderBase):\n    def __init__(self, prefix):\n        from sentence_transformers import SentenceTransformer\n        model = os.getenv(f\"{prefix}_HUGGINGFACE_MODEL\",\n                          \"sentence-transformers/all-MiniLM-L6-v2\")\n        self.model = SentenceTransformer(model)\n\n    def embed(self, texts):\n        arr = self.model.encode(texts, convert_to_numpy=True)\n        return [v.tolist() for v in arr]\n\nclass CohereEmbedding(EmbeddingProviderBase):\n    def __init__(self, prefix):\n        import cohere\n        key = os.getenv(f\"{prefix}_COHERE_API_KEY\")\n        if not key:\n            raise RuntimeError(f\"{prefix}_COHERE_API_KEY not set\")\n        self.client = cohere.Client(key)\n        self.model  = os.getenv(f\"{prefix}_COHERE_EMBEDDING_MODEL\", None)\n\n    def embed(self, texts):\n        if self.model:\n            return self.client.embed(texts=texts, model=self.model).embeddings\n        return self.client.embed(texts=texts).embeddings\n\ndef get_embedding_provider(name, prefix):\n    n = name.strip().lower()\n    if n in (\"openai\",\"openai-api\"):     return OpenAIEmbedding(prefix)\n    if n in (\"azure\",\"azure-openai\"):    return AzureOpenAIEmbedding(prefix)\n    if n in (\"huggingface\",\"hf\",\"local\"):return HuggingFaceEmbedding(prefix)\n    if n == \"cohere\":                    return CohereEmbedding(prefix)\n    raise ValueError(f\"Unknown embedding provider '{name}'\")\n\n################################################################################\n# Vector Indexers\n################################################################################\nclass VectorIndexerBase:\n    def index_batch(self, docs, embeddings): raise NotImplementedError\n\nclass AzureVectorIndexer(VectorIndexerBase):\n    def __init__(self, prefix: str):\n        endpoint  = os.getenv(f\"{prefix}_AZURE_ENDPOINT\")\n        key       = os.getenv(f\"{prefix}_AZURE_KEY\")\n        indexname = (\n            os.getenv(f\"{prefix}_AZURE_INDEX\")\n            or os.getenv(f\"{prefix}_AZURE_PLATFORM_INDEX\")\n            or os.getenv(f\"{prefix}_AZURE_DOCS_INDEX\")\n        )\n        if not (endpoint and key and indexname):\n            raise RuntimeError(f\"Azure Search not configured for {prefix}\")\n        cred = AzureKeyCredential(key)\n        self.client = SearchClient(endpoint, indexname, cred)\n        self.field  = os.getenv(f\"{prefix}_AZURE_VECTOR_COLUMNS\", \"embedding\")\n\n\n    def index_batch(self, docs: List[dict], embeddings: List[List[float]]) -> None:\n        payload = []\n        for doc, emb in zip(docs, embeddings):\n            # base fields\n            item = {\n                \"id\":      doc.get(\"id\", str(uuid.uuid4())),\n                \"content\": doc.get(\"content\", \"\"),\n                self.field: emb\n            }\n\n            # flatten everything else:\n            for k, v in doc.items():\n                if k in (\"id\", \"content\"):\n                    continue\n                if k == \"metadata\":\n                    # domain-index expects a string\n                    item[k] = json.dumps(v)\n                else:\n                    # events-index (and others) expect dicts/lists as-is\n                    item[k] = v\n\n            payload.append(item)\n\n        print(f\"[AzureVectorIndexer] Uploading {len(payload)} docs to '{self.client._index_name}'\")\n        result = self.client.upload_documents(documents=payload)\n        print(f\"[AzureVectorIndexer] Upload result: {result}\")\n\nclass ChromaVectorIndexer(VectorIndexerBase):\n    \"\"\"\n    Uses the LAYER prefix to decide recreate behavior, and accepts\n    either List[str] or List[dict] with metadata (or auto-extracts one).\n    \"\"\"\n    def __init__(self, collection_name: str, layer_prefix: str):\n        self.collection_name = collection_name\n        self.layer_prefix    = layer_prefix\n\n        base_db_path    = os.getenv(f\"{layer_prefix}_CHROMA_DB_PATH\", \"./chroma_db\")\n        recreate_env    = os.getenv(f\"{layer_prefix}_CHROMA_RECREATE_INDEX\", \"\").strip().lower()\n        self.db_path    = os.path.join(base_db_path, collection_name)\n\n        print(f\"[ChromaVectorIndexer] collection='{collection_name}', db_path='{self.db_path}'\")\n\n        already = os.path.isdir(self.db_path) and os.listdir(self.db_path)\n        if already:\n            print(f\"[ChromaVectorIndexer] Found existing data in {self.db_path}\")\n            if recreate_env == \"\":\n                ans = input(f\"Collection '{collection_name}' found. (R)ecreate or (A)ppend? [R/a]: \").strip().lower()\n                if ans.startswith(\"r\"):\n                    self._recreate()\n            elif recreate_env == \"true\":\n                self._recreate()\n            else:\n                print(f\"[ChromaVectorIndexer] Appending (env said 'false').\")\n        else:\n            print(f\"[ChromaVectorIndexer] No existing data. Creating {self.db_path}…\")\n            os.makedirs(self.db_path, exist_ok=True)\n\n        self.client     = chromadb.PersistentClient(\n            path=self.db_path,\n            settings=Settings(allow_reset=True, anonymized_telemetry=False)\n        )\n        self.collection = self.client.get_or_create_collection(collection_name)\n        print(f\"[ChromaVectorIndexer] Ready for docs in collection '{collection_name}'\")\n\n    def _recreate(self):\n        print(f\"[ChromaVectorIndexer] Recreating by removing {self.db_path}…\")\n        try:\n            shutil.rmtree(self.db_path)\n        except FileNotFoundError:\n            pass\n        except Exception as e:\n            print(f\"[ChromaVectorIndexer] Warning: failed to remove {self.db_path}: {e}\")\n        os.makedirs(self.db_path, exist_ok=True)\n        print(f\"[ChromaVectorIndexer] Done. Fresh start at {self.db_path}.\")\n\n    def index_batch(\n        self,\n        docs: List[Union[str, dict]],\n        embeddings: List[List[float]]\n    ) -> None:\n        if not docs or not embeddings:\n            raise ValueError(\"Docs and embeddings cannot be empty\")\n        if len(docs) != len(embeddings):\n            raise ValueError(\"Docs and embeddings must have the same length\")\n\n        # DICT CASE: grab id/content, auto-extract metadata if missing\n        if isinstance(docs[0], dict):\n            ids       = [doc.get(\"id\", str(uuid.uuid4())) for doc in docs]\n            contents  = [doc[\"content\"]               for doc in docs]\n            metadatas = []\n\n            for doc in docs:\n                # prefer an explicit metadata field…\n                raw_meta = doc.get(\"metadata\", None)\n                if raw_meta is None:\n                    # …otherwise pack every other key into metadata\n                    raw_meta = { k: v for k, v in doc.items() if k not in (\"id\", \"content\") }\n\n                # flatten any non-primitive values\n                flat = {}\n                for k, v in raw_meta.items():\n                    if isinstance(v, (str, int, float, bool)):\n                        flat[k] = v\n                    else:\n                        flat[k] = json.dumps(v)\n                metadatas.append(flat)\n\n            print(f\"[ChromaVectorIndexer] Adding {len(ids)} docs with metadata to '{self.collection_name}'\")\n            self.collection.upsert(\n                ids=ids,\n                documents=contents,\n                embeddings=embeddings,\n                metadatas=metadatas\n            )\n\n        # STRING CASE: exactly as before\n        else:\n            ids = [str(uuid.uuid4()) for _ in docs]\n            print(f\"[ChromaVectorIndexer] Adding {len(ids)} plain docs to '{self.collection_name}'\")\n            self.collection.upsert(\n                documents=docs,\n                embeddings=embeddings,\n                ids=ids\n            )\n\n\n\n\nclass ElasticVectorIndexer(VectorIndexerBase):\n    def __init__(self, prefix):\n        from elasticsearch import Elasticsearch\n        host = os.getenv(f\"{prefix}_ELASTIC_HOST\", \"localhost:9200\")\n        user = os.getenv(f\"{prefix}_ELASTIC_USER\")\n        pw   = os.getenv(f\"{prefix}_ELASTIC_PASSWORD\")\n        self.client = Elasticsearch(hosts=[host], http_auth=(user,pw) if user else None)\n        self.index  = os.getenv(f\"{prefix}_ELASTIC_INDEX\") or os.getenv(f\"{prefix}_INDEX_NAME\")\n\n    def index_batch(self, docs, embeddings):\n        from elasticsearch.helpers import bulk\n        actions = [{\n          \"_index\": self.index,\n          \"_id\":    str(uuid.uuid4()),\n          \"_source\": {\"content\": t, \"vector\": v}\n        } for t,v in zip(docs, embeddings)]\n        bulk(self.client, actions)\n\ndef get_vector_indexer(backend_name: str, layer_prefix: str):\n    n = backend_name.strip().lower()\n    if n == \"azure\":\n        return AzureVectorIndexer(layer_prefix)\n    elif n == \"chroma\":\n        # read the actual collection name from e.g. \"EVENTS_CHROMA_COLLECTION\" \n        # or fallback to \"EVENTS_CHROMA_INDEX\"\n        # or default to e.g. f\"{layer_prefix.lower()}-index\"\n        col_name = (\n            os.getenv(f\"{layer_prefix}_CHROMA_COLLECTION\")\n            or os.getenv(f\"{layer_prefix}_CHROMA_INDEX\")\n            or f\"{layer_prefix.lower()}-index\"\n        )\n        return ChromaVectorIndexer(col_name, layer_prefix)\n    elif n in (\"elastic\",\"es\"):\n        return ElasticVectorIndexer(layer_prefix)\n    else:\n        raise ValueError(f\"Unknown vector backend '{backend_name}'\")\n\n################################################################################\n# The embedding + indexing pipeline \n################################################################################\n\ndef _worker(chunk: List[str], provider_name: str, backend_name: str, prefix: str):\n    embedder = get_embedding_provider(provider_name, prefix)\n    indexer  = get_vector_indexer(backend_name, prefix)\n\n    batch_sz = int(os.getenv(f\"{prefix}_EMBED_BATCH_SIZE\", \"64\"))\n    for i in range(0, len(chunk), batch_sz):\n        batch = chunk[i : i + batch_sz]\n\n        # if we received dicts, pull out the text for embedding…\n        if isinstance(batch[0], dict):\n            texts = [item[\"content\"] for item in batch]\n            docs  = batch\n        else:\n            texts = batch\n            docs  = batch\n\n        embeddings = embedder.embed(texts)\n        indexer.index_batch(docs, embeddings)\n\ndef index_documents(docs: List[str], provider_name: str, backend_name: str, layer_prefix: str):\n    if not docs:\n        return\n\n    num = int(os.getenv(f\"{layer_prefix}_OMP_NUM_THREADS\", \"1\"))\n    # Force concurrency=1 for Chroma:\n    if backend_name.lower() == \"chroma\":\n        num = 1\n\n    if num > 1:\n        size = len(docs) // num or len(docs)\n        chunks = [docs[i:i+size] for i in range(0, len(docs), size)]\n        from concurrent.futures import ProcessPoolExecutor\n        with ProcessPoolExecutor(max_workers=num) as ex:\n            for c in chunks:\n                ex.submit(_worker, c, provider_name, backend_name, layer_prefix)\n    else:\n        _worker(docs, provider_name, backend_name, layer_prefix)\n"
  }
]